{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.3):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "meanLabels = np.mean(labelDf.mean())\n",
    "stdLabels = np.std(labelDf.std())\n",
    "maxLabels = np.max(labelDf.max())\n",
    "minLabels = np.min(labelDf.min())\n",
    "\n",
    "trainLabels = labelDf.loc[[int(star) for star in train_stars]]\n",
    "meanTrainLabels = np.mean(trainLabels.mean())\n",
    "stdTrainLabels = np.std(trainLabels.std())\n",
    "maxTrainLabels = np.max(trainLabels.max())\n",
    "minTrainLabels = np.min(trainLabels.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTrainLabels)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def maxOfTrain(train_stars):\n",
    "    maxTrain = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,1]\n",
    "            maxTrain = max(maxTrain, np.max(x))  \n",
    "    return maxTrain\n",
    "maxTrain= maxOfTrain(train_stars)\n",
    "\n",
    "def calcRollingMeans(windows, features):\n",
    "    arr=[features]\n",
    "    for window in windows:\n",
    "        padded_data = np.pad(features, ((window, 0),(0,0)), mode='edge')\n",
    "        cumsum = np.cumsum(padded_data, axis=0)\n",
    "        result = (cumsum[window:,:] - cumsum[:-window,:]) / window\n",
    "        arr.append(result)\n",
    "\n",
    "        # calc diff of rolling mean & compensate rolling mean with it\n",
    "        diff = np.diff(result,axis=0)\n",
    "        mean_lin_slope = np.mean(diff[window:,:],axis=0)\n",
    "        accumulatedSlopes = np.cumsum(np.ones_like(result)*mean_lin_slope, axis=0)\n",
    "        compensatedSignal = result - accumulatedSlopes\n",
    "        arr.append(compensatedSignal)\n",
    "    \n",
    "        # add diff shape\n",
    "        diffCorr = np.zeros_like(result)\n",
    "        diffCorr[1:,:] = diff\n",
    "        arr.append(diffCorr)\n",
    "    return np.stack(arr,axis=-1)\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            f = data['a'][0,:,0:283,1]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            meanL = np.mean(labels)\n",
    "            stdL = np.std(labels)\n",
    "            #labels = (labels-meanL)/ stdL #*100\n",
    "            f = np.reshape(f,(-1,25,283))\n",
    "            f = np.mean(f,axis=1)\n",
    "            maxValpWL = np.mean(f,axis=0) #bring all values to a similar scale -> differences are now comparable\n",
    "            f = 100*(f / maxValpWL) # map to [-100*diff,0]\n",
    "            featureMax = np.reshape(np.ones_like(f) * 100/maxValpWL, (225,283,1))\n",
    "            # my network can esitmate the -100*diff and has 100/maxVal\n",
    "            # lambda=(x_top-x_bottom)/x_top = x_diff / x_top = (x_diff/maxVal) / (x_top/maxVal)\n",
    "            #       = 1 - x_bottom/x_top  -> scalieren \n",
    "\n",
    "            # x_bottom/maxVal - 1, x_top/maxVal - 1, \n",
    "\n",
    "            f = calcRollingMeans([30,50,80,100],f)\n",
    "            f = np.concatenate([f, featureMax], axis = 2) #give info about magnitude as well\n",
    "            #f = np.transpose(f, (1, 0, 2))\n",
    "            return f, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "\n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list), reshuffle_each_iteration=True)\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([225,283,14])), tf.ensure_shape(y, tf.TensorShape(283)))) #5625\n",
    "    #dataset = dataset.unbatch()\n",
    "    #dataset = dataset.map(lambda x, y: (x, y))\n",
    "    #if shuffle:\n",
    "    #    dataset = dataset.shuffle(buffer_size=len(star_list)*283, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('helpers_origiData_cnn7FullPred.npz',maxTrainLabels=maxTrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)\n",
    "small_dataset = create_dataset(train_stars[0:batch_size], batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        return x\n",
    "    \n",
    "class Reshape3(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (None,-1,x.shape[2]))\n",
    "        return x\n",
    "    \n",
    "class Reshape4(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        x = tf.reshape(x, (x.shape[0], x.shape[1], x.shape[2]*x.shape[3]))\n",
    "        return x\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        # Compute the output shape\n",
    "        return (input_shape[0],input_shape[2], input_shape[1] * input_shape[3])\n",
    "    \n",
    "class reduce(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        mean = tf.expand_dims(mean, axis=-1)\n",
    "        return mean\n",
    "class reduce1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        return mean\n",
    "    \n",
    "class reduce11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = x[:,:,:,0]\n",
    "        return x\n",
    "    \n",
    "class tile(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x,mean):\n",
    "        x = tf.concat([x,mean],axis=-1)\n",
    "        return x\n",
    "    \n",
    "class tile2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x,mean):\n",
    "        x = tf.concat([x,tf.expand_dims(mean,axis=-1)],axis=-2)\n",
    "        return x\n",
    "    \n",
    "class meanOfWavelengths(tf.keras.layers.Layer):\n",
    "    def __init__(self, concat=True,**kwargs):\n",
    "        self.concat=concat\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        m = tf.expand_dims(tf.reduce_mean(x,axis=-1),axis=-1)\n",
    "        x = tf.concat([x,m],axis=-1)\n",
    "        return x if self.concat else m\n",
    "\n",
    "# gated linear unit, splits input in 2 batches, second batch is activation\n",
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x, mask=None):\n",
    "        x,gate = tf.split(x, 2, axis = -1)\n",
    "        # swish = gate * sigmoid(gate) (sigmoid = between 0..1)\n",
    "        x = x*tf.keras.activations.swish(gate) # use one input as a gate such that the network is able to focus on information\n",
    "        return x\n",
    "\n",
    "class GLUMlp(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim_expand, dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim_expand = dim_expand\n",
    "        self.dim = dim\n",
    "        # same operation as dense layer\n",
    "        self.dense_1 = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(None, self.dim_expand), activation = 'linear', bias_axes = 'd')\n",
    "        self.glu_1 = GLU()\n",
    "        self.dense_2 = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(None, self.dim), activation = 'linear', bias_axes = 'd')\n",
    "    def call(self, x, training = False):\n",
    "        #print('glu_input',x.shape)\n",
    "        x = self.dense_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.glu_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.dense_2(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "class ScaleBias(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.scale_bias = tf.keras.layers.EinsumDense(\"abc,c->abc\",output_shape=(None, input_shape[-1]),activation = 'linear', bias_axes = 'c')\n",
    "    def call(self, x, mask=None):\n",
    "        return self.scale_bias(x)\n",
    "\n",
    "#attention gets calculated along first dimension!\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "        self.ffn = GLUMlp(feed_forward_dim, embed_dim)\n",
    "        #self.ffn = tf.keras.layers.Dense(feed_forward_dim)\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # normalization by a * (input-mean) /sqrt(var + eps) +b    where a and b are learned, eps is to avoid div/0\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        #self.scale_bias_1 = ScaleBias()\n",
    "        #self.scale_bias_2 = ScaleBias()\n",
    "    def call(self, x, training = None):\n",
    "        residual = x\n",
    "        #print('before att')\n",
    "        x = self.att(x, x)\n",
    "        #x = self.scale_bias_1(x)\n",
    "        x = self.layer_norm_1(x + residual)\n",
    "        #x = x+residual\n",
    "        residual = x\n",
    "        #print('after att')\n",
    "        x = self.ffn(x, training = training)\n",
    "        #print('after glu')\n",
    "        #x = self.scale_bias_2(x)\n",
    "        x = self.layer_norm_2(x + residual)\n",
    "        return x\n",
    "    \n",
    "# is effectively an attention mechanism to allow some columns to be used / turned off\n",
    "# effective channel attention!\n",
    "class ECA(tf.keras.layers.Layer):\n",
    "    # TF implementation from https://www.kaggle.com/code/hoyso48/1st-place-solution-training\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False) # only one1D convolution with kernel size\n",
    "    def call(self, inputs):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs) # works not on batch size, but on next dimension, e.g. batch_size, 60x9 -> works on 60, so output is batch_size x 9\n",
    "        nn = tf.expand_dims(nn, -1) # a,c -> a,c,1\n",
    "        nn = self.conv(nn) # a,c,1 -> a,c,1 (1, because conv is only having 1 filter)\n",
    "        nn = tf.squeeze(nn, -1) # a,c,1 -> a,c\n",
    "        nn = tf.nn.sigmoid(nn) # a,c -> a,c\n",
    "        nn = nn[:,None,:] # a,1,c -> e.g. batch_size,1,9\n",
    "        return inputs * nn # a,1,c * a,b,c applies broadcasting / elementwise multiplication -> turns input on or off column wise\n",
    "\n",
    "\n",
    "class HeadDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, head_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.head_dim = head_dim\n",
    "    def build(self, input_shape):\n",
    "        self.length = input_shape[1]\n",
    "        self.dim = input_shape[2]\n",
    "        self.dense = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(self.length, self.head_dim), activation = 'swish', bias_axes = 'd') #siwsh is causing a self gating\n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "class Conv1DBlockSqueezeformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, channel_size, kernel_size, dilation_rate=1,\n",
    "                 expand_ratio=2, se_ratio=0.25, activation='swish', name=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.channel_size = channel_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "        self.activation = activation\n",
    "        self.scale_bias = ScaleBias()\n",
    "        self.glu_layer = GLU()\n",
    "        self.ffn = GLUMlp(channel_size*4, channel_size)\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.scale_bias_1 = ScaleBias()\n",
    "        self.scale_bias_2 = ScaleBias()\n",
    "    def build(self, input_shape):\n",
    "        self.length = input_shape[1]\n",
    "        self.channels_in = input_shape[2]\n",
    "        self.channels_expand = self.channels_in * self.expand_ratio\n",
    "        self.dwconv = tf.keras.layers.DepthwiseConv1D(self.kernel_size,dilation_rate=self.dilation_rate,padding='same',use_bias=False)\n",
    "        self.BatchNormalization_layer = tf.keras.layers.BatchNormalization(momentum=0.95)\n",
    "        self.conv_activation = tf.keras.layers.Activation(self.activation)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ECA_layer = ECA() #convolutional attention\n",
    "        self.expand = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(self.length, self.channels_expand), activation = 'linear', bias_axes = 'd')\n",
    "        self.project =tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(self.length, self.channel_size), activation = 'linear', bias_axes = 'd')\n",
    "    def call(self, x, training = None):\n",
    "        skip = x\n",
    "        #print(x.shape)\n",
    "        x = self.expand(x) #dense layer expands time dimension\n",
    "        #print(x.shape)\n",
    "        x = self.glu_layer(x) # gating of input through linear gating unit, 2 halfs, second half = activation of first(=input)\n",
    "        #print('glu',x.shape)\n",
    "        x = self.dwconv(x)\n",
    "        #print('conv filter',x.shape)\n",
    "        x = self.BatchNormalization_layer(x)\n",
    "        #print('batchnorm',x.shape)\n",
    "        x = self.conv_activation(x)\n",
    "        #print('activation f',x.shape)\n",
    "        x = self.ECA_layer(x) #conv attention\n",
    "        #print('eca',x.shape)\n",
    "        x = self.project(x)\n",
    "        #print(x.shape)\n",
    "        x = self.scale_bias_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = x+skip\n",
    "\n",
    "        residual = x\n",
    "        x = self.ffn(x) # ff + gate\n",
    "        x = self.scale_bias_2(x)\n",
    "        x = self.layer_norm_2(x + residual)\n",
    "        return x\n",
    "\n",
    "\n",
    "class transf1d(tf.keras.layers.Layer):\n",
    "    def __init__(self, inputDim, num_heads, feed_forward_dim, reshape=False):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputDim//num_heads)\n",
    "        #self.ffn = GLUMlp(feed_forward_dim, embed_dim)\n",
    "        self.ffn2 = tf.keras.layers.Dense(feed_forward_dim)\n",
    "        self.reshape1 = Reshape11()\n",
    "        self.reshape2 = Reshape11()\n",
    "        self.reshape = reshape\n",
    "    def call(self, x, training = None):\n",
    "        residual = x\n",
    "        x = self.att(x,x)\n",
    "        x = x + residual\n",
    "        if self.reshape:\n",
    "            x = self.reshape1(x)\n",
    "        #x = self.ffn(x)\n",
    "        x = self.ffn2(x)\n",
    "        #x = self.reshape2()(x)\n",
    "        return x\n",
    "class att1d(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "    def call(self, x, training = None):\n",
    "        residual = x\n",
    "        x = self.att(x,x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "    \n",
    "class custConv1dDepthwise(tf.keras.layers.Layer):\n",
    "    def __init__(self, arr, meanInit=True):\n",
    "        super().__init__()\n",
    "        self.arr = arr\n",
    "        self.maxSlidingWindow = max(self.arr)\n",
    "        if meanInit:\n",
    "            self.convArr = [tf.keras.layers.DepthwiseConv1D(kernel_size=kernelS,strides=1,padding='valid', depth_multiplier=1,activation='linear', depthwise_initializer=tf.keras.initializers.Constant(1.0 / kernelS)) for kernelS in self.arr]\n",
    "        else:\n",
    "            self.convArr = [tf.keras.layers.DepthwiseConv1D(kernel_size=kernelS,strides=1,padding='valid', depth_multiplier=1) for kernelS in self.arr]\n",
    "    def call(self, x, training = None):\n",
    "        #print(x.shape)\n",
    "        outDim = x.shape[1] - self.maxSlidingWindow +1\n",
    "        out=[]\n",
    "        for i in range(len(self.arr)):  \n",
    "            #print(x.shape)\n",
    "            x0=self.convArr[i](x)\n",
    "            thisOutDim = x.shape[1] - self.arr[i] +1 \n",
    "            startIdx = int((thisOutDim - outDim)/2)\n",
    "            x0 = x0[:,startIdx:startIdx+outDim,:]\n",
    "            out.append(x0)\n",
    "        x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "        #print('out',x.shape)\n",
    "        return x\n",
    "    \n",
    "class custConv1d(tf.keras.layers.Layer):\n",
    "    def __init__(self, arr, meanInit=True, filters=8):\n",
    "        super().__init__()\n",
    "        self.arr = arr\n",
    "        self.maxSlidingWindow = max(self.arr)\n",
    "        if meanInit:\n",
    "            self.convArr = [tf.keras.layers.Conv1D(filters=filters, kernel_size=(kernelS), padding='valid', kernel_initializer=tf.keras.initializers.Constant(1.0 / kernelS)) for kernelS in self.arr]\n",
    "        else:\n",
    "            self.convArr = [tf.keras.layers.Conv1D(filters=filters, kernel_size=(kernelS), padding='valid') for kernelS in self.arr]\n",
    "    def call(self, x, training = None):\n",
    "        outDim = x.shape[-2] - self.maxSlidingWindow +1\n",
    "        out=[]\n",
    "        for i in range(len(self.arr)):  \n",
    "            x0=self.convArr[i](x)\n",
    "            thisOutDim = x.shape[-2] - self.arr[i] +1 \n",
    "            startIdx = int((thisOutDim - outDim)/2)\n",
    "            x0 = x0[:,startIdx:startIdx+outDim,:]\n",
    "            out.append(x0)\n",
    "        x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = 225\n",
    "representations = 13\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "\n",
    "class conv1dInfoFlow(tf.keras.layers.Layer):\n",
    "    def __init__(self, infoFlow, filters):\n",
    "        super().__init__()\n",
    "        self.numFieldsInfoFlow =infoFlow\n",
    "        self.filters = filters \n",
    "        self.att = tf.keras.layers.Conv2D(kernel_size=(1,self.numFieldsInfoFlow ),strides=1,padding='valid',filters=self.filters)\n",
    "        self.startConv = int(self.numFieldsInfoFlow/2)\n",
    "        self.endConv = 283 - self.startConv\n",
    "    def call(self, x, training = None):\n",
    "        #print(x.shape)\n",
    "        # residual connection to allow info flow between neighboring wavelenghts -> padding makes problems\n",
    "        # expect shape to be samples,time, wavelength, embeddings/filters\n",
    "        x0 = x[:,:,0:self.startConv,:]\n",
    "        x1 = x[:,:,self.startConv:self.endConv,:]\n",
    "        x2 = x[:,:,self.endConv:283,:]\n",
    "        #print(x0.shape, x1.shape, x2.shape)\n",
    "\n",
    "        xConv = self.att(x)\n",
    "        #print(xConv.shape)\n",
    "        x1 = (x1 + xConv)/2\n",
    "        \n",
    "        x = tf.keras.layers.Concatenate(axis=2)([x0,x1,x2])\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "    \n",
    "class conv1dInfoFlowPad(tf.keras.layers.Layer):\n",
    "    def __init__(self, infoFlow, filters):\n",
    "        super().__init__()\n",
    "        self.numFieldsInfoFlow =infoFlow\n",
    "        self.filters = filters \n",
    "        self.att = tf.keras.layers.Conv2D(kernel_size=(1,self.numFieldsInfoFlow ),strides=1,padding='valid',filters=self.filters)\n",
    "        self.startConv = int(self.numFieldsInfoFlow/2)\n",
    "    def call(self, x, training = None):\n",
    "        #print(x.shape)\n",
    "        # residual connection to allow info flow between neighboring wavelenghts -> padding makes problems\n",
    "        # expect shape to be samples,time, wavelength, embeddings/filters\n",
    "        x0 = x[:,:,0:1,:]\n",
    "        x2 = x[:,:,282:283,:]\n",
    "        # padd with first and last value\n",
    "        out=[]\n",
    "        for i in range(self.startConv):\n",
    "            out.append(x0)\n",
    "        out.append(x)\n",
    "        for i in range(self.startConv):\n",
    "            out.append(x2)\n",
    "\n",
    "        x = tf.keras.layers.Concatenate(axis=2)(out)\n",
    "        x = self.att(x)\n",
    "        return x\n",
    "\n",
    "def cnn1():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,0]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    for i in range(3): \n",
    "        print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        #x = tf.keras.layers.DepthwiseConv1D(kernel_size=50,strides=1,padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(kernel_size=(50,1),strides=1,padding='valid',filters=8*(i+1))(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = Reshape11()(x[:,:,:,0])\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(100)(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    \n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model\n",
    "\n",
    "def cnn2():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,0]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    for i in range(3): \n",
    "        print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        #x = tf.keras.layers.DepthwiseConv1D(kernel_size=50,strides=1,padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(kernel_size=(50,1),strides=1,padding='valid',filters=8*(i+1))(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = tf.keras.layers.Dense(283)(x[:,:,:,0])\n",
    "    x = Reshape11()(x)\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(100)(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    \n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model\n",
    "\n",
    "#overfitts\n",
    "def cnn3():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,0]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    for i in range(2): \n",
    "        #print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        #x = tf.keras.layers.DepthwiseConv1D(kernel_size=50,strides=1,padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(kernel_size=(20,1),strides=1,padding='same',filters=8*(i+1))(x)\n",
    "        x = tf.keras.layers.AveragePooling2D((2,1))(x)\n",
    "        x = tf.keras.layers.Dense(10)(x)\n",
    "    x = tf.keras.layers.Conv2D(kernel_size=(5,5),strides=1,padding='same',filters=8)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(283*2)(x)\n",
    "    \n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model\n",
    "\n",
    "# still overfitts\n",
    "def cnn4():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,0]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    for i in range(2): \n",
    "        #print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        #x = tf.keras.layers.DepthwiseConv1D(kernel_size=50,strides=1,padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(kernel_size=(20,1),strides=1,padding='same',filters=8*(i+1))(x)\n",
    "        x = tf.keras.layers.AveragePooling2D((2,1))(x)\n",
    "        x = tf.keras.layers.Dense(10)(x)\n",
    "    x = tf.keras.layers.Conv2D(kernel_size=(5,5),strides=1,padding='same',filters=8)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    #x = x[:,:,:,0]\n",
    "    x = reduce11()(x)\n",
    "    #x = tf.keras.layers.Dense(283)(x)\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    x = tf.keras.layers.Dense(2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(10)(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)#[:,:,0])\n",
    "    return model\n",
    "\n",
    "def cnn5():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1:13]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    filters=2\n",
    "    for i in range(4): \n",
    "        #print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        #x = tf.keras.layers.DepthwiseConv1D(kernel_size=50,strides=1,padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(kernel_size=(25,1),strides=1,padding='valid',filters=filters*(i+1))(x)\n",
    "        #x = tf.keras.layers.AveragePooling2D((2,1))(x)\n",
    "        #x = tf.keras.layers.Dense(10)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(20,1),strides=1,padding='valid',filters=filters)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(15,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(10,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    x = tf.keras.layers.Conv2D(kernel_size=(10,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "\n",
    "    #x = conv1dInfoFlowPad(5, filters)(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    #x = x[:,:,:,0]\n",
    "    #x = Reshape4()(x)\n",
    "    x = reduce11()(x)\n",
    "\n",
    "    #x0 = tf.keras.layers.Dense(283)(x)\n",
    "    #x = x+x0\n",
    "    #x0 = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding='same', depth_multiplier=1,activation='relu')(x) #merge info betwee\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = transf1d(inputDim=283, num_heads=4, feed_forward_dim=29)(x) #better\n",
    "    \n",
    "    #x = tf.keras.layers.Dense(50)(x)\n",
    "    #x = tf.keras.layers.Dense(2)(x)\n",
    "    if 0:\n",
    "        # can't fit small dataset, keeps at 57/57\n",
    "        x = tf.keras.layers.DepthwiseConv1D(kernel_size=2,strides=1,padding='valid', depth_multiplier=1,activation='linear')(x)\n",
    "        x_pred = x[:,0,:]\n",
    "        #x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    if 0:\n",
    "        # overfitts\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "\n",
    "    if 0:\n",
    "        # overfitts\n",
    "        x = Reshape11()(x)\n",
    "        x = tf.keras.layers.Dense(1)(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "\n",
    "    if 1:\n",
    "        # overfitts\n",
    "        #x = Reshape11()(x)\n",
    "        #x = tf.keras.layers.Dense(400, activation='relu')(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "        x_pred = x[:,:,0]\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)#[:,:,0])\n",
    "    return model\n",
    "#already better than single pred!!! \n",
    "representations = 14\n",
    "def cnn6():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,1:13]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    filters=2\n",
    "    for i in range(3): \n",
    "        #print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        x = tf.keras.layers.DepthwiseConv2D(kernel_size=(50,1),strides=(1,1),padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        #x = tf.keras.layers.Conv2D(kernel_size=(25,1),strides=1,padding='valid',filters=filters*(i+1))(x)\n",
    "        #x = tf.keras.layers.AveragePooling2D((2,1))(x)\n",
    "        #x = tf.keras.layers.Dense(10-i*2)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(20,1),strides=1,padding='valid',filters=filters)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(15,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(10,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(10,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = conv1dInfoFlowPad(5, filters=1)(x)\n",
    "\n",
    "    #x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    #x = x[:,:,:,0]\n",
    "    #x = Reshape4()(x)\n",
    "    x = reduce11()(x)\n",
    "\n",
    "    #x0 = tf.keras.layers.Dense(283)(x)\n",
    "    #x = x+x0\n",
    "    #x0 = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding='same', depth_multiplier=1,activation='relu')(x) #merge info betwee\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    #x = transf1d(inputDim=283, num_heads=4, feed_forward_dim=29)(x) #better\n",
    "\n",
    "    x = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_pred = x[:,:,0]\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)#[:,:,0])\n",
    "    return model\n",
    "\n",
    "def cnn7():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,1:13]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    filters=2\n",
    "    for i in range(3): \n",
    "        #print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        x = tf.keras.layers.DepthwiseConv2D(kernel_size=(50,1),strides=(1,1),padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        #x = tf.keras.layers.Conv2D(kernel_size=(25,1),strides=1,padding='valid',filters=filters*(i+1))(x)\n",
    "        #x = tf.keras.layers.AveragePooling2D((2,1))(x)\n",
    "        #x = tf.keras.layers.Dense(10-i*2)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(20,1),strides=1,padding='valid',filters=filters)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(15,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(10,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    #x = tf.keras.layers.Conv2D(kernel_size=(10,1),strides=1,padding='valid',filters=filters*1)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = conv1dInfoFlowPad(5, filters=1)(x)\n",
    "\n",
    "    #x = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    #x = x[:,:,:,0]\n",
    "    #x = Reshape4()(x)\n",
    "    x = reduce11()(x)\n",
    "\n",
    "    #x0 = tf.keras.layers.Dense(283)(x)\n",
    "    #x = x+x0\n",
    "    #x0 = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding='same', depth_multiplier=1,activation='relu')(x) #merge info betwee\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    #x = transf1d(inputDim=283, num_heads=4, feed_forward_dim=29)(x) #better\n",
    "\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_var = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    #x_pred = x[:,:,0]\n",
    "    x = Reshape2()(x_pred, x_var)\n",
    "    model = tf.keras.Model(inp, x)#[:,:,0])\n",
    "    return model\n",
    "\n",
    "def cnn8():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp#[:,:,:,1:13]\n",
    "    timeP = timepoints\n",
    "    kernelS = 20\n",
    "    filters=2\n",
    "    for i in range(3): \n",
    "        #print(x.shape)      \n",
    "        #x = tf.keras.layers.Conv1D(filters=283, kernel_size=(20), padding='valid')(x)\n",
    "        #x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "        x = tf.keras.layers.DepthwiseConv2D(kernel_size=(50,1),strides=(1,1),padding='valid', depth_multiplier=1,activation='relu')(x)\n",
    "        #x = tf.keras.layers.Conv2D(kernel_size=(25,1),strides=1,padding='valid',filters=filters*(i+1))(x)\n",
    "    #x = tf.keras.layers.Dense(1)(x)\n",
    "    x = conv1dInfoFlowPad(5, filters=1)(x)\n",
    "    #x = conv1dInfoFlowPad(5, filters=1)(x)\n",
    "    x = reduce11()(x)\n",
    "    x = Reshape11()(x)\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(79)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_var = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    #x_pred = x[:,:,0]\n",
    "    x = Reshape2()(x_pred, x_var)\n",
    "    model = tf.keras.Model(inp, x)#[:,:,0])\n",
    "    return model\n",
    "#model = cnnDepthwise() \n",
    "#model = cnn2D() \n",
    "#model= squeezeformer()\n",
    "#model = fcn() \n",
    "#model = cnn1() \n",
    "#model = transformer()\n",
    "#model = cnnAttentin()\n",
    "#model = singleWL()\n",
    "#modelPre=cnn6()\n",
    "model = cnn8()\n",
    "#model=cnn5converges()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 6# len(model.layers)\n",
    "for i,layer in enumerate(model.layers):\n",
    "    if i<n_layers:\n",
    "        layer.set_weights(modelPre.layers[i].get_weights())\n",
    "\n",
    "#newWeights = model.layers[n_layers-1].get_weights()\n",
    "#newWeights[0][:,0] = modelPre.layers[n_layers-1].get_weights()[0][:,0]\n",
    "#newWeights[1][0] = modelPre.layers[n_layers-1].get_weights()[1][0]\n",
    "#model.layers[n_layers-1].set_weights(newWeights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPre.layers, model.layers, model.layers[1].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPre.load_weights('full_model_con6_0025_0019.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('full_model_con7_0026_0028_varTrained.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = iter(train_dataset)\n",
    "batch=next(batch_iter)\n",
    "batch1=next(batch_iter)\n",
    "#batch2=next(batch_iter)\n",
    "out = model(batch[0])\n",
    "dataset_iterator = iter(test_dataset)\n",
    "test_batch1 = next(dataset_iterator)\n",
    "#test_batch2 = next(dataset_iterator)\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseVal(y_trueScaled, y_pred):\n",
    "    lossPred =tf.square(y_pred[:, :,0] - y_trueScaled)\n",
    "    rmLossLog = tf.reduce_mean(lossPred)\n",
    "    return rmLossLog\n",
    "def maeVal(y_trueScaled, y_pred):\n",
    "    lossPred =tf.abs(y_pred[:, :,0] - y_trueScaled)\n",
    "    rmLossLog = tf.reduce_mean(lossPred)\n",
    "    return rmLossLog\n",
    "\n",
    "def val_mse(y_trueScaled, y_pred):\n",
    "    logConfidence = tf.stop_gradient(tf.math.exp(y_pred[:, :,1])) # logSigma = log(sigma / maxVal)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred =tf.square(y_pred[:, :,0] - y_trueScaled)\n",
    "    loss_2 = tf.square(lossPred-(logConfidence))\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLoss = tf.reduce_mean(lossPred)\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    combinedLoss = rmLoss #+ rmLossLog\n",
    "    return combinedLoss\n",
    "\n",
    "def logL(y_trueScaled, y_pred):\n",
    "    val = tf.stop_gradient(y_pred[:, :,0])\n",
    "    var = y_pred[:, :,1]\n",
    "    logConfidence = tf.math.exp(var) # logSigma = log(sigma / maxVal)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, var + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred = tf.stop_gradient(tf.square(val - y_trueScaled))\n",
    "    loss_2 = tf.abs(lossPred-logConfidence)\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    return rmLossLog\n",
    "\n",
    "def comb_mse(y_trueScaled, y_pred):\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / maxVal)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred =tf.square((y_pred[:, :,0]) - y_trueScaled)\n",
    "    loss_2 = tf.square(lossPred-(logConfidence))\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLoss = tf.reduce_mean(lossPred)\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    combinedLoss = rmLoss + rmLossLog*1e-4\n",
    "    return combinedLoss\n",
    "\n",
    "def comb_mae(y_trueScaled, y_pred):\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / maxVal)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred =tf.square((y_pred[:, :,0]) - y_trueScaled)\n",
    "    lossMae =tf.abs((y_pred[:, :,0]) - y_trueScaled)\n",
    "    loss_2 = tf.abs(lossPred-(logConfidence))\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLoss = tf.reduce_mean(lossMae)\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    combinedLoss = rmLoss + rmLossLog#*1e-4\n",
    "    return combinedLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/deviations_model-{epoch:02d}.weights.h5\",\n",
    "    save_weights_only=True,  # Set to False if you want to save the entire model\n",
    "    save_freq=300 * 4,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-6)\n",
    "model.compile(#loss='mse'\n",
    "              loss=maeVal#comb_mae#val_mse#onlyVar_mse#maeVal#\n",
    "              #loss='mse'#maeSingleWL#'mae'            \n",
    "              #,metrics=[maeVal,mseVal,logL]\n",
    "              #, optimizer=optimizer\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(#train_dataset, \n",
    "                    batch[0],batch[1], #verbose=2,\n",
    "                    #small_dataset,\n",
    "                    #validation_data=test_dataset,\n",
    "                    validation_data=test_batch1,\n",
    "                    epochs=400, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )\n",
    "\n",
    "# batch normalization essential for gradient to travel downstream!\n",
    "# with batch of 12 we converge well to mae ~3.3, mse 57 after ~3000 epochs\n",
    "\n",
    "# loss: 0.0455 - val_loss: 0.0460\n",
    "# base loss: 0.0254 - val_loss: 0.0259\n",
    "# 11 for crossflow loss: 0.0306 - val_loss: 0.0313\n",
    "# 3  for crossflow loss: 0.0281 - val_loss: 0.0292\n",
    "# 7  for crossflow loss: 0.0282 - val_loss: 0.0279\n",
    "\n",
    "# for no dense layer before crossflow: loss: 0.0315 - val_loss: 0.0334 (after 400 epochs) , loss: 0.0266 - val_loss: 0.0290 (after 1200 epochs), loss: 0.0252 - val_loss: 0.0245 (1999 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(#train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    small_dataset,\n",
    "                    #validation_data=test_dataset,\n",
    "                    validation_data=test_batch1,\n",
    "                    epochs=800, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('deviationModelCnn56_5_75.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = model.predict(normData)\n",
    "def calcStatsBase(b, plot=True, display=False):\n",
    "    outputs = model.predict(b[0])#[:,:,0]\n",
    "    print(outputs.shape)\n",
    "    pred = outputs[:,0:283]\n",
    "\n",
    "    if display:\n",
    "        print(pred[0:10:,0:2,0], b[1][0:10:,0:2])\n",
    "        print(pred[0:10:,0:2,0]*maxLabels, b[1][0:10:,0:2]*maxLabels)\n",
    "\n",
    "    mae = np.sum(np.abs(pred[:,:]*maxTrainLabels-b[1]*maxTrainLabels)) / pred.shape[0] / pred.shape[1]\n",
    "    mse = np.sum(np.abs(pred[:,:]*maxTrainLabels-b[1]*maxTrainLabels)**2) / pred.shape[0] / pred.shape[1]\n",
    "    print('mae',mae,'mse', mse)\n",
    "    mae = np.sum(np.abs(pred[:,:]-b[1])) / pred.shape[0] / pred.shape[1]\n",
    "    mse = np.sum(np.abs(pred[:,:]-b[1])**2) / pred.shape[0] / pred.shape[1]\n",
    "    print('mae',mae,'mse', mse)\n",
    "    for i in range(outputs.shape[0]):\n",
    "        mae = np.sum(np.abs(pred[i,:]-b[1][i,:])) / pred.shape[1]\n",
    "        mse = np.sum(np.abs(pred[i,:]-b[1][i,:])**2) / pred.shape[1]\n",
    "        #print('row',i,'mae',mae,'mse', mse)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    m = min(100, batch[0].shape[0])\n",
    "    for i in range(m): #range(12):# \n",
    "        fig.add_trace(go.Scatter(y=b[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3),visible='legendonly'))\n",
    "        fig.add_trace(go.Scatter(y=pred[i,:],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def calcStatsLogLoss(b, plot=True, display=False):\n",
    "    outputs = model.predict(b[0])#[:,:,0]\n",
    "    print(outputs.shape)\n",
    "    pred = outputs[:,:,0]\n",
    "\n",
    "    if display:\n",
    "        print(pred[0:10:,0:2,0], b[1][0:10:,0:2])\n",
    "        print(pred[0:10:,0:2,0]*maxLabels, b[1][0:10:,0:2]*maxLabels)\n",
    "\n",
    "    mae = np.sum(np.abs(pred[:,:]*maxTrainLabels-b[1]*maxTrainLabels)) / pred.shape[0] / pred.shape[1]\n",
    "    mse = np.sum(np.abs(pred[:,:]*maxTrainLabels-b[1]*maxTrainLabels)**2) / pred.shape[0] / pred.shape[1]\n",
    "    print('mae',mae,'mse', mse)\n",
    "    mae = np.sum(np.abs(pred[:,:]-b[1])) / pred.shape[0] / pred.shape[1]\n",
    "    mse = np.sum(np.abs(pred[:,:]-b[1])**2) / pred.shape[0] / pred.shape[1]\n",
    "    print('mae',mae,'mse', mse)\n",
    "    for i in range(outputs.shape[0]):\n",
    "        mae = np.sum(np.abs(pred[i,:]-b[1][i,:])) / pred.shape[1]\n",
    "        mse = np.sum(np.abs(pred[i,:]-b[1][i,:])**2) / pred.shape[1]\n",
    "        #print('row',i,'mae',mae,'mse', mse)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    m = min(100, batch[0].shape[0])\n",
    "    for i in range(m): #range(12):# \n",
    "        fig.add_trace(go.Scatter(y=b[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3),visible='legendonly'))\n",
    "        fig.add_trace(go.Scatter(y=pred[i,:],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcStatsLogLoss(next(iter(small_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcStatsLogLoss(test_batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(batch[0])\n",
    "outputs = pred[:,:,0]\n",
    "\n",
    "mae = np.sum(np.abs(outputs*100*maxLabels-batch[1][:,:]*100*maxLabels)) / outputs.shape[0]\n",
    "mse = np.sum(np.abs(outputs*100*maxLabels-batch[1][:,:]*100*maxLabels)**2) / outputs.shape[0]\n",
    "mae1 = np.sum(np.abs(outputs*maxLabels-batch[1][:,:]*maxLabels)) / outputs.shape[0]\n",
    "mse1 = np.sum(np.abs(outputs*maxLabels-batch[1][:,:]*maxLabels)**2) / outputs.shape[0]\n",
    "mae,mae1,tf.exp(pred[:,:,1]),outputs, batch[1]#[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_batch1[0])[:,:,0]\n",
    "\n",
    "mae = np.sum(np.abs(outputs*100*maxLabels-test_batch1[1][:,:]*100*maxLabels)) / outputs.shape[0]\n",
    "mse = np.sum(np.abs(outputs*100*maxLabels-test_batch1[1][:,:]*100*maxLabels)**2) / outputs.shape[0]\n",
    "mae1 = np.sum(np.abs(outputs*maxLabels-test_batch1[1][:,:]*maxLabels)) / outputs.shape[0]\n",
    "mse1 = np.sum(np.abs(outputs*maxLabels-test_batch1[1][:,:]*maxLabels)**2) / outputs.shape[0]\n",
    "mae,mae1, #outputs, batch[1][:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize input data\n",
    "fig = go.Figure()\n",
    "for i in range(64):\n",
    "    fig.add_trace(go.Scatter(y=batch[0][i,:,0,1],mode='markers',name=f'{i}',marker=dict(size=3)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(batch[0])#[:,:,0]\n",
    "fig = go.Figure()\n",
    "i = 0\n",
    "for i in range(3):\n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=out[i,:],mode='markers',name=f'pred',marker=dict(size=3)))\n",
    "fig.show()\n",
    "\n",
    "plt.hist(batch[1][i,:], bins=30, edgecolor='blue',alpha=0.7)\n",
    "plt.hist(out[i,:], bins=30, edgecolor='red',alpha=0.7)\n",
    "plt.title('Histogram of Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(test_batch1[0])#[:,:,0]\n",
    "fig = go.Figure()\n",
    "i = 0\n",
    "for i in range(3):\n",
    "    fig.add_trace(go.Scatter(y=test_batch1[1][i,:],mode='markers',name=f'gt',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=out[i,:],mode='markers',name=f'pred',marker=dict(size=3)))\n",
    "fig.show()\n",
    "\n",
    "plt.hist(test_batch1[1][i,:], bins=30, edgecolor='blue',alpha=0.7)\n",
    "plt.hist(out[i,:], bins=30, edgecolor='red',alpha=0.7)\n",
    "plt.title('Histogram of Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchid = 2\n",
    "wl = 0\n",
    "x = batch[0][batchid:batchid+1,:,:,1]  \n",
    "print(x.shape)  \n",
    "fig = plt.figure()\n",
    "plt.plot(x[0,:,wl])\n",
    "plt.title(f'input')\n",
    "plt.show()\n",
    "\n",
    "for layers in range(len(model.layers)-1):\n",
    "    x = model.layers[layers+1](x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    if len(x.shape)>=3:\n",
    "        if len(x.shape) == 4:\n",
    "            for i in range(x.shape[-1]):\n",
    "                fig = plt.figure()\n",
    "                plt.plot(x[0,:,wl,i])\n",
    "                plt.title(f'layer {layers+1}')\n",
    "                plt.show()\n",
    "        else:\n",
    "            fig = plt.figure()\n",
    "            if x.shape[2] == 283:\n",
    "                plt.plot(x[0,:,wl])\n",
    "            else:\n",
    "                if x.shape[2] == 1:\n",
    "                    print(x[:,wl,:], batch[1][batchid:batchid+1,0:1])\n",
    "                else:\n",
    "                    plt.plot(x[0,wl,:])\n",
    "            plt.title(f'layer {layers+1}')\n",
    "            plt.show()\n",
    "    else:\n",
    "        if x.shape[1] >1:\n",
    "            fig=plt.figure()\n",
    "            plt.plot(x[0])\n",
    "            plt.title(f'layer{layers+1}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(x, batch[1][batchid:batchid+1,0:1])\n",
    "#print(model.layers[2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchid = 0\n",
    "wl = 282\n",
    "x = batch[0][batchid:batchid+1,:,:,:]  \n",
    "print(x.shape)  \n",
    "fig = plt.figure()\n",
    "plt.plot(x[0,:,wl,1])\n",
    "plt.title(f'input')\n",
    "plt.show()\n",
    "\n",
    "x = x[:,:,:,1:13]\n",
    "for layers in range(len(model.layers)-1):\n",
    "    print(x.shape)\n",
    "    x = model.layers[layers+1](x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    if len(x.shape) == 4:\n",
    "        fig = plt.figure()\n",
    "        for i in range(x.shape[3]):\n",
    "            \n",
    "            plt.plot(np.reshape(x[batchid,:,wl,i],(-1)))\n",
    "            plt.title(f'layer {layers+1}')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.plot(x[0,:])\n",
    "        plt.title(f'layer {layers+1}')\n",
    "        plt.show()\n",
    "#print(model.layers[2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(batch[0][batchid,:,wl,1]  )\n",
    "plt.title(f'input')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][batchid:batchid+1,:,wl,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[1])\n",
    "for i in range(4):\n",
    "    fig=plt.figure()\n",
    "    plt.plot(x[i,:,0])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

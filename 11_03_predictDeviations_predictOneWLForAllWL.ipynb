{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.1):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "meanLabels = np.mean(labelDf.mean())\n",
    "stdLabels = np.std(labelDf.std())\n",
    "maxLabels = np.max(labelDf.max())\n",
    "minLabels = np.min(labelDf.min())\n",
    "\n",
    "trainLabels = labelDf.loc[[int(star) for star in train_stars]]\n",
    "meanTrainLabels = np.mean(trainLabels.mean())\n",
    "stdTrainLabels = np.std(trainLabels.std())\n",
    "maxTrainLabels = np.max(trainLabels.max())\n",
    "minTrainLabels = np.min(trainLabels.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTrainLabels)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrain(train_stars):\n",
    "    i = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,1]\n",
    "            x = x / np.max(x,axis=0)\n",
    "            if i ==0:\n",
    "                mean = np.mean(x,axis=(0))\n",
    "                sumS = np.sum(x**2,axis=0)\n",
    "            else:\n",
    "                mean = mean + np.mean(x, axis=(0))\n",
    "                sumS += np.sum(x**2,axis=0)\n",
    "            i=i+1\n",
    "    meanTrain = mean / i\n",
    "    stdTrain = np.sqrt(sumS / (i*x.shape[0]) - meanTrain**2)    \n",
    "    return meanTrain, stdTrain\n",
    "meanTrain, stdTrain = calcMeanAndStdOfTrain(train_stars)\n",
    "\n",
    "def normalize_over_train(features, labels):\n",
    "    #features = (features - meanTrain) / (stdTrain + 1e-6)\n",
    "    features = (features) / (stdTrain + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "def calcRollingMeans(windows, features):\n",
    "    arr=[features]\n",
    "    for window in windows:\n",
    "        padded_data = np.pad(features, ((window, 0),(0,0)), mode='edge')\n",
    "        cumsum = np.cumsum(padded_data, axis=0)\n",
    "        result = (cumsum[window:,:] - cumsum[:-window,:]) / window\n",
    "        arr.append(result)\n",
    "\n",
    "        # calc diff of rolling mean & compensate rolling mean with it\n",
    "        diff = np.diff(result,axis=0)\n",
    "        mean_lin_slope = np.mean(diff[window:,:],axis=0)\n",
    "        accumulatedSlopes = np.cumsum(np.ones_like(result)*mean_lin_slope, axis=0)\n",
    "        compensatedSignal = result - accumulatedSlopes\n",
    "        arr.append(compensatedSignal)\n",
    "    \n",
    "        # add diff shape\n",
    "        diffCorr = np.zeros_like(result)\n",
    "        diffCorr[1:,:] = diff\n",
    "        arr.append(diffCorr)\n",
    "    return np.stack(arr,axis=-1)\n",
    "\n",
    "# normalize over time per samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrainPerStar(x):\n",
    "    mean = np.mean(x,axis=(0))\n",
    "    sumS = np.sum(x**2,axis=0)\n",
    "    stdTrain = np.sqrt(sumS / (x.shape[0]) - mean**2)    \n",
    "    return mean, stdTrain\n",
    "def normalize_per_sample(features, labels):\n",
    "    m,s = calcMeanAndStdOfTrainPerStar(features)\n",
    "    features = (features) / (s + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,1]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            meanL = np.mean(labels)\n",
    "            stdL = np.std(labels)\n",
    "            labels = (labels-meanL)/ stdL #*100\n",
    "            features = np.reshape(features,(-1,25,283))\n",
    "            features = np.mean(features,axis=1)\n",
    "            maxValpWL = np.max(features,axis=0)\n",
    "            features = features / maxValpWL -1\n",
    "            #features, labels = normalize_per_sample(features,labels)\n",
    "            features, labels = normalize_over_train(features,labels)\n",
    "            features = calcRollingMeans([30,50,80,100],features)\n",
    "\n",
    "            features = np.transpose(features, (1, 0, 2))\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "\n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list), reshuffle_each_iteration=True)\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([283,225,13])), tf.ensure_shape(y, tf.TensorShape(283)))) #5625\n",
    "    dataset = dataset.unbatch()\n",
    "    dataset = dataset.map(lambda x, y: (x, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list)*283, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline 2 - experiments\n",
    "- calc diff to mean light curve\n",
    "- decompose input by mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.1):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "meanLabels = np.mean(labelDf.mean())\n",
    "stdLabels = np.std(labelDf.std())\n",
    "maxLabels = np.max(labelDf.max())\n",
    "minLabels = np.min(labelDf.min())\n",
    "\n",
    "trainLabels = labelDf.loc[[int(star) for star in train_stars]]\n",
    "meanTrainLabels = np.mean(trainLabels.mean())\n",
    "stdTrainLabels = np.std(trainLabels.std())\n",
    "maxTrainLabels = np.max(trainLabels.max())\n",
    "minTrainLabels = np.min(trainLabels.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTrainLabels)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def maxOfTrain(train_stars):\n",
    "    maxTrain = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,1]\n",
    "            maxTrain = max(maxTrain, np.max(x))  \n",
    "    return maxTrain\n",
    "maxTrain= maxOfTrain(train_stars)\n",
    "\n",
    "def calcRollingMeans(windows, features):\n",
    "    arr=[features]\n",
    "    for window in windows:\n",
    "        padded_data = np.pad(features, ((window, 0),(0,0)), mode='edge')\n",
    "        cumsum = np.cumsum(padded_data, axis=0)\n",
    "        result = (cumsum[window:,:] - cumsum[:-window,:]) / window\n",
    "        arr.append(result)\n",
    "\n",
    "        # calc diff of rolling mean & compensate rolling mean with it\n",
    "        diff = np.diff(result,axis=0)\n",
    "        mean_lin_slope = np.mean(diff[window:,:],axis=0)\n",
    "        accumulatedSlopes = np.cumsum(np.ones_like(result)*mean_lin_slope, axis=0)\n",
    "        compensatedSignal = result - accumulatedSlopes\n",
    "        arr.append(compensatedSignal)\n",
    "    \n",
    "        # add diff shape\n",
    "        diffCorr = np.zeros_like(result)\n",
    "        diffCorr[1:,:] = diff\n",
    "        arr.append(diffCorr)\n",
    "    return np.stack(arr,axis=-1)\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            f = data['a'][0,:,0:283,1]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            meanL = np.mean(labels)\n",
    "            stdL = np.std(labels)\n",
    "            #labels = (labels-meanL)/ stdL #*100\n",
    "            f = np.reshape(f,(-1,25,283))\n",
    "            f = np.mean(f,axis=1)\n",
    "            maxValpWL = np.mean(f,axis=0) #bring all values to a similar scale -> differences are now comparable\n",
    "            f = 100*(f / maxValpWL) # map to [-100*diff,0]\n",
    "            featureMax = np.reshape(np.ones_like(f) * 100/maxValpWL, (225,283,1))\n",
    "            # my network can esitmate the -100*diff and has 100/maxVal\n",
    "            # lambda=(x_top-x_bottom)/x_top = x_diff / x_top = (x_diff/maxVal) / (x_top/maxVal)\n",
    "            #       = 1 - x_bottom/x_top  -> scalieren \n",
    "\n",
    "            # x_bottom/maxVal - 1, x_top/maxVal - 1, \n",
    "\n",
    "            f = calcRollingMeans([30,50,80,100],f)\n",
    "            f = np.concatenate([f, featureMax], axis = 2) #give info about magnitude as well\n",
    "            f = np.transpose(f, (1, 0, 2))\n",
    "            return f, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "\n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list), reshuffle_each_iteration=True)\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([283,225,14])), tf.ensure_shape(y, tf.TensorShape(283)))) #5625\n",
    "    dataset = dataset.unbatch()\n",
    "    dataset = dataset.map(lambda x, y: (x, y))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list)*283, reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 283*4#*4#*10\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train = create_dataset(train_stars[0:4], batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        return x\n",
    "    \n",
    "class Reshape3(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.reshape(x, (None,-1,x.shape[2]))\n",
    "        return x\n",
    "    \n",
    "class reduce(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        mean = tf.expand_dims(mean, axis=-1)\n",
    "        return mean\n",
    "class reduce1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        return mean\n",
    "    \n",
    "class tile(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x,mean):\n",
    "        x = tf.concat([x,mean],axis=-1)\n",
    "        return x\n",
    "    \n",
    "class tile2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x,mean):\n",
    "        x = tf.concat([x,tf.expand_dims(mean,axis=-1)],axis=-2)\n",
    "        return x\n",
    "    \n",
    "class meanOfWavelengths(tf.keras.layers.Layer):\n",
    "    def __init__(self, concat=True,**kwargs):\n",
    "        self.concat=concat\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        m = tf.expand_dims(tf.reduce_mean(x,axis=-1),axis=-1)\n",
    "        x = tf.concat([x,m],axis=-1)\n",
    "        return x if self.concat else m\n",
    "\n",
    "# gated linear unit, splits input in 2 batches, second batch is activation\n",
    "class GLU(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x, mask=None):\n",
    "        x,gate = tf.split(x, 2, axis = -1)\n",
    "        # swish = gate * sigmoid(gate) (sigmoid = between 0..1)\n",
    "        x = x*tf.keras.activations.swish(gate) # use one input as a gate such that the network is able to focus on information\n",
    "        return x\n",
    "\n",
    "class GLUMlp(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim_expand, dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.dim_expand = dim_expand\n",
    "        self.dim = dim\n",
    "        # same operation as dense layer\n",
    "        self.dense_1 = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(None, self.dim_expand), activation = 'linear', bias_axes = 'd')\n",
    "        self.glu_1 = GLU()\n",
    "        self.dense_2 = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(None, self.dim), activation = 'linear', bias_axes = 'd')\n",
    "    def call(self, x, training = False):\n",
    "        #print('glu_input',x.shape)\n",
    "        x = self.dense_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.glu_1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.dense_2(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "\n",
    "class ScaleBias(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, input_shape):\n",
    "        self.scale_bias = tf.keras.layers.EinsumDense(\"abc,c->abc\",output_shape=(None, input_shape[-1]),activation = 'linear', bias_axes = 'c')\n",
    "    def call(self, x, mask=None):\n",
    "        return self.scale_bias(x)\n",
    "\n",
    "#attention gets calculated along first dimension!\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "        self.ffn = GLUMlp(feed_forward_dim, embed_dim)\n",
    "        #self.ffn = tf.keras.layers.Dense(feed_forward_dim)\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6) # normalization by a * (input-mean) /sqrt(var + eps) +b    where a and b are learned, eps is to avoid div/0\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        #self.scale_bias_1 = ScaleBias()\n",
    "        #self.scale_bias_2 = ScaleBias()\n",
    "    def call(self, x, training = None):\n",
    "        residual = x\n",
    "        #print('before att')\n",
    "        x = self.att(x, x)\n",
    "        #x = self.scale_bias_1(x)\n",
    "        x = self.layer_norm_1(x + residual)\n",
    "        #x = x+residual\n",
    "        residual = x\n",
    "        #print('after att')\n",
    "        x = self.ffn(x, training = training)\n",
    "        #print('after glu')\n",
    "        #x = self.scale_bias_2(x)\n",
    "        x = self.layer_norm_2(x + residual)\n",
    "        return x\n",
    "    \n",
    "# is effectively an attention mechanism to allow some columns to be used / turned off\n",
    "# effective channel attention!\n",
    "class ECA(tf.keras.layers.Layer):\n",
    "    # TF implementation from https://www.kaggle.com/code/hoyso48/1st-place-solution-training\n",
    "    def __init__(self, kernel_size=5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.conv = tf.keras.layers.Conv1D(1, kernel_size=kernel_size, strides=1, padding=\"same\", use_bias=False) # only one1D convolution with kernel size\n",
    "    def call(self, inputs):\n",
    "        nn = tf.keras.layers.GlobalAveragePooling1D()(inputs) # works not on batch size, but on next dimension, e.g. batch_size, 60x9 -> works on 60, so output is batch_size x 9\n",
    "        nn = tf.expand_dims(nn, -1) # a,c -> a,c,1\n",
    "        nn = self.conv(nn) # a,c,1 -> a,c,1 (1, because conv is only having 1 filter)\n",
    "        nn = tf.squeeze(nn, -1) # a,c,1 -> a,c\n",
    "        nn = tf.nn.sigmoid(nn) # a,c -> a,c\n",
    "        nn = nn[:,None,:] # a,1,c -> e.g. batch_size,1,9\n",
    "        return inputs * nn # a,1,c * a,b,c applies broadcasting / elementwise multiplication -> turns input on or off column wise\n",
    "\n",
    "\n",
    "class HeadDense(tf.keras.layers.Layer):\n",
    "    def __init__(self, head_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.head_dim = head_dim\n",
    "    def build(self, input_shape):\n",
    "        self.length = input_shape[1]\n",
    "        self.dim = input_shape[2]\n",
    "        self.dense = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(self.length, self.head_dim), activation = 'swish', bias_axes = 'd') #siwsh is causing a self gating\n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "class Conv1DBlockSqueezeformer(tf.keras.layers.Layer):\n",
    "    def __init__(self, channel_size, kernel_size, dilation_rate=1,\n",
    "                 expand_ratio=2, se_ratio=0.25, activation='swish', name=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.channel_size = channel_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.se_ratio = se_ratio\n",
    "        self.activation = activation\n",
    "        self.scale_bias = ScaleBias()\n",
    "        self.glu_layer = GLU()\n",
    "        self.ffn = GLUMlp(channel_size*4, channel_size)\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.scale_bias_1 = ScaleBias()\n",
    "        self.scale_bias_2 = ScaleBias()\n",
    "    def build(self, input_shape):\n",
    "        self.length = input_shape[1]\n",
    "        self.channels_in = input_shape[2]\n",
    "        self.channels_expand = self.channels_in * self.expand_ratio\n",
    "        self.dwconv = tf.keras.layers.DepthwiseConv1D(self.kernel_size,dilation_rate=self.dilation_rate,padding='same',use_bias=False)\n",
    "        self.BatchNormalization_layer = tf.keras.layers.BatchNormalization(momentum=0.95)\n",
    "        self.conv_activation = tf.keras.layers.Activation(self.activation)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ECA_layer = ECA() #convolutional attention\n",
    "        self.expand = tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(self.length, self.channels_expand), activation = 'linear', bias_axes = 'd')\n",
    "        self.project =tf.keras.layers.EinsumDense(\"abc,cd->abd\",output_shape=(self.length, self.channel_size), activation = 'linear', bias_axes = 'd')\n",
    "    def call(self, x, training = None):\n",
    "        skip = x\n",
    "        #print(x.shape)\n",
    "        x = self.expand(x) #dense layer expands time dimension\n",
    "        #print(x.shape)\n",
    "        x = self.glu_layer(x) # gating of input through linear gating unit, 2 halfs, second half = activation of first(=input)\n",
    "        #print('glu',x.shape)\n",
    "        x = self.dwconv(x)\n",
    "        #print('conv filter',x.shape)\n",
    "        x = self.BatchNormalization_layer(x)\n",
    "        #print('batchnorm',x.shape)\n",
    "        x = self.conv_activation(x)\n",
    "        #print('activation f',x.shape)\n",
    "        x = self.ECA_layer(x) #conv attention\n",
    "        #print('eca',x.shape)\n",
    "        x = self.project(x)\n",
    "        #print(x.shape)\n",
    "        x = self.scale_bias_1(x)\n",
    "        #print(x.shape)\n",
    "\n",
    "        x = x+skip\n",
    "\n",
    "        residual = x\n",
    "        x = self.ffn(x) # ff + gate\n",
    "        x = self.scale_bias_2(x)\n",
    "        x = self.layer_norm_2(x + residual)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "timepoints = 225\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "\n",
    "class transf1d(tf.keras.layers.Layer):\n",
    "    def __init__(self, inputDim, num_heads, feed_forward_dim, reshape=True):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=inputDim//num_heads)\n",
    "        #self.ffn = GLUMlp(feed_forward_dim, embed_dim)\n",
    "        self.ffn2 = tf.keras.layers.Dense(feed_forward_dim)\n",
    "        self.reshape1 = Reshape11()\n",
    "        self.reshape2 = Reshape11()\n",
    "        self.reshape = reshape\n",
    "    def call(self, x, training = None):\n",
    "        residual = x\n",
    "        x = self.att(x,x)\n",
    "        x = x + residual\n",
    "        if self.reshape:\n",
    "            x = self.reshape1(x)\n",
    "        #x = self.ffn(x)\n",
    "        x = self.ffn2(x)\n",
    "        #x = self.reshape2()(x)\n",
    "        return x\n",
    "class att1d(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "    def call(self, x, training = None):\n",
    "        residual = x\n",
    "        x = self.att(x,x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "    \n",
    "class custConv1dDepthwise(tf.keras.layers.Layer):\n",
    "    def __init__(self, arr, meanInit=True):\n",
    "        super().__init__()\n",
    "        self.arr = arr\n",
    "        self.maxSlidingWindow = max(self.arr)\n",
    "        if meanInit:\n",
    "            self.convArr = [tf.keras.layers.DepthwiseConv1D(kernel_size=kernelS,strides=1,padding='valid', depth_multiplier=1,activation='linear', depthwise_initializer=tf.keras.initializers.Constant(1.0 / kernelS)) for kernelS in self.arr]\n",
    "        else:\n",
    "            self.convArr = [tf.keras.layers.DepthwiseConv1D(kernel_size=kernelS,strides=1,padding='valid', depth_multiplier=1) for kernelS in self.arr]\n",
    "    def call(self, x, training = None):\n",
    "        #print(x.shape)\n",
    "        outDim = x.shape[1] - self.maxSlidingWindow +1\n",
    "        out=[]\n",
    "        for i in range(len(self.arr)):  \n",
    "            #print(x.shape)\n",
    "            x0=self.convArr[i](x)\n",
    "            thisOutDim = x.shape[1] - self.arr[i] +1 \n",
    "            startIdx = int((thisOutDim - outDim)/2)\n",
    "            x0 = x0[:,startIdx:startIdx+outDim,:]\n",
    "            out.append(x0)\n",
    "        x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "        #print('out',x.shape)\n",
    "        return x\n",
    "    \n",
    "class custConv1d(tf.keras.layers.Layer):\n",
    "    def __init__(self, arr, meanInit=True, filters=8):\n",
    "        super().__init__()\n",
    "        self.arr = arr\n",
    "        self.maxSlidingWindow = max(self.arr)\n",
    "        if meanInit:\n",
    "            self.convArr = [tf.keras.layers.Conv1D(filters=filters, kernel_size=(kernelS), padding='valid', kernel_initializer=tf.keras.initializers.Constant(1.0 / kernelS)) for kernelS in self.arr]\n",
    "        else:\n",
    "            self.convArr = [tf.keras.layers.Conv1D(filters=filters, kernel_size=(kernelS), padding='valid') for kernelS in self.arr]\n",
    "    def call(self, x, training = None):\n",
    "        outDim = x.shape[-2] - self.maxSlidingWindow +1\n",
    "        out=[]\n",
    "        for i in range(len(self.arr)):  \n",
    "            x0=self.convArr[i](x)\n",
    "            thisOutDim = x.shape[-2] - self.arr[i] +1 \n",
    "            startIdx = int((thisOutDim - outDim)/2)\n",
    "            x0 = x0[:,startIdx:startIdx+outDim,:]\n",
    "            out.append(x0)\n",
    "        x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "        return x\n",
    "\n",
    " \n",
    "# Batch norm causes massive difference between train and inference time!!!!\n",
    "# during training mean & std dev of batch are used to normalize\n",
    "# during inference a rolling mean&stdDev is used for normalization\n",
    "def cnn():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths))\n",
    "    x = inp[:,:,0:1]\n",
    "    timeP = timepoints\n",
    "    #x = custConv1d([10,30,50])(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=32)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=64)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=128)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=256)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(500, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "# with layer normalization we don't have so much overfitting, however we can only fit the mean\n",
    "def cnn2():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths))\n",
    "    x = inp[:,:,0:1]\n",
    "    timeP = timepoints\n",
    "    #x = custConv1d([10,30,50])(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=32)(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=64)(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=128)(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=256)(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(500, activation='relu')(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "# can't fit due to vanishing gradients\n",
    "def cnn3():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths))\n",
    "    x = inp[:,:,0:1]\n",
    "    timeP = timepoints\n",
    "    #x = custConv1d([10,30,50])(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=32)(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=32)(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=32)(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = custConv1d([50], meanInit=False, filters=32)(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(500, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "# we can fit the small dataset, but overfit quite heavily\n",
    "def cnn4():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths))\n",
    "    x = inp[:,:,0:1]\n",
    "    timeP = timepoints\n",
    "    x = custConv1d([50], meanInit=False, filters=8)(x)\n",
    "    for i in range(2):\n",
    "        x = custConv1d([50], meanInit=False, filters=32* 2**(i))(x)\n",
    "        #x = tf.keras.layers.MaxPool1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1)(x) # collapse filters to 1 signal -> helps\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    #x = tf.keras.layers.Dense(500, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "def cnn5():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths))\n",
    "    x = inp[:,:,0:1]\n",
    "    timeP = timepoints\n",
    "    x = custConv1d([50], meanInit=True, filters=1)(x)\n",
    "    for i in range(2):\n",
    "        x = custConv1d([50], meanInit=False, filters=8* 2**(i))(x)\n",
    "        #x = tf.keras.layers.MaxPool1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1)(x) # collapse filters to 1 signal -> helps\n",
    "    #x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    #x = tf.keras.layers.Dense(500, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(20, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "# network is capable, but doesn't generalize well -> add more features to it\n",
    "def cnn6():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths))\n",
    "    x = inp[:,:,0:1]\n",
    "    timeP = timepoints\n",
    "    x = custConv1d([50,40,30,20,10], meanInit=True, filters=1)(x)\n",
    "    for i in range(2):\n",
    "        x = custConv1d([5,10,3], meanInit=False, filters=8* 2**(i))(x)\n",
    "        x = tf.keras.layers.MaxPool1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1)(x) # collapse filters to 1 signal -> helps\n",
    "    #x = Reshape11()(x)\n",
    "    #x = GLUMlp(dim_expand=x.shape[2]*2,dim=x.shape[2])(x) # select timepoints\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    #x = tf.keras.layers.Dense(500, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    for _ in range(3):\n",
    "        x = tf.keras.layers.Dense(37, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "embed = 13\n",
    "def cnn7():\n",
    "    inp = tf.keras.Input(shape=(timepoints, embed))\n",
    "    x = inp[:,:,1:13]\n",
    "    x = tf.keras.layers.LayerNormalization()(x)\n",
    "    for i in range(5):\n",
    "        x = custConv1d([40], meanInit=False, filters=64)(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dense(50)(x)\n",
    "        #reduce time dimension\n",
    "        #x = Reshape11()(x)\n",
    "        #x = tf.keras.layers.Dense(x.shape[2]-20)(x)\n",
    "        #x = Reshape11()(x)\n",
    "        #x = tf.keras.layers.MaxPool1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(64)(x) # collapse filters to 1 signal -> helps\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(30)(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    #x = Reshape11()(x)\n",
    "    #x = GLUMlp(dim_expand=x.shape[2]*2,dim=x.shape[2])(x) # select timepoints\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    for i in range(3):\n",
    "        x = tf.keras.layers.Dense(1000, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x = tf.keras.layers.Dense(300, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(30, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    #for _ in range(3):\n",
    "    #    x = tf.keras.layers.Dense(37, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "embed = 13\n",
    "def cnn8():\n",
    "    inp = tf.keras.Input(shape=(timepoints, embed))\n",
    "    x = inp[:,:,1:13]\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "   # for i in range(5):\n",
    "   #     x = custConv1d([5], meanInit=False, filters=10)(x)\n",
    "   # x = custConv1d([5], meanInit=False, filters=5)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    for i in range(1):\n",
    "        x = tf.keras.layers.Dense(700, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "embed = 13\n",
    "def cnn9():\n",
    "    inp = tf.keras.Input(shape=(timepoints, embed))\n",
    "    x = inp#[:,:,1:13]\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    for i in range(3):\n",
    "        x = transf1d(inputDim=225,embed_dim=13, num_heads=4, feed_forward_dim=2*13)(x)\n",
    "    #x = custConv1d([50], meanInit=False, filters=5)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #for i in range(1):\n",
    "    #    x = tf.keras.layers.Dense(700, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "embed = 13\n",
    "def cnn10():\n",
    "    inp = tf.keras.Input(shape=(timepoints, embed))\n",
    "    x = inp#[:,:,1:13]\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    for i in range(3):\n",
    "        x = transf1d(inputDim=225, num_heads=4, feed_forward_dim=2*13, reshape=False)(x)\n",
    "    #x = custConv1d([50], meanInit=False, filters=5)(x)\n",
    "    x = tf.keras.layers.Dense(1)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #for i in range(1):\n",
    "    #    x = tf.keras.layers.Dense(700, activation='relu')(x)\n",
    "    #x = tf.keras.layers.Dense(100)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model\n",
    "\n",
    "def cnn11():\n",
    "    inp = tf.keras.Input(shape=(timepoints, 14))\n",
    "    x = inp[:,:,1:14]\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    out=[]\n",
    "    #for i in range(x.shape[2]):\n",
    "    for i in range(4):\n",
    "    #    x = transf1d(inputDim=225, num_heads=4, feed_forward_dim=2*13, reshape=False)(x)\n",
    "        x = custConv1d([50], meanInit=False, filters=4)(x)\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(x[:,:,i:i+1])\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(y)\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(y)\n",
    "        #out.append(y)\n",
    "    #x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "\n",
    "\n",
    "    #x = custConv1d([50], meanInit=False, filters=1)(x)\n",
    "    x = tf.keras.layers.Dense(2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    #for i in range(1):\n",
    "    #x = tf.keras.layers.Dense(700, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "def cnn12():\n",
    "    inp = tf.keras.Input(shape=(timepoints, 14))\n",
    "    x = inp[:,:,1:14]\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    out=[]\n",
    "    #for i in range(x.shape[2]):\n",
    "    for i in range(4):\n",
    "    #    x = transf1d(inputDim=225, num_heads=4, feed_forward_dim=2*13, reshape=False)(x)\n",
    "        x = custConv1d([50], meanInit=False, filters=4)(x)\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(x[:,:,i:i+1])\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(y)\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(y)\n",
    "        #out.append(y)\n",
    "    #x = tf.keras.layers.Concatenate(axis=-1)(out)\n",
    "\n",
    "    x = custConv1d([20], meanInit=False, filters=4)(x)\n",
    "    x = custConv1d([5], meanInit=False, filters=4)(x)\n",
    "    #x = custConv1d([50], meanInit=False, filters=1)(x)\n",
    "    x = tf.keras.layers.Dense(2)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    #for i in range(1):\n",
    "    #x = tf.keras.layers.Dense(700, activation='relu')(x)\n",
    "    #x = tf.keras.layers.Dense(50)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "def cnn13():\n",
    "    inp = tf.keras.Input(shape=(timepoints, 14))\n",
    "    x = inp[:,:,1:14]\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    out=[]\n",
    "    #for i in range(x.shape[2]):\n",
    "    for i in range(1):\n",
    "    #    x = transf1d(inputDim=225, num_heads=4, feed_forward_dim=2*13, reshape=False)(x)\n",
    "        x = custConv1d([50], meanInit=False, filters=1)(x)\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(x[:,:,i:i+1])\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(y)\n",
    "        #y = custConv1dDepthwise([50], meanInit=False)(y)\n",
    "        #out.append(y)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.LayerNormalization()(x)\n",
    "    #for i in range(1):\n",
    "    #x = tf.keras.layers.Dense(700, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = tf.keras.Model(inp, x_pred)\n",
    "    return model  \n",
    "\n",
    "#model = cnn2D() \n",
    "#model= squeezeformer()\n",
    "#model = fcn() \n",
    "#model = cnn1() \n",
    "#model = transformer()\n",
    "#model = cnnAttentin()\n",
    "#model = singleWL()\n",
    "model=cnn13()\n",
    "#model=cnn5converges()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[4].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('fullPred_singleWL_cnn12_010_021_newDataPipeline.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = iter(train_dataset)\n",
    "batch=next(batch_iter)\n",
    "#batch1=next(batch_iter)\n",
    "#batch2=next(batch_iter)\n",
    "out = model(batch[0])\n",
    "dataset_iterator = iter(test_dataset)\n",
    "test_batch1 = next(dataset_iterator)\n",
    "#test_batch2 = next(dataset_iterator)\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/single_wl_deviations_model-{epoch:02d}.weights.h5\",\n",
    "    save_weights_only=True,  # Set to False if you want to save the entire model\n",
    "    save_freq=300 * 4,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss='mae'\n",
    "              #loss='mse'#maeSingleWL#'mae'            \n",
    "              #,metrics=[log_likelihood_maxScaling]\n",
    "              ,metrics=['mse']\n",
    "              , optimizer=optimizer)\n",
    "\n",
    "#history = model.fit(#train_dataset, \n",
    "#                    #batch[0],batch[1], #verbose=2,\n",
    "#                    small_train,\n",
    "#                    #validation_data=test_dataset,\n",
    "#                    validation_data=test_batch1,\n",
    "#                    epochs=100, batch_size=batch_size,\n",
    "#                    #callbacks=[lr_callback]\n",
    "#                    )\n",
    "\n",
    "\n",
    "# batch normalization essential for gradient to travel downstream!\n",
    "# with batch of 12 we converge well to mae ~3.3, mse 57 after ~3000 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(#train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    small_train,\n",
    "                    #validation_data=test_dataset,\n",
    "                    validation_data=test_batch1,\n",
    "                    epochs=1000, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(small_train))\n",
    "outs = model.predict(b[0])\n",
    "np.mean(np.abs(outs - b[1][:,0:1].numpy())), outs[0:20],b[1][0:20,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs[:,0] ,b[1][:,0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = model.predict(test_batch1[0])\n",
    "np.mean(np.abs(outs[:,0] - test_batch1[1][:,0].numpy())),outs[0:20],test_batch1[1][0:20,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(#train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    small_train,\n",
    "                    #validation_data=test_dataset,\n",
    "                    validation_data=test_batch1,\n",
    "                    epochs=100, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('deviationModelCnn56_5_75.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcStats(b, plot=True, display=False):\n",
    "    outputs = model.predict(b[0])\n",
    "    print(outputs.shape)\n",
    "    pred = outputs[:,0]\n",
    "\n",
    "    if display:\n",
    "        print(pred[0:10:,0:2,0], b[1][0:10:,0:2])\n",
    "        print(pred[0:10:,0:2,0]*maxLabels, b[1][0:10:,0:2]*maxLabels)\n",
    "\n",
    "    mae = np.sum(np.abs(pred-b[1])) / pred.shape[0] #/ pred.shape[1]\n",
    "    mse = np.sum(np.abs(pred-b[1])**2) / pred.shape[0]# / pred.shape[1]\n",
    "    print('mae',mae,'mse', mse)\n",
    "\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    #m = min(100, batch[0].shape[0])\n",
    "    #for i in range(m): #range(12):# \n",
    "    print(b[1].shape)\n",
    "    fig.add_trace(go.Scatter(y=b[1],mode='markers',name=f'gt',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred,mode='markers',name=f'pred',marker=dict(size=3)))\n",
    "    fig.show()\n",
    "\n",
    "    plt.hist(batch[1][:], bins=30, edgecolor='blue',alpha=0.7)\n",
    "    plt.hist(out[:], bins=30, edgecolor='red',alpha=0.7)\n",
    "    plt.title('Histogram of Data')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcStats(next(iter(small_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcStats(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcStats(test_batch1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(batch[0])\n",
    "\n",
    "mae = np.sum(np.abs(outputs*100*maxLabels-batch[1][:,0:1]*100*maxLabels)) / outputs.shape[0]\n",
    "mse = np.sum(np.abs(outputs*100*maxLabels-batch[1][:,0:1]*100*maxLabels)**2) / outputs.shape[0]\n",
    "mae1 = np.sum(np.abs(outputs*maxLabels-batch[1][:,0:1]*maxLabels)) / outputs.shape[0]\n",
    "mse1 = np.sum(np.abs(outputs*maxLabels-batch[1][:,0:1]*maxLabels)**2) / outputs.shape[0]\n",
    "mae,mae1, outputs, batch[1][:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputst = model.predict(test_batch[0])\n",
    "\n",
    "mae = np.sum(np.abs(outputst*100*maxLabels-test_batch[1][:,0:1]*100*maxLabels)) / outputs.shape[0]\n",
    "mse = np.sum(np.abs(outputst*100*maxLabels-test_batch[1][:,0:1]*100*maxLabels)**2) / outputs.shape[0]\n",
    "mae1 = np.sum(np.abs(outputst*maxLabels-test_batch[1][:,0:1]*maxLabels)) / outputs.shape[0]\n",
    "mse1 = np.sum(np.abs(outputs*maxLabels-test_batch[1][:,0:1]*maxLabels)**2) / outputs.shape[0]\n",
    "mae,mae1, outputst, test_batch[1][:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize input data\n",
    "fig = go.Figure()\n",
    "for i in range(100):\n",
    "    fig.add_trace(go.Scatter(y=batch[0][i,:,0],mode='markers',name=f'{i}',marker=dict(size=3)))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(batch[0])\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=batch[1][:],mode='markers',name=f'gt',marker=dict(size=3)))\n",
    "fig.add_trace(go.Scatter(y=out[:,0],mode='markers',name=f'pred',marker=dict(size=3)))\n",
    "fig.show()\n",
    "\n",
    "plt.hist(batch[1][:], bins=30, edgecolor='blue',alpha=0.7)\n",
    "plt.hist(out[:], bins=30, edgecolor='red',alpha=0.7)\n",
    "plt.title('Histogram of Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.predict(test_batch1[0])\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(y=test_batch1[1][:],mode='markers',name=f'gt',marker=dict(size=3)))\n",
    "fig.add_trace(go.Scatter(y=out[:,0],mode='markers',name=f'pred',marker=dict(size=3)))\n",
    "fig.show()\n",
    "\n",
    "plt.hist(test_batch1[1][:], bins=30, edgecolor='blue',alpha=0.7)\n",
    "plt.hist(out[:], bins=30, edgecolor='red',alpha=0.7)\n",
    "plt.title('Histogram of Data')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch1[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualize layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchid = 2\n",
    "wl = 0\n",
    "x = batch[0][batchid:batchid+1,:,:,1]  \n",
    "print(x.shape)  \n",
    "fig = plt.figure()\n",
    "plt.plot(x[0,:,wl])\n",
    "plt.title(f'input')\n",
    "plt.show()\n",
    "\n",
    "for layers in range(len(model.layers)-1):\n",
    "    x = model.layers[layers+1](x)\n",
    "    print(x.shape)\n",
    "    \n",
    "    if len(x.shape)>=3:\n",
    "        if len(x.shape) == 4:\n",
    "            for i in range(x.shape[-1]):\n",
    "                fig = plt.figure()\n",
    "                plt.plot(x[0,:,wl,i])\n",
    "                plt.title(f'layer {layers+1}')\n",
    "                plt.show()\n",
    "        else:\n",
    "            fig = plt.figure()\n",
    "            if x.shape[2] == 283:\n",
    "                plt.plot(x[0,:,wl])\n",
    "            else:\n",
    "                if x.shape[2] == 1:\n",
    "                    print(x[:,wl,:], batch[1][batchid:batchid+1,0:1])\n",
    "                else:\n",
    "                    plt.plot(x[0,wl,:])\n",
    "            plt.title(f'layer {layers+1}')\n",
    "            plt.show()\n",
    "    else:\n",
    "        if x.shape[1] >1:\n",
    "            fig=plt.figure()\n",
    "            plt.plot(x[0])\n",
    "            plt.title(f'layer{layers+1}')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(x, batch[1][batchid:batchid+1,0:1])\n",
    "#print(model.layers[2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print1Val(batchRowId, wl, batch, only1PerGraph=True):\n",
    "    x = batch[0][batchRowId:batchRowId+1,:,:]  \n",
    "    print(x.shape)  \n",
    "    fig = plt.figure()\n",
    "    for i in range(x.shape[2]):\n",
    "        plt.plot(x[0,:,i])\n",
    "    plt.title(f'input')\n",
    "    plt.show()\n",
    "\n",
    "    x = x[:,:,1:14]\n",
    "    for layers in range(len(model.layers)-1):\n",
    "        i = layers + 1\n",
    "        print(x.shape)\n",
    "        x = model.layers[i](x)\n",
    "        print(x.shape)\n",
    "        if x.shape[0] ==1 and x.shape[1] == 1 and len(x.shape)==2:\n",
    "            print(x, batch[1][batchid:batchid+1,wl])\n",
    "        else:\n",
    "        \n",
    "            fig = plt.figure()\n",
    "            if len(x.shape) == 3:\n",
    "                for i in range(x.shape[2]):\n",
    "                    #fig = plt.figure()\n",
    "                    plt.plot(np.reshape(x[0,:,i],(-1)))\n",
    "                    if only1PerGraph:\n",
    "                        break\n",
    "                plt.title(f'layer {i}')\n",
    "                plt.show()\n",
    "            else:\n",
    "                plt.plot(x[0,:])\n",
    "                plt.title(f'layer {i}')\n",
    "                plt.show()\n",
    "    #print(model.layers[2].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print1Val(0,0,test_batch1, only1PerGraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print1Val(0,0,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(batch[0][batchid,:,wl,1]  )\n",
    "plt.title(f'input')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0][batchid:batchid+1,:,wl,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[1])\n",
    "for i in range(4):\n",
    "    fig=plt.figure()\n",
    "    plt.plot(x[i,:,0])\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "lablels = labelDf.copy()\n",
    "\n",
    "meanTarget = np.mean(labelDf.mean())\n",
    "stdTarget = np.std(labelDf.std())\n",
    "maxTarget = np.max(labelDf.max())\n",
    "minTarget = np.min(labelDf.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTarget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.2):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrain(train_stars):\n",
    "    i = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,:]\n",
    "            if i ==0:\n",
    "                mean = np.mean(x,axis=(0))\n",
    "                sumS = np.sum(x**2,axis=0)\n",
    "            else:\n",
    "                mean = mean + np.mean(x, axis=(0))\n",
    "                sumS += np.sum(x**2,axis=0)\n",
    "            i=i+1\n",
    "    meanTrain = mean / i\n",
    "    stdTrain = np.sqrt(sumS / (i*x.shape[0]) - meanTrain**2)    \n",
    "    return meanTrain, stdTrain\n",
    "#meanTrain, stdTrain = calcMeanAndStdOfTrain(train_stars)\n",
    "\n",
    "def normalize_over_train(features, labels):\n",
    "    features = (features - meanTrain) / (stdTrain + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "# normalize over time per samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrainPerStar(x):\n",
    "    mean = np.mean(x,axis=(0))\n",
    "    sumS = np.sum(x**2,axis=0)\n",
    "    stdTrain = np.sqrt(sumS / (x.shape[0]) - mean**2)    \n",
    "    return mean, stdTrain\n",
    "def normalize_per_sample(features, labels):\n",
    "    m,s = calcMeanAndStdOfTrainPerStar(features)\n",
    "    features = (features) / (s + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            features = np.reshape(features,(-1,25,283,4))\n",
    "            features = np.mean(features,axis=1)\n",
    "            #features, labels = normalize_per_sample(features,labels)\n",
    "            features, labels = normalize_over_train(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([225, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283])))) #5625\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025517145902829814"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez('helpers.npz', meanTrain=meanTrain, stdTrain=stdTrain,max=max,min=min,std=std,mean=mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.00255171)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded = np.load('helpers.npz')\n",
    "meanTrain=loaded['meanTrain']\n",
    "stdTrain=loaded['stdTrain']\n",
    "maxTarget=loaded['max']\n",
    "minTarget=loaded['min']\n",
    "stdTarget=loaded['std']\n",
    "meanTarget=loaded['mean']\n",
    "meanTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 12\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">221</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">400,728</span> │ get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">110</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">106</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">400,728</span> │ average_pooling1… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_4 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">53</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">400,728</span> │ average_pooling1… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_5 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape11_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ average_pooling1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape11</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,500</span> │ reshape11_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28300</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">28,301,000</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">283,283</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,001</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│                     │                   │            │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">283,283</span> │ dense_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape22_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape22</span>)         │                   │            │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m225\u001b[0m, \u001b[38;5;34m283\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m4\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m225\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m221\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │    \u001b[38;5;34m400,728\u001b[0m │ get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m110\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m106\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │    \u001b[38;5;34m400,728\u001b[0m │ average_pooling1… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_4 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m53\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │    \u001b[38;5;34m400,728\u001b[0m │ average_pooling1… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_5 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape11_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m24\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ average_pooling1… │\n",
       "│ (\u001b[38;5;33mReshape11\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │      \u001b[38;5;34m2,500\u001b[0m │ reshape11_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28300\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ dense_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1000\u001b[0m)      │ \u001b[38;5;34m28,301,000\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m)       │    \u001b[38;5;34m283,283\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m1,001\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│                     │                   │            │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m)       │    \u001b[38;5;34m283,283\u001b[0m │ dense_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape22_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m2\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],      │\n",
       "│ (\u001b[38;5;33mReshape22\u001b[0m)         │                   │            │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,073,251</span> (114.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m30,073,251\u001b[0m (114.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">30,073,251</span> (114.72 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m30,073,251\u001b[0m (114.72 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timepoints = 225\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "\n",
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        return x\n",
    "    \n",
    "class reduce(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        mean = tf.expand_dims(mean, axis=-1)\n",
    "        return mean\n",
    "class reduce1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        return mean\n",
    "\n",
    "def fcnM(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "    #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(283, activation='relu')(x)#, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    \n",
    "    #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(283, activation='relu')(x)#, kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n",
    "    #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(x)\n",
    "    x_pred = x_pred+mean\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "def cnnM(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "    #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    #mean = reduce1()(x)\n",
    "    #mean = tf.keras.layers.BatchNormalization(epsilon=1e-6)(mean)\n",
    "    #mean = tf.keras.layers.Dense(1000)(mean)\n",
    "    #mean = tf.keras.layers.BatchNormalization(epsilon=1e-6)(mean)\n",
    "    #mean = tf.keras.layers.Dense(100)(mean)\n",
    "    #mean = tf.keras.layers.Dense(1,activation='linear')(mean)\n",
    "\n",
    "    #x = Reshape11()(x)\n",
    "    dim = timepoints\n",
    "    #x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(5), padding='valid')(x)\n",
    "    #x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(50), padding='valid')(x)\n",
    "    for i in range(3):\n",
    "        x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(5), padding='valid')(x)\n",
    "        x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "        #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "    #x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(600), padding='valid')(x)\n",
    "    #x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(20), padding='valid')(x)\n",
    "    #x = tf.keras.layers.MaxPooling1D(50)(x) \n",
    "    \n",
    "    #x = ReduceDim()(x)\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    #x = tf.keras.layers.BatchNormalization(epsilon=1e-6)(x)\n",
    "    mean = tf.keras.layers.Dense(1,activation='relu')(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_pred = x_pred+mean\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "def cnnMean(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    mean = reduce()(x)\n",
    "    #mean = tf.keras.layers.BatchNormalization(epsilon=1e-6)(mean)\n",
    "    filters=32\n",
    "    for i in range(3):\n",
    "        mean = tf.keras.layers.Conv1D(filters=filters, kernel_size=(10), padding='valid')(mean)\n",
    "        mean = tf.keras.layers.AveragePooling1D(3)(mean) \n",
    "        filters = filters*2\n",
    "    mean = tf.keras.layers.Flatten()(mean)\n",
    "    #mean = tf.keras.layers.LayerNormalization(epsilon=1e-6)(mean)\n",
    "    mean = tf.keras.layers.Dense(1000)(mean)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(mean)\n",
    "    model = tf.keras.Model(inp, mean)\n",
    "    return model\n",
    "\n",
    "def fcnMean(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    mean = reduce1()(x)\n",
    "    #mean = tf.keras.layers.LayerNormalization(epsilon=1e-6)(mean)\n",
    "    mean = tf.keras.layers.Dense(1000)(mean)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(mean)\n",
    "    #mean = mean[:,0]\n",
    "    model = tf.keras.Model(inp, mean)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = cnnM() \n",
    "#model = buildTransfModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('320_epochs_logLoss_8_12.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.float64,\n",
       " tf.float32,\n",
       " tf.float32,\n",
       " TensorShape([12, 225, 283, 4]),\n",
       " TensorShape([12, 283]),\n",
       " TensorShape([12, 283, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "test_batch = next(iter(test_dataset))\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9988300237125592"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_likelihood_zScoreTarget(y_trueZScore, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueZScore * stdTarget + meanTarget   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predZScore = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predZScore * stdTarget + meanTarget\n",
    "    stdDev = tf.exp(log_sigma)*stdTarget  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = log_sigma + tf.math.log(stdTarget)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / stdDev)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdTarget*stdTarget) + tf.square(y_trueZScore))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal)*283*5625 - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScaling(y_trueMax, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueMax * maxTarget #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTarget #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTarget  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = tf.math.log(sigma)# + tf.math.log(max)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / sigma)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdTarget*stdTarget) + tf.square((y_true - meanTarget)/stdTarget))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal)*283*5625 - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScalingComp(y_trueMax,y_predMax):\n",
    "    y_true = y_trueMax * maxTarget #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_pred = y_predMax[:, :,0]*maxTarget\n",
    "    log_sigma = y_predMax[:, :,1]\n",
    "    sigma_pred = np.sqrt(tf.exp(log_sigma)*maxTarget)\n",
    "    sigma_true = 1e-5 # or 1e-5?\n",
    "\n",
    "    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred))\n",
    "    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=sigma_true * np.ones_like(y_true)))\n",
    "    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=meanTarget * np.ones_like(y_true), scale=stdTarget * np.ones_like(y_true)))\n",
    "\n",
    "    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n",
    "    return submit_score\n",
    "#log_likelihood_zScoreTarget(batch[1], out)\n",
    "#log_likelihood_maxScaling(batch[1], out)\n",
    "log_likelihood_maxScalingComp(batch[1],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=300\n",
    "LR_SCHEDULE = [0.01*((np.cos(step/epochs *np.pi) if np.cos(step/epochs*np.pi)>0.001 else np.sin(step/epochs*np.pi))) for step in range(epochs)]\n",
    "plt.figure()\n",
    "plt.plot(LR_SCHEDULE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_logSigma(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    loss = tf.math.abs(y_true_zScore - y_predZScore)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    is_large = tf.reduce_any(tf.greater(y_pred[:, :,1], 5.0))\n",
    "    #tf.print(y_pred[:,:,1])\n",
    "    def true_fn():\n",
    "        print(\"Tensor contains large values\")\n",
    "        return y_pred[:,:,1] + tf.math.exp(5.0)\n",
    "    \n",
    "    def false_fn():\n",
    "        return tf.math.exp(y_pred[:,:,1])\n",
    "\n",
    "    logConfidence = tf.cond(is_large, true_fn,false_fn)\n",
    "    loss_log = tf.math.abs(loss-(logConfidence))\n",
    "    l = tf.reduce_sum(loss, axis=-1) + tf.reduce_sum(loss_log,axis=-1)\n",
    "    return l\n",
    "\n",
    "def loss_abs(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]\n",
    "    logConfidence = y_pred[:, :,1]\n",
    "    loss = tf.math.abs(y_true_zScore - y_predZScore)\n",
    "    loss_2 = tf.math.abs(loss-(logConfidence))\n",
    "    l = tf.reduce_sum(loss, axis=-1) + tf.reduce_sum(loss_2,axis=-1)\n",
    "    return l \n",
    "\n",
    "\n",
    "def loss_mae(y_true_zScore,y_pred):\n",
    "    y_true_meanVal = tf.math.reduce_mean(y_true_zScore,axis=1)\n",
    "    y_pred = y_pred[:,0]\n",
    "    absVal = tf.math.abs(y_true_meanVal - y_pred)\n",
    "    m = tf.reduce_mean(absVal,axis=0)\n",
    "    return m\n",
    "\n",
    "\n",
    "\n",
    "class DynamicLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.epoch = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if tf.less(self.epoch, 5):\n",
    "            return loss_abs(y_true, y_pred)\n",
    "        else:\n",
    "            return loss_logSigma(y_true, y_pred)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        self.epoch.assign_add(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "Tensor contains large values\n",
      "Tensor contains large values\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - log_likelihood_max_scaling: nan - loss: 84840.0000Tensor contains large values\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 338ms/step - log_likelihood_max_scaling: nan - loss: 84488.7891 - val_log_likelihood_max_scaling: -1742024343552.0000 - val_loss: 40339.0000\n",
      "Epoch 2/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 340ms/step - log_likelihood_max_scaling: nan - loss: 34832.3125 - val_log_likelihood_max_scaling: nan - val_loss: 3395.9048\n",
      "Epoch 3/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 348ms/step - log_likelihood_max_scaling: nan - loss: 2464.3582 - val_log_likelihood_max_scaling: nan - val_loss: 319.0421\n",
      "Epoch 4/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 435ms/step - log_likelihood_max_scaling: nan - loss: 258.0143 - val_log_likelihood_max_scaling: nan - val_loss: 219.9009\n",
      "Epoch 5/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 435ms/step - log_likelihood_max_scaling: nan - loss: 196.0117 - val_log_likelihood_max_scaling: nan - val_loss: 174.2463\n",
      "Epoch 6/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 452ms/step - log_likelihood_max_scaling: nan - loss: 168.8205 - val_log_likelihood_max_scaling: nan - val_loss: 155.1371\n",
      "Epoch 7/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 417ms/step - log_likelihood_max_scaling: nan - loss: 156.1512 - val_log_likelihood_max_scaling: nan - val_loss: 150.5988\n",
      "Epoch 8/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 420ms/step - log_likelihood_max_scaling: nan - loss: 137.6705 - val_log_likelihood_max_scaling: nan - val_loss: 124.8067\n",
      "Epoch 9/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 351ms/step - log_likelihood_max_scaling: nan - loss: 129.8204 - val_log_likelihood_max_scaling: -26688806971918209620377600.0000 - val_loss: 155.7334\n",
      "Epoch 10/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 352ms/step - log_likelihood_max_scaling: -13602367366771877607374848.0000 - loss: 143.9790 - val_log_likelihood_max_scaling: -195643267327603477839872.0000 - val_loss: 155.7672\n",
      "Epoch 11/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 349ms/step - log_likelihood_max_scaling: -43114589535882866327552.0000 - loss: 128.8206 - val_log_likelihood_max_scaling: -115509660248937857024.0000 - val_loss: 116.8783\n",
      "Epoch 12/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 350ms/step - log_likelihood_max_scaling: -89105504234408771584.0000 - loss: 112.8161 - val_log_likelihood_max_scaling: -4160799664498540544.0000 - val_loss: 135.2569\n",
      "Epoch 13/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - log_likelihood_max_scaling: -1993167053602684928.0000 - loss: 111.9953 - val_log_likelihood_max_scaling: -89898682027081728.0000 - val_loss: 100.6485\n",
      "Epoch 14/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - log_likelihood_max_scaling: -31571733362245632.0000 - loss: 97.4146 - val_log_likelihood_max_scaling: -2354704814178304.0000 - val_loss: 106.2622\n",
      "Epoch 15/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 427ms/step - log_likelihood_max_scaling: -1210608338862080.0000 - loss: 113.9132 - val_log_likelihood_max_scaling: -13180189278208.0000 - val_loss: 111.0045\n",
      "Epoch 16/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 431ms/step - log_likelihood_max_scaling: -7176091336704.0000 - loss: 90.7093 - val_log_likelihood_max_scaling: -332057608192.0000 - val_loss: 84.1176\n",
      "Epoch 17/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 425ms/step - log_likelihood_max_scaling: -250601046016.0000 - loss: 85.2658 - val_log_likelihood_max_scaling: -23749322752.0000 - val_loss: 87.9014\n",
      "Epoch 18/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 430ms/step - log_likelihood_max_scaling: -25640482816.0000 - loss: 81.0771 - val_log_likelihood_max_scaling: -4283834368.0000 - val_loss: 75.9810\n",
      "Epoch 19/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 423ms/step - log_likelihood_max_scaling: -4278854912.0000 - loss: 85.9087 - val_log_likelihood_max_scaling: -68398120.0000 - val_loss: 86.3407\n",
      "Epoch 20/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 442ms/step - log_likelihood_max_scaling: -74276400.0000 - loss: 77.4396 - val_log_likelihood_max_scaling: -18599038.0000 - val_loss: 112.0542\n",
      "Epoch 21/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 438ms/step - log_likelihood_max_scaling: -9613252.0000 - loss: 84.5760 - val_log_likelihood_max_scaling: -562165.9375 - val_loss: 75.6303\n",
      "Epoch 22/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 445ms/step - log_likelihood_max_scaling: -660612.5000 - loss: 72.7706 - val_log_likelihood_max_scaling: -63691.5352 - val_loss: 77.5700\n",
      "Epoch 23/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 430ms/step - log_likelihood_max_scaling: -108508.3984 - loss: 80.5317 - val_log_likelihood_max_scaling: -6562.8687 - val_loss: 71.6239\n",
      "Epoch 24/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 430ms/step - log_likelihood_max_scaling: -5330.9990 - loss: 67.6274 - val_log_likelihood_max_scaling: -2587.4026 - val_loss: 75.8216\n",
      "Epoch 25/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 428ms/step - log_likelihood_max_scaling: -2233.4661 - loss: 76.1976 - val_log_likelihood_max_scaling: -380.7716 - val_loss: 75.0391\n",
      "Epoch 26/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 429ms/step - log_likelihood_max_scaling: -223.9743 - loss: 70.7767 - val_log_likelihood_max_scaling: -13.6496 - val_loss: 67.7020\n",
      "Epoch 27/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 455ms/step - log_likelihood_max_scaling: -96.8685 - loss: 71.2623 - val_log_likelihood_max_scaling: -10.6863 - val_loss: 85.8911\n",
      "Epoch 28/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 461ms/step - log_likelihood_max_scaling: -2.7813 - loss: 69.8066 - val_log_likelihood_max_scaling: -3.8643 - val_loss: 66.2828\n",
      "Epoch 29/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 448ms/step - log_likelihood_max_scaling: -4.1559 - loss: 62.8099 - val_log_likelihood_max_scaling: 0.3652 - val_loss: 65.1853\n",
      "Epoch 30/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 412ms/step - log_likelihood_max_scaling: 0.2271 - loss: 62.7944 - val_log_likelihood_max_scaling: 0.6314 - val_loss: 59.8071\n",
      "Epoch 31/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 330ms/step - log_likelihood_max_scaling: 0.6102 - loss: 58.1049 - val_log_likelihood_max_scaling: 0.6108 - val_loss: 52.5793\n",
      "Epoch 32/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 339ms/step - log_likelihood_max_scaling: 0.6595 - loss: 54.7579 - val_log_likelihood_max_scaling: 0.6759 - val_loss: 47.1406\n",
      "Epoch 33/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 333ms/step - log_likelihood_max_scaling: 0.6796 - loss: 45.7147 - val_log_likelihood_max_scaling: 0.6826 - val_loss: 50.8964\n",
      "Epoch 34/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 339ms/step - log_likelihood_max_scaling: 0.6795 - loss: 48.5130 - val_log_likelihood_max_scaling: 0.6832 - val_loss: 42.4608\n",
      "Epoch 35/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 336ms/step - log_likelihood_max_scaling: 0.6884 - loss: 41.6334 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 35.7998\n",
      "Epoch 36/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 339ms/step - log_likelihood_max_scaling: 0.6990 - loss: 39.0421 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 33.0231\n",
      "Epoch 37/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 336ms/step - log_likelihood_max_scaling: nan - loss: 542.9779 - val_log_likelihood_max_scaling: nan - val_loss: 35045.0820\n",
      "Epoch 38/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 341ms/step - log_likelihood_max_scaling: nan - loss: 2273593.5000 - val_log_likelihood_max_scaling: nan - val_loss: 1045215.3125\n",
      "Epoch 39/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 336ms/step - log_likelihood_max_scaling: nan - loss: 348509.9062 - val_log_likelihood_max_scaling: nan - val_loss: 42627.0352\n",
      "Epoch 40/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 356ms/step - log_likelihood_max_scaling: nan - loss: 41120.9805 - val_log_likelihood_max_scaling: nan - val_loss: 35385.5039\n",
      "Epoch 41/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - log_likelihood_max_scaling: nan - loss: 30631.5527 - val_log_likelihood_max_scaling: nan - val_loss: 16304.1514\n",
      "Epoch 42/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 376ms/step - log_likelihood_max_scaling: nan - loss: 12869.5234 - val_log_likelihood_max_scaling: nan - val_loss: 4024.7283\n",
      "Epoch 43/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 440ms/step - log_likelihood_max_scaling: nan - loss: 2542.4175 - val_log_likelihood_max_scaling: nan - val_loss: 1239.0609\n",
      "Epoch 44/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 427ms/step - log_likelihood_max_scaling: nan - loss: 1327.7031 - val_log_likelihood_max_scaling: nan - val_loss: 1506.3064\n",
      "Epoch 45/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 427ms/step - log_likelihood_max_scaling: nan - loss: 1137.5531 - val_log_likelihood_max_scaling: nan - val_loss: 451.8621\n",
      "Epoch 46/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 435ms/step - log_likelihood_max_scaling: nan - loss: 390.3252 - val_log_likelihood_max_scaling: nan - val_loss: 820.6647\n",
      "Epoch 47/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 443ms/step - log_likelihood_max_scaling: nan - loss: 469.0240 - val_log_likelihood_max_scaling: nan - val_loss: 247.3988\n",
      "Epoch 48/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 434ms/step - log_likelihood_max_scaling: nan - loss: 372.3542 - val_log_likelihood_max_scaling: nan - val_loss: 357.7924\n",
      "Epoch 49/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 403ms/step - log_likelihood_max_scaling: nan - loss: 320.5302 - val_log_likelihood_max_scaling: nan - val_loss: 316.7823\n",
      "Epoch 50/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 425ms/step - log_likelihood_max_scaling: nan - loss: 258.2368 - val_log_likelihood_max_scaling: nan - val_loss: 624.4826\n",
      "Epoch 51/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 423ms/step - log_likelihood_max_scaling: nan - loss: 472.1443 - val_log_likelihood_max_scaling: nan - val_loss: 311.3965\n",
      "Epoch 52/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 428ms/step - log_likelihood_max_scaling: nan - loss: 265.6985 - val_log_likelihood_max_scaling: nan - val_loss: 301.4914\n",
      "Epoch 53/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 422ms/step - log_likelihood_max_scaling: nan - loss: 232.9439 - val_log_likelihood_max_scaling: nan - val_loss: 192.3822\n",
      "Epoch 54/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 422ms/step - log_likelihood_max_scaling: nan - loss: 168.4922 - val_log_likelihood_max_scaling: nan - val_loss: 161.9195\n",
      "Epoch 55/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 422ms/step - log_likelihood_max_scaling: nan - loss: 165.2652 - val_log_likelihood_max_scaling: nan - val_loss: 178.2735\n",
      "Epoch 56/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 424ms/step - log_likelihood_max_scaling: nan - loss: 187.8674 - val_log_likelihood_max_scaling: nan - val_loss: 140.1078\n",
      "Epoch 57/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 423ms/step - log_likelihood_max_scaling: nan - loss: 155.8359 - val_log_likelihood_max_scaling: nan - val_loss: 181.1974\n",
      "Epoch 58/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 424ms/step - log_likelihood_max_scaling: nan - loss: 201.3714 - val_log_likelihood_max_scaling: nan - val_loss: 117.6637\n",
      "Epoch 59/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 375ms/step - log_likelihood_max_scaling: nan - loss: 143.4417 - val_log_likelihood_max_scaling: nan - val_loss: 267.1578\n",
      "Epoch 60/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 357ms/step - log_likelihood_max_scaling: nan - loss: 244.4260 - val_log_likelihood_max_scaling: nan - val_loss: 137.3864\n",
      "Epoch 61/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 413ms/step - log_likelihood_max_scaling: nan - loss: 131.9639 - val_log_likelihood_max_scaling: nan - val_loss: 105.8916\n",
      "Epoch 62/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 436ms/step - log_likelihood_max_scaling: nan - loss: 150.9709 - val_log_likelihood_max_scaling: nan - val_loss: 123.6797\n",
      "Epoch 63/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 430ms/step - log_likelihood_max_scaling: nan - loss: 137.7065 - val_log_likelihood_max_scaling: nan - val_loss: 206.8121\n",
      "Epoch 64/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 423ms/step - log_likelihood_max_scaling: nan - loss: 170.1599 - val_log_likelihood_max_scaling: nan - val_loss: 143.3802\n",
      "Epoch 65/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 452ms/step - log_likelihood_max_scaling: nan - loss: 133.2782 - val_log_likelihood_max_scaling: nan - val_loss: 189.9940\n",
      "Epoch 66/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - log_likelihood_max_scaling: nan - loss: 188.2121 - val_log_likelihood_max_scaling: nan - val_loss: 105.9318\n",
      "Epoch 67/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 363ms/step - log_likelihood_max_scaling: nan - loss: 143.3505 - val_log_likelihood_max_scaling: nan - val_loss: 114.9863\n",
      "Epoch 68/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - log_likelihood_max_scaling: nan - loss: 135.9041 - val_log_likelihood_max_scaling: nan - val_loss: 115.5661\n",
      "Epoch 69/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 362ms/step - log_likelihood_max_scaling: nan - loss: 128.0654 - val_log_likelihood_max_scaling: nan - val_loss: 128.7630\n",
      "Epoch 70/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 355ms/step - log_likelihood_max_scaling: nan - loss: 121.4143 - val_log_likelihood_max_scaling: nan - val_loss: 188.0866\n",
      "Epoch 71/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 357ms/step - log_likelihood_max_scaling: nan - loss: 138.2590 - val_log_likelihood_max_scaling: nan - val_loss: 118.0898\n",
      "Epoch 72/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 358ms/step - log_likelihood_max_scaling: nan - loss: 120.0102 - val_log_likelihood_max_scaling: nan - val_loss: 136.3391\n",
      "Epoch 73/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 360ms/step - log_likelihood_max_scaling: nan - loss: 134.0822 - val_log_likelihood_max_scaling: nan - val_loss: 92.3025\n",
      "Epoch 74/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - log_likelihood_max_scaling: nan - loss: 126.6690 - val_log_likelihood_max_scaling: nan - val_loss: 114.6469\n",
      "Epoch 75/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 369ms/step - log_likelihood_max_scaling: nan - loss: 119.5678 - val_log_likelihood_max_scaling: nan - val_loss: 107.6724\n",
      "Epoch 76/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 355ms/step - log_likelihood_max_scaling: nan - loss: 109.3417 - val_log_likelihood_max_scaling: nan - val_loss: 106.4595\n",
      "Epoch 77/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 361ms/step - log_likelihood_max_scaling: nan - loss: 116.7242 - val_log_likelihood_max_scaling: nan - val_loss: 96.5565\n",
      "Epoch 78/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 362ms/step - log_likelihood_max_scaling: nan - loss: 134.5308 - val_log_likelihood_max_scaling: nan - val_loss: 163.1011\n",
      "Epoch 79/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 359ms/step - log_likelihood_max_scaling: nan - loss: 142.0443 - val_log_likelihood_max_scaling: nan - val_loss: 102.3190\n",
      "Epoch 80/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 366ms/step - log_likelihood_max_scaling: nan - loss: 112.8891 - val_log_likelihood_max_scaling: nan - val_loss: 129.4008\n",
      "Epoch 81/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 358ms/step - log_likelihood_max_scaling: nan - loss: 117.7194 - val_log_likelihood_max_scaling: nan - val_loss: 100.7547\n",
      "Epoch 82/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 361ms/step - log_likelihood_max_scaling: nan - loss: 117.3758 - val_log_likelihood_max_scaling: nan - val_loss: 104.1712\n",
      "Epoch 83/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 358ms/step - log_likelihood_max_scaling: nan - loss: 98.8333 - val_log_likelihood_max_scaling: nan - val_loss: 144.5596\n",
      "Epoch 84/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 361ms/step - log_likelihood_max_scaling: nan - loss: 119.5987 - val_log_likelihood_max_scaling: nan - val_loss: 128.1301\n",
      "Epoch 85/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 361ms/step - log_likelihood_max_scaling: nan - loss: 110.2077 - val_log_likelihood_max_scaling: nan - val_loss: 91.6961\n",
      "Epoch 86/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 438ms/step - log_likelihood_max_scaling: nan - loss: 96.2581 - val_log_likelihood_max_scaling: nan - val_loss: 130.2172\n",
      "Epoch 87/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 433ms/step - log_likelihood_max_scaling: nan - loss: 125.3938 - val_log_likelihood_max_scaling: nan - val_loss: 103.5028\n",
      "Epoch 88/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 430ms/step - log_likelihood_max_scaling: nan - loss: 107.9484 - val_log_likelihood_max_scaling: nan - val_loss: 99.9733\n",
      "Epoch 89/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 421ms/step - log_likelihood_max_scaling: nan - loss: 105.5890 - val_log_likelihood_max_scaling: nan - val_loss: 102.0504\n",
      "Epoch 90/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 430ms/step - log_likelihood_max_scaling: nan - loss: 120.8465 - val_log_likelihood_max_scaling: nan - val_loss: 125.6948\n",
      "Epoch 91/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 436ms/step - log_likelihood_max_scaling: nan - loss: 113.1795 - val_log_likelihood_max_scaling: nan - val_loss: 107.5336\n",
      "Epoch 92/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 424ms/step - log_likelihood_max_scaling: nan - loss: 124.3749 - val_log_likelihood_max_scaling: nan - val_loss: 153.8900\n",
      "Epoch 93/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 377ms/step - log_likelihood_max_scaling: nan - loss: 120.4341 - val_log_likelihood_max_scaling: nan - val_loss: 100.7551\n",
      "Epoch 94/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 369ms/step - log_likelihood_max_scaling: nan - loss: 133.7320 - val_log_likelihood_max_scaling: nan - val_loss: 122.3274\n",
      "Epoch 95/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 365ms/step - log_likelihood_max_scaling: nan - loss: 105.5396 - val_log_likelihood_max_scaling: nan - val_loss: 111.4068\n",
      "Epoch 96/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 378ms/step - log_likelihood_max_scaling: nan - loss: 117.3446 - val_log_likelihood_max_scaling: nan - val_loss: 115.7904\n",
      "Epoch 97/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 372ms/step - log_likelihood_max_scaling: nan - loss: 107.4662 - val_log_likelihood_max_scaling: nan - val_loss: 89.3159\n",
      "Epoch 98/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 368ms/step - log_likelihood_max_scaling: nan - loss: 99.3851 - val_log_likelihood_max_scaling: nan - val_loss: 119.5061\n",
      "Epoch 99/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 365ms/step - log_likelihood_max_scaling: nan - loss: 120.6937 - val_log_likelihood_max_scaling: nan - val_loss: 143.6544\n",
      "Epoch 100/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 366ms/step - log_likelihood_max_scaling: nan - loss: 130.4509 - val_log_likelihood_max_scaling: -8167944456981245329408000.0000 - val_loss: 100.9580\n",
      "Epoch 101/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 365ms/step - log_likelihood_max_scaling: -10903175210302438031491072.0000 - loss: 93.8536 - val_log_likelihood_max_scaling: -559604139186145911111680.0000 - val_loss: 93.2435\n",
      "Epoch 102/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - log_likelihood_max_scaling: -1775889092793477609029632.0000 - loss: 95.0158 - val_log_likelihood_max_scaling: -4116962500884195287498752.0000 - val_loss: 141.9242\n",
      "Epoch 103/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 367ms/step - log_likelihood_max_scaling: -1138162596138404538220544.0000 - loss: 113.4548 - val_log_likelihood_max_scaling: -184952370330572261687296.0000 - val_loss: 135.8394\n",
      "Epoch 104/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 431ms/step - log_likelihood_max_scaling: -194505694090526709514240.0000 - loss: 104.7315 - val_log_likelihood_max_scaling: -38266424004621881049088.0000 - val_loss: 103.1849\n",
      "Epoch 105/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 432ms/step - log_likelihood_max_scaling: -34685413782515995377664.0000 - loss: 103.2342 - val_log_likelihood_max_scaling: -1818381126108971008000.0000 - val_loss: 79.6959\n",
      "Epoch 106/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 433ms/step - log_likelihood_max_scaling: -9160583413799929774080.0000 - loss: 116.0378 - val_log_likelihood_max_scaling: -290497076546976612352.0000 - val_loss: 81.1461\n",
      "Epoch 107/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 421ms/step - log_likelihood_max_scaling: -2039845497047426793472.0000 - loss: 95.0034 - val_log_likelihood_max_scaling: -1047935912262143836160.0000 - val_loss: 99.8353\n",
      "Epoch 108/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 431ms/step - log_likelihood_max_scaling: -588401835609163825152.0000 - loss: 94.0465 - val_log_likelihood_max_scaling: -116083728463939239936.0000 - val_loss: 100.6787\n",
      "Epoch 109/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 422ms/step - log_likelihood_max_scaling: -133461140305031987200.0000 - loss: 95.6607 - val_log_likelihood_max_scaling: -26255860483244425216.0000 - val_loss: 106.8704\n",
      "Epoch 110/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 432ms/step - log_likelihood_max_scaling: -35341032704580780032.0000 - loss: 99.5756 - val_log_likelihood_max_scaling: -5812050900016955392.0000 - val_loss: 86.9964\n",
      "Epoch 111/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 409ms/step - log_likelihood_max_scaling: -5128807777447182336.0000 - loss: 97.8977 - val_log_likelihood_max_scaling: -369036956010545152.0000 - val_loss: 80.6972\n",
      "Epoch 112/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - log_likelihood_max_scaling: -1621898535664353280.0000 - loss: 100.4910 - val_log_likelihood_max_scaling: -204430204087566336.0000 - val_loss: 100.7089\n",
      "Epoch 113/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 364ms/step - log_likelihood_max_scaling: -453736940903071744.0000 - loss: 104.0070 - val_log_likelihood_max_scaling: -56250907502837760.0000 - val_loss: 82.7369\n",
      "Epoch 114/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 362ms/step - log_likelihood_max_scaling: -103110722984083456.0000 - loss: 88.7671 - val_log_likelihood_max_scaling: -17242365253124096.0000 - val_loss: 69.3103\n",
      "Epoch 115/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 369ms/step - log_likelihood_max_scaling: -22269950890606592.0000 - loss: 82.7265 - val_log_likelihood_max_scaling: -10641774687027200.0000 - val_loss: 86.5092\n",
      "Epoch 116/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 362ms/step - log_likelihood_max_scaling: -6180865960837120.0000 - loss: 84.8673 - val_log_likelihood_max_scaling: -1401596139274240.0000 - val_loss: 69.1500\n",
      "Epoch 117/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 366ms/step - log_likelihood_max_scaling: -1110756993007616.0000 - loss: 79.2680 - val_log_likelihood_max_scaling: -148142196523008.0000 - val_loss: 73.2441\n",
      "Epoch 118/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 370ms/step - log_likelihood_max_scaling: -331042590294016.0000 - loss: 87.6891 - val_log_likelihood_max_scaling: -58796428754944.0000 - val_loss: 78.6600\n",
      "Epoch 119/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 433ms/step - log_likelihood_max_scaling: -92309098070016.0000 - loss: 83.1188 - val_log_likelihood_max_scaling: -54006592307200.0000 - val_loss: 80.4170\n",
      "Epoch 120/120\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 444ms/step - log_likelihood_max_scaling: -24930546614272.0000 - loss: 80.8998 - val_log_likelihood_max_scaling: -1251597484032.0000 - val_loss: 59.4127\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#model.compile(loss=loss_logSigma,metrics=[log_likelihood_maxScaling], optimizer=optimizer)\n",
    "model.compile(loss=loss_logSigma,metrics=[log_likelihood_maxScaling], optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    #validation_data=(test_batch[0],test_batch[1]),\n",
    "                    epochs=120, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('320_epochs_logLoss_8_12.keras')\n",
    "# Save weights\n",
    "model.save_weights('320_epochs_logLoss_8_12.weights.h5')\n",
    "\n",
    "# Load weights\n",
    "#loaded_weights = model.load_weights('170_epochs_accLoss_reluActivation_23_23.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('120_epochs_accLoss31_30.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 320ms/step - log_likelihood_max_scaling: 0.6713 - loss: 7.6528 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.6153\n",
      "Epoch 2/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 411ms/step - log_likelihood_max_scaling: 0.6823 - loss: 11.0545 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.1805\n",
      "Epoch 3/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 409ms/step - log_likelihood_max_scaling: 0.6940 - loss: 7.4634 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 13.9168\n",
      "Epoch 4/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 404ms/step - log_likelihood_max_scaling: 0.6931 - loss: 8.9968 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.9837\n",
      "Epoch 5/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 413ms/step - log_likelihood_max_scaling: 0.6776 - loss: 8.2306 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 15.9271\n",
      "Epoch 6/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 412ms/step - log_likelihood_max_scaling: 0.6757 - loss: 10.6716 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 14.1547\n",
      "Epoch 7/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 421ms/step - log_likelihood_max_scaling: 0.6806 - loss: 6.6109 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.2324\n",
      "Epoch 8/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 410ms/step - log_likelihood_max_scaling: 0.6954 - loss: 5.9008 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.0907\n",
      "Epoch 9/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6968 - loss: 5.8642 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.5681\n",
      "Epoch 10/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6770 - loss: 6.4046 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.9236\n",
      "Epoch 11/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6685 - loss: 24.9714 - val_log_likelihood_max_scaling: 0.6832 - val_loss: 22.7686\n",
      "Epoch 12/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - log_likelihood_max_scaling: 0.6991 - loss: 26.6103 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.9068\n",
      "Epoch 13/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 410ms/step - log_likelihood_max_scaling: 0.6922 - loss: 7.1966 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.0546\n",
      "Epoch 14/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - log_likelihood_max_scaling: 0.6890 - loss: 7.7640 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.8236\n",
      "Epoch 15/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 409ms/step - log_likelihood_max_scaling: 0.6859 - loss: 6.9575 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.9602\n",
      "Epoch 16/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6890 - loss: 6.5666 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.2924\n",
      "Epoch 17/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6775 - loss: 6.8624 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.8899\n",
      "Epoch 18/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - log_likelihood_max_scaling: 0.6755 - loss: 6.9227 - val_log_likelihood_max_scaling: 0.6832 - val_loss: 14.4401\n",
      "Epoch 19/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6942 - loss: 9.1970 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.6534\n",
      "Epoch 20/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 404ms/step - log_likelihood_max_scaling: 0.6844 - loss: 5.5661 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.2284\n",
      "Epoch 21/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - log_likelihood_max_scaling: 0.6960 - loss: 6.4263 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.2272\n",
      "Epoch 22/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 410ms/step - log_likelihood_max_scaling: 0.6787 - loss: 5.3143 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 10.8965\n",
      "Epoch 23/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6914 - loss: 5.6283 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.2127\n",
      "Epoch 24/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.7019 - loss: 5.9711 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 10.9912\n",
      "Epoch 25/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6850 - loss: 6.1434 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.1113\n",
      "Epoch 26/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6958 - loss: 6.2454 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.8358\n",
      "Epoch 27/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - log_likelihood_max_scaling: 0.6876 - loss: 5.4041 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.6780\n",
      "Epoch 28/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6867 - loss: 6.3955 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.3501\n",
      "Epoch 29/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6981 - loss: 5.1852 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.2576\n",
      "Epoch 30/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6867 - loss: 6.0457 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.6042\n",
      "Epoch 31/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 407ms/step - log_likelihood_max_scaling: 0.6893 - loss: 7.0032 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.5867\n",
      "Epoch 32/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 410ms/step - log_likelihood_max_scaling: 0.6829 - loss: 5.1171 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 10.8783\n",
      "Epoch 33/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 412ms/step - log_likelihood_max_scaling: 0.6841 - loss: 5.2538 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.3010\n",
      "Epoch 34/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - log_likelihood_max_scaling: 0.6477 - loss: 5.6609 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.8782\n",
      "Epoch 35/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - log_likelihood_max_scaling: 0.6841 - loss: 7.7094 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 13.5731\n",
      "Epoch 36/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 401ms/step - log_likelihood_max_scaling: 0.6926 - loss: 6.8629 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.4805\n",
      "Epoch 37/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 405ms/step - log_likelihood_max_scaling: 0.6827 - loss: 7.1789 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.6629\n",
      "Epoch 38/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6827 - loss: 8.3328 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.1765\n",
      "Epoch 39/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 408ms/step - log_likelihood_max_scaling: 0.6914 - loss: 6.1083 - val_log_likelihood_max_scaling: 0.6832 - val_loss: 17.2373\n",
      "Epoch 40/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 412ms/step - log_likelihood_max_scaling: 0.6881 - loss: 11.9824 - val_log_likelihood_max_scaling: 0.6832 - val_loss: 14.5672\n",
      "Epoch 41/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 412ms/step - log_likelihood_max_scaling: 0.6689 - loss: 9.1763 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.7460\n",
      "Epoch 42/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 402ms/step - log_likelihood_max_scaling: 0.7042 - loss: 6.4916 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.8712\n",
      "Epoch 43/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 413ms/step - log_likelihood_max_scaling: 0.6893 - loss: 6.7772 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.2615\n",
      "Epoch 44/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 414ms/step - log_likelihood_max_scaling: 0.6991 - loss: 5.4055 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.5679\n",
      "Epoch 45/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 419ms/step - log_likelihood_max_scaling: 0.6776 - loss: 8.0410 - val_log_likelihood_max_scaling: 0.6832 - val_loss: 15.1393\n",
      "Epoch 46/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 417ms/step - log_likelihood_max_scaling: 0.6906 - loss: 7.0860 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.8639\n",
      "Epoch 47/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 409ms/step - log_likelihood_max_scaling: 0.6965 - loss: 7.5258 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 11.9246\n",
      "Epoch 48/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 415ms/step - log_likelihood_max_scaling: 0.6772 - loss: 6.1332 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 14.4371\n",
      "Epoch 49/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 406ms/step - log_likelihood_max_scaling: 0.6712 - loss: 8.5519 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 14.2613\n",
      "Epoch 50/50\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 414ms/step - log_likelihood_max_scaling: 0.6877 - loss: 7.3125 - val_log_likelihood_max_scaling: 0.6833 - val_loss: 12.2066\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    #validation_data=next(iter(test_dataset)),\n",
    "                    epochs=50, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try couldn't fit the values, just predicted mean if I kept the shape (output layer of shape 1 - tensor 283x100 -> 283x1)\n",
    "# having a flatten layer between converges\n",
    "\n",
    "# flatten layer and 12 samples -> predict the same for all 12 samples, maybe not enough filters\n",
    "\n",
    "# PROBLEM why we can't fit multiple targets: layer normalization!! use batch norm instead\n",
    "\n",
    "#---- with batch norm\n",
    "# cnn model + mean estimation, loss ~80, but predicting differnt mean\n",
    "# fcn model, loss ~81\n",
    "# fcn model / min scaling -> loss 0.8 / 27 (lots of negative predictions)\n",
    "# fcn model / max scaling / relu activation -> 3.1/8 (lots of 0 predictions) / with scale of 100, loss =14.9/43691\n",
    "# cnn model / max scaling / mean pred -> 6.0/inf\n",
    "# cnn ... no layer norm in beginning -> 15\n",
    "\n",
    "# loss function for every output (batch,283) / 100 epochs\n",
    "# cnn 1.5 loss\n",
    "# cnn with smaller LR 0.22(also after 200 epochs)\n",
    "# cnn with separate mean prediction loss 20.5 (lr0.0001) vs 3.5(lr0.0005) / can't even fit 2 samples (0.5 for lr 0.0005)\n",
    "\n",
    "# cnn without mean prediction (2 samples, lr0.0005) 22.4   / lr0.001 0.4 loss, but targets still fit badly / only fitting target noVar 0.08 still bad\n",
    "\n",
    "# difference between train / test = batch norm has significant effect here\n",
    "#fcn + mean, 2 samples LR0.0005 -> \n",
    "#fcn + mean, 2 samples only loss on target -> \n",
    "#fcn + mean, 1 sample, only loss on target -> 0.03 targets are far off\n",
    "#fcn, 1 sample, only loss on target -> 0.4 targets are far off\n",
    "# -> train data was not normalized!!\n",
    "\n",
    "# with regularization / without regularization doesn't matter that much as long as sample is normalized\n",
    "# normalization per sample -> predict the same for all targets ~0.0978\n",
    "# norm per sample + bis estimation -> predict same for all targets (besides 1) ~0.0978\n",
    "\n",
    "# with learning rate schedule -> 0.06 lots more possible to not get stuck in local minima\n",
    "\n",
    "#cnn / norm over train / bias estimation / lr0.01 / only target -> ~45 sum loss\n",
    "#cnn / norm over train / bias estimation / lf0.01 / target + loss2 -> ~47 after 95 epochs (15 after~150epochs)\n",
    "\n",
    "#cnn / norm over train / bais est / lr0.01 / target + loss / activation function relu instead of linear (conf + bias / still nan bc stddev =0, log(0) = nan)\n",
    "# 39/40 but training seems to be a lot more stable\n",
    "# after 170 epochs 23.7/23.5\n",
    "# after 220 epochs 11/19 (but already went down to 14/16)\n",
    "# after 250 epochs 12/16 (but already 16/15)\n",
    "# after 300 epochs 12/17\n",
    "\n",
    "# log loss\n",
    "#after 120 epochs -> 80/59 \n",
    "#after 170 epochs -> 30/37  (30/27 before)  / 0.67/0.68 log likelihood\n",
    "# 17.7/17.5 after 177 epochs (best so far)\n",
    "# 220 -> 83/79\n",
    "# 270 -> 26/29\n",
    "# 320 -> 8.7/13\n",
    "# 350 -> 8.9/12.5 (was already 11)\n",
    "\n",
    "\n",
    "# Assuming 'history' is your model's training history\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "#plt.plot(epochs, test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.06610027, 0.08522013],\n",
       "        [0.19229797, 0.20303626],\n",
       "        [0.273583  , 0.31230897],\n",
       "        [0.15744403, 0.12355568],\n",
       "        [0.21963206, 0.221281  ],\n",
       "        [0.59530675, 0.58689016],\n",
       "        [0.43527368, 0.46160156],\n",
       "        [0.27138215, 0.2528267 ],\n",
       "        [0.20721841, 0.22699125],\n",
       "        [0.5539123 , 0.60307556],\n",
       "        [0.18370745, 0.1905589 ],\n",
       "        [0.4288279 , 0.45899945]], dtype=float32),\n",
       " <tf.Tensor: shape=(12, 2), dtype=float32, numpy=\n",
       " array([[0.09698724, 0.09683569],\n",
       "        [0.20825225, 0.21028523],\n",
       "        [0.2878623 , 0.2889837 ],\n",
       "        [0.14158063, 0.14114746],\n",
       "        [0.18724738, 0.19135208],\n",
       "        [0.5811523 , 0.58253735],\n",
       "        [0.43278694, 0.4439398 ],\n",
       "        [0.24758054, 0.248844  ],\n",
       "        [0.1983679 , 0.19857608],\n",
       "        [0.5931966 , 0.5926916 ],\n",
       "        [0.18650211, 0.18677506],\n",
       "        [0.4276639 , 0.44121537]], dtype=float32)>,\n",
       " array([[0.11075503, 0.11957327],\n",
       "        [0.11075962, 0.12040351],\n",
       "        [0.11317819, 0.12182653],\n",
       "        [0.1097092 , 0.11822083],\n",
       "        [0.11760895, 0.12246984],\n",
       "        [0.13123302, 0.1393909 ],\n",
       "        [0.12554474, 0.13439935],\n",
       "        [0.11624186, 0.12504236],\n",
       "        [0.11853863, 0.12500525],\n",
       "        [0.13865338, 0.14328212],\n",
       "        [0.1134529 , 0.12257776],\n",
       "        [0.12683597, 0.13413839]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pred = model.predict(normData)\n",
    "pred = model.predict(test_batch[0])\n",
    "pred[:,0:2,0], test_batch[1][:,0:2] ,np.sqrt(np.exp(pred[:,0:2,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "[0.9989527189602132, 0.9995865162658214, 0.9994470712971446, 0.9995566807314736, 0.9996655509799878, 0.9994467575229277, 0.99929054539063, 0.9993027758416327, 0.9994853735524719, 0.999536592902753, 0.9992798176756721, 0.9990068391378235]\n",
      "0.999379770021546\n"
     ]
    }
   ],
   "source": [
    "aggScore = []\n",
    "for x,y in test_dataset:\n",
    "    pred = model.predict(x)\n",
    "    aggScore.append(log_likelihood_maxScalingComp(y,pred))\n",
    "print(aggScore)\n",
    "print(np.mean(aggScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall',(loss_fn0(batch[1],pred)))\n",
    "for i in range(batch_size):\n",
    "    print(f'batch {i}',np.mean(loss_fn0(batch[1][i,:],pred[i:i+1,:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(4):#[2,6,10,20,100]:\n",
    "    fig.add_trace(go.Scatter(y=batch[0][i,:,0,0],mode='markers',name=f'f_{i}',marker=dict(size=3)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(12):#[2,6,10,20,100]:\n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wl_1</th>\n",
       "      <th>wl_2</th>\n",
       "      <th>wl_3</th>\n",
       "      <th>wl_4</th>\n",
       "      <th>wl_5</th>\n",
       "      <th>wl_6</th>\n",
       "      <th>wl_7</th>\n",
       "      <th>wl_8</th>\n",
       "      <th>wl_9</th>\n",
       "      <th>wl_10</th>\n",
       "      <th>...</th>\n",
       "      <th>sigma_274</th>\n",
       "      <th>sigma_275</th>\n",
       "      <th>sigma_276</th>\n",
       "      <th>sigma_277</th>\n",
       "      <th>sigma_278</th>\n",
       "      <th>sigma_279</th>\n",
       "      <th>sigma_280</th>\n",
       "      <th>sigma_281</th>\n",
       "      <th>sigma_282</th>\n",
       "      <th>sigma_283</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000212</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000360</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000256</td>\n",
       "      <td>0.000207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.002097</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.002744</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.002064</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>0.000182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.000245</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000218</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000319</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.000136</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000329</td>\n",
       "      <td>0.000231</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000522</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000187</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000230</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000207</td>\n",
       "      <td>0.000233</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.001560</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000310</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.001915</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000166</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.001872</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.001460</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000201</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.000230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000084</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000263</td>\n",
       "      <td>0.000575</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000340</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000328</td>\n",
       "      <td>0.000232</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000234</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 566 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        wl_1      wl_2      wl_3      wl_4      wl_5      wl_6      wl_7  \\\n",
       "0   0.000315  0.000864  0.000797  0.000839  0.000543  0.000735  0.000598   \n",
       "1   0.001416  0.001582  0.001860  0.001760  0.001857  0.001891  0.002271   \n",
       "2   0.002138  0.002097  0.002545  0.002326  0.002379  0.002744  0.002619   \n",
       "3   0.001158  0.000841  0.001297  0.001200  0.000946  0.001285  0.000659   \n",
       "4   0.000951  0.001538  0.001928  0.001886  0.001514  0.002018  0.001528   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "10 -0.000136  0.000484  0.000342  0.000329  0.000231  0.000361  0.000522   \n",
       "11  0.001228  0.001475  0.001218  0.001347  0.001539  0.001560  0.001749   \n",
       "0   0.001852  0.001961  0.002129  0.002086  0.002074  0.002334  0.001974   \n",
       "1   0.001126  0.001983  0.001753  0.001639  0.001841  0.001872  0.002128   \n",
       "2  -0.000084  0.000765  0.000311  0.000263  0.000575  0.000680  0.001238   \n",
       "\n",
       "        wl_8      wl_9     wl_10  ...  sigma_274  sigma_275  sigma_276  \\\n",
       "0   0.000845  0.000754  0.000764  ...   0.000199   0.000329   0.000255   \n",
       "1   0.001700  0.001577  0.001469  ...   0.000191   0.000290   0.000253   \n",
       "2   0.002064  0.002173  0.002228  ...   0.000175   0.000246   0.000236   \n",
       "3   0.000958  0.000904  0.000804  ...   0.000187   0.000331   0.000261   \n",
       "4   0.001377  0.001309  0.001330  ...   0.000186   0.000332   0.000241   \n",
       "..       ...       ...       ...  ...        ...        ...        ...   \n",
       "10  0.000250  0.000114  0.000062  ...   0.000187   0.000339   0.000241   \n",
       "11  0.001230  0.001322  0.001155  ...   0.000192   0.000310   0.000255   \n",
       "0   0.001915  0.001881  0.001839  ...   0.000163   0.000236   0.000214   \n",
       "1   0.001472  0.001460  0.001487  ...   0.000201   0.000332   0.000259   \n",
       "2   0.000194  0.000340  0.000389  ...   0.000193   0.000328   0.000232   \n",
       "\n",
       "    sigma_277  sigma_278  sigma_279  sigma_280  sigma_281  sigma_282  \\\n",
       "0    0.000212   0.000227   0.000246   0.000360   0.000213   0.000244   \n",
       "1    0.000222   0.000223   0.000252   0.000348   0.000208   0.000256   \n",
       "2    0.000180   0.000196   0.000209   0.000289   0.000144   0.000204   \n",
       "3    0.000218   0.000219   0.000245   0.000339   0.000222   0.000229   \n",
       "4    0.000208   0.000218   0.000231   0.000319   0.000200   0.000228   \n",
       "..        ...        ...        ...        ...        ...        ...   \n",
       "10   0.000198   0.000222   0.000230   0.000347   0.000207   0.000233   \n",
       "11   0.000227   0.000223   0.000248   0.000351   0.000208   0.000251   \n",
       "0    0.000166   0.000174   0.000193   0.000270   0.000136   0.000180   \n",
       "1    0.000228   0.000234   0.000254   0.000353   0.000215   0.000257   \n",
       "2    0.000197   0.000235   0.000234   0.000363   0.000191   0.000247   \n",
       "\n",
       "    sigma_283  \n",
       "0    0.000215  \n",
       "1    0.000207  \n",
       "2    0.000182  \n",
       "3    0.000200  \n",
       "4    0.000219  \n",
       "..        ...  \n",
       "10   0.000227  \n",
       "11   0.000201  \n",
       "0    0.000167  \n",
       "1    0.000230  \n",
       "2    0.000227  \n",
       "\n",
       "[135 rows x 566 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predArray = []\n",
    "for x,y in test_dataset:\n",
    "    pred = model.predict(x)\n",
    "    #transform labels back to real domain\n",
    "    scaledPred = pred[:,:,0]*maxTarget\n",
    "    stdDev = (np.exp(pred[:,:,1])*maxTarget)\n",
    "\n",
    "    #concatenate to df\n",
    "    arr = np.concatenate((scaledPred,stdDev),axis=1)\n",
    "    df = pd.DataFrame(arr, columns=sampleSub.columns[1:])\n",
    "    predArray.append(df)\n",
    "predArray = pd.concat(predArray)\n",
    "predArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wl_1</th>\n",
       "      <th>wl_2</th>\n",
       "      <th>wl_3</th>\n",
       "      <th>wl_4</th>\n",
       "      <th>wl_5</th>\n",
       "      <th>wl_6</th>\n",
       "      <th>wl_7</th>\n",
       "      <th>wl_8</th>\n",
       "      <th>wl_9</th>\n",
       "      <th>wl_10</th>\n",
       "      <th>...</th>\n",
       "      <th>wl_274</th>\n",
       "      <th>wl_275</th>\n",
       "      <th>wl_276</th>\n",
       "      <th>wl_277</th>\n",
       "      <th>wl_278</th>\n",
       "      <th>wl_279</th>\n",
       "      <th>wl_280</th>\n",
       "      <th>wl_281</th>\n",
       "      <th>wl_282</th>\n",
       "      <th>wl_283</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>planet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1949187031</th>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3875101730</th>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.001678</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.001684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4247918843</th>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.002302</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.002297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.002319</td>\n",
       "      <td>0.002318</td>\n",
       "      <td>0.002316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012051641</th>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.001143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>612015401</th>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2320457425</th>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000452</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556934812</th>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.001506</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.001345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1121250116</th>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>0.001852</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.001844</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001899</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615603560</th>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.001570</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.001557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871770446</th>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>0.000483</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.000481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 283 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                wl_1      wl_2      wl_3      wl_4      wl_5      wl_6  \\\n",
       "planet_id                                                                \n",
       "1949187031  0.000775  0.000774  0.000774  0.000774  0.000774  0.000774   \n",
       "3875101730  0.001664  0.001681  0.001679  0.001676  0.001679  0.001674   \n",
       "4247918843  0.002301  0.002310  0.002307  0.002301  0.002302  0.002300   \n",
       "1012051641  0.001132  0.001128  0.001128  0.001127  0.001127  0.001127   \n",
       "612015401   0.001496  0.001529  0.001530  0.001530  0.001529  0.001525   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2320457425  0.000440  0.000445  0.000445  0.000443  0.000445  0.000443   \n",
       "2556934812  0.001366  0.001529  0.001513  0.001490  0.001533  0.001506   \n",
       "1121250116  0.001866  0.001857  0.001852  0.001846  0.001846  0.001843   \n",
       "1615603560  0.001392  0.001501  0.001490  0.001473  0.001502  0.001485   \n",
       "871770446   0.000467  0.000517  0.000514  0.000510  0.000519  0.000513   \n",
       "\n",
       "                wl_7      wl_8      wl_9     wl_10  ...    wl_274    wl_275  \\\n",
       "planet_id                                           ...                       \n",
       "1949187031  0.000774  0.000774  0.000774  0.000774  ...  0.000790  0.000790   \n",
       "3875101730  0.001674  0.001680  0.001678  0.001675  ...  0.001689  0.001689   \n",
       "4247918843  0.002298  0.002298  0.002298  0.002297  ...  0.002319  0.002318   \n",
       "1012051641  0.001127  0.001128  0.001128  0.001128  ...  0.001145  0.001145   \n",
       "612015401   0.001527  0.001533  0.001533  0.001533  ...  0.001487  0.001487   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "2320457425  0.000442  0.000443  0.000442  0.000442  ...  0.000452  0.000451   \n",
       "2556934812  0.001485  0.001503  0.001480  0.001468  ...  0.001349  0.001350   \n",
       "1121250116  0.001842  0.001844  0.001843  0.001841  ...  0.001900  0.001899   \n",
       "1615603560  0.001471  0.001483  0.001470  0.001464  ...  0.001577  0.001575   \n",
       "871770446   0.000509  0.000513  0.000509  0.000507  ...  0.000484  0.000484   \n",
       "\n",
       "              wl_276    wl_277    wl_278    wl_279    wl_280    wl_281  \\\n",
       "planet_id                                                                \n",
       "1949187031  0.000790  0.000790  0.000789  0.000789  0.000789  0.000789   \n",
       "3875101730  0.001688  0.001688  0.001687  0.001687  0.001686  0.001686   \n",
       "4247918843  0.002317  0.002317  0.002319  0.002320  0.002320  0.002319   \n",
       "1012051641  0.001144  0.001144  0.001144  0.001144  0.001144  0.001144   \n",
       "612015401   0.001487  0.001487  0.001487  0.001487  0.001487  0.001487   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2320457425  0.000451  0.000451  0.000451  0.000451  0.000451  0.000450   \n",
       "2556934812  0.001352  0.001352  0.001349  0.001346  0.001346  0.001348   \n",
       "1121250116  0.001897  0.001897  0.001899  0.001900  0.001900  0.001899   \n",
       "1615603560  0.001574  0.001572  0.001570  0.001569  0.001567  0.001565   \n",
       "871770446   0.000485  0.000484  0.000483  0.000482  0.000482  0.000482   \n",
       "\n",
       "              wl_282    wl_283  \n",
       "planet_id                       \n",
       "1949187031  0.000789  0.000788  \n",
       "3875101730  0.001685  0.001684  \n",
       "4247918843  0.002318  0.002316  \n",
       "1012051641  0.001143  0.001143  \n",
       "612015401   0.001487  0.001487  \n",
       "...              ...       ...  \n",
       "2320457425  0.000450  0.000450  \n",
       "2556934812  0.001348  0.001345  \n",
       "1121250116  0.001897  0.001896  \n",
       "1615603560  0.001561  0.001557  \n",
       "871770446   0.000482  0.000481  \n",
       "\n",
       "[135 rows x 283 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLabels = lablels.loc[[int(star) for star in test_stars]]\n",
    "testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "def score(\n",
    "        solution: pd.DataFrame,\n",
    "        submission: pd.DataFrame,\n",
    "        naive_mean: float,\n",
    "        naive_sigma: float,\n",
    "        sigma_true: float\n",
    "    ) -> float:\n",
    "\n",
    "    if submission.min().min() < 0:\n",
    "        raise ParticipantVisibleError('Negative values in the submission')\n",
    "    for col in submission.columns:\n",
    "        if not pandas.api.types.is_numeric_dtype(submission[col]):\n",
    "            raise ParticipantVisibleError(f'Submission column {col} must be a number')\n",
    "\n",
    "    n_wavelengths = len(solution.columns)\n",
    "    if len(submission.columns) != n_wavelengths*2:\n",
    "        raise ParticipantVisibleError('Wrong number of columns in the submission')\n",
    "\n",
    "    y_pred = submission.iloc[:, :n_wavelengths].values\n",
    "    # Set a non-zero minimum sigma pred to prevent division by zero errors.\n",
    "    sigma_pred = np.clip(submission.iloc[:, n_wavelengths:].values, a_min=10**-15, a_max=None)\n",
    "    y_true = solution.values\n",
    "\n",
    "    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred, scale=sigma_pred))\n",
    "    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=sigma_true * np.ones_like(y_true)))\n",
    "    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=naive_mean * np.ones_like(y_true), scale=naive_sigma * np.ones_like(y_true)))\n",
    "\n",
    "    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n",
    "    return float(np.clip(submit_score, 0.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999998621"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "predArray = predArray.clip(lower=0)\n",
    "score(testLabels, predArray, naive_mean=meanTarget, naive_sigma=stdTarget*stdTarget, sigma_true=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversion_helpers import *\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_folder = \"\"  \n",
    "train_adc_info = pd.read_csv(os.path.join(path_folder, 'train_adc_info.csv'))\n",
    "train_adc_info = train_adc_info.set_index('planet_id')\n",
    "axis_info = pd.read_parquet(os.path.join(path_folder,'axis_info.parquet'))\n",
    "\n",
    "DO_MASK = True  # filter out non responsive pixels\n",
    "DO_THE_NL_CORR = True # most time consuming step, you can choose to ignore it for rapid prototyping, nonlinear correction due to artefacts when reading pixels\n",
    "DO_DARK = True  # dark current is accumulating over time in the pixels, need to compensate that (seems like integration artefact)\n",
    "DO_FLAT = True  # pixel to pixel variation correction (e.g. how pixels respond differently when illuminated uniformly)\n",
    "TIME_BINNING = False  #do a time binning on choosen frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planet_id = 612015401\n",
    "AIRS_cds_binned, FGS1_cds_binned,AIRS_cds_original, FGS1_cds_original = calibrateData(planet_id,train_adc_info,axis_info,DO_MASK,DO_THE_NL_CORR,DO_DARK,DO_FLAT,TIME_BINNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.loc[labels.planet_id == planet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = AIRS_cds_binned.sum(axis=(2,3))\n",
    "print(b.shape)\n",
    "plt.plot(b[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = AIRS_cds_original.sum(axis=(2,3))\n",
    "print(b.shape)\n",
    "plt.plot(b[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRS_cds_binned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization per frame: -> bad idea, wavelengths in one frame get comparable, but not super critical I guess, no strong impact there\n",
    "mean_p_frame = np.mean(AIRS_cds_binned, axis=(2,3), keepdims=True)\n",
    "std_p_frame = np.std(AIRS_cds_binned, axis=(2,3), keepdims=True)\n",
    "min_p_frame = np.min(AIRS_cds_binned, axis=(2,3), keepdims=True)\n",
    "max_p_frame = np.max(AIRS_cds_binned, axis=(2,3), keepdims=True)\n",
    "\n",
    "zScoreAIRS = (AIRS_cds_binned - mean_p_frame) / std_p_frame # gets rid of overall trend -> frames get uncomparable between each other\n",
    "zScoreAIRS = (AIRS_cds_binned - min_p_frame) / (max_p_frame - min_p_frame) # frames get uncomparable between each other\n",
    "b = zScoreAIRS.sum(axis=(2,3))\n",
    "print(b.shape)\n",
    "plt.plot(b[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization per wavelength: seems like the overall trend is also gone, individual wavelength is also not showing a trend\n",
    "# wavlength over time should be comparable\n",
    "mean = np.mean(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "std = np.std(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "min = np.min(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "max = np.max(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "\n",
    "zScoreAIRS = (AIRS_cds_binned - mean) / std # gets rid of overall trend -> frames get uncomparable between each other\n",
    "#zScoreAIRS = (AIRS_cds_binned - min) / (max - min) # frames get uncomparable between each other\n",
    "b = zScoreAIRS.sum(axis=(2,3))\n",
    "b = zScoreAIRS.sum(axis=(3))\n",
    "print(b.shape)\n",
    "plt.plot(b[0,:,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing over entire intensity signal: makes stars comparable, but we don't really want that I guess, spectra are different\n",
    "mean = np.mean(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "std = np.std(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "min = np.min(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "max = np.max(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "\n",
    "zScoreAIRS = (AIRS_cds_binned - mean) / std # gets rid of overall trend -> frames get uncomparable between each other\n",
    "#zScoreAIRS = (AIRS_cds_binned - min) / (max - min) # frames get uncomparable between each other\n",
    "b = zScoreAIRS.sum(axis=(2,3))\n",
    "#b = zScoreAIRS.sum(axis=(3))\n",
    "print(b.shape)\n",
    "plt.plot(b[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIRS_cds_binned.shape, FGS1_cds_binned.shape,AIRS_cds_original.shape, FGS1_cds_original.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeData(planet_id,train_adc_info,axis_info,DO_MASK,DO_THE_NL_CORR,DO_DARK,DO_FLAT,TIME_BINNING, zScoreNorm=True):\n",
    "    path_folder = \"\"  \n",
    "    AIRS_cds_binned, FGS1_cds_binned,AIRS_cds_original, FGS1_cds_original = calibrateData(planet_id,train_adc_info,axis_info,DO_MASK,DO_THE_NL_CORR,DO_DARK,DO_FLAT,TIME_BINNING)\n",
    "    \n",
    "    if zScoreNorm:\n",
    "        mean = np.mean(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        std = np.std(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        zScoreAIRSPlanet = (AIRS_cds_binned - mean) / std\n",
    "\n",
    "        mean = np.mean(FGS1_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        std = np.std(FGS1_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        zScoreFGS1Planet = (FGS1_cds_binned - mean) / std\n",
    "\n",
    "\n",
    "        mean = np.mean(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "        std = np.std(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "        zScoreAIRSWaveL = (AIRS_cds_binned - mean) / std\n",
    "        mean = np.mean(FGS1_cds_binned, axis=(1,2), keepdims=True)\n",
    "        std = np.std(FGS1_cds_binned, axis=(1,2), keepdims=True)\n",
    "        zScoreFGS1WaveL = (FGS1_cds_binned - mean) / std\n",
    "    else:\n",
    "        min = np.min(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        max = np.max(AIRS_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        zScoreAIRSPlanet = (AIRS_cds_binned - min) / (max-min)\n",
    "\n",
    "        min = np.min(FGS1_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        max = np.max(FGS1_cds_binned, axis=(1,2,3), keepdims=True)\n",
    "        zScoreFGS1Planet = (FGS1_cds_binned - min) / (max-min)\n",
    "\n",
    "\n",
    "        min = np.min(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "        max = np.max(AIRS_cds_binned, axis=(1,2), keepdims=True)\n",
    "        zScoreAIRSWaveL = (AIRS_cds_binned - min) / (max-min)\n",
    "\n",
    "    # compress data, cleanedl\n",
    "    AIRS_cds_cleaned_compressed = AIRS_cds_binned.sum(axis=3)  # 1x5625x282\n",
    "    FGS1_cds_cleaned_compressed = FGS1_cds_binned.sum(axis=(2,3)) # 1x67500\n",
    "    FGS1_cds_cleaned_compressed = np.reshape(FGS1_cds_cleaned_compressed, (1,5625,-1)) #1x5625x12\n",
    "\n",
    "    # compress original data\n",
    "    AIRS_cds_original_compressed = AIRS_cds_original.sum(axis=3)  # 1x5625x282\n",
    "    FGS1_cds_original_compressed = FGS1_cds_original.sum(axis=(2,3)) # 1x67500\n",
    "    FGS1_cds_original_compressed = np.reshape(FGS1_cds_original_compressed, (1,5625,-1)) #1x5625x12\n",
    "\n",
    "\n",
    "    # compress normalized data by planet\n",
    "    AIRS_cds_PlanetNorm_compressed = zScoreAIRSPlanet.sum(axis=3)  # 1x5625x282\n",
    "    FGS1_cds_PlanetNorm_compressed = zScoreFGS1Planet.sum(axis=(2,3)) # 1x67500\n",
    "    FGS1_cds_PlanetNorm_compressed = np.reshape(FGS1_cds_PlanetNorm_compressed, (1,5625,-1)) #1x5625x12\n",
    "\n",
    "    # compress normlized data by wavelength\n",
    "    AIRS_cds_WaveLNorm_compressed = zScoreAIRSWaveL.sum(axis=3)  # 1x5625x282\n",
    "    FGS1_cds_WaveLNorm_compressed = zScoreFGS1WaveL.sum(axis=(2,3)) # 1x67500\n",
    "    FGS1_cds_WaveLNorm_compressed = np.reshape(FGS1_cds_WaveLNorm_compressed, (1,5625,-1)) #1x5625x12\n",
    "\n",
    "\n",
    "    \n",
    "    compressed_clean = np.concatenate([AIRS_cds_cleaned_compressed,np.sum(FGS1_cds_cleaned_compressed, axis=2, keepdims=True),np.mean(FGS1_cds_cleaned_compressed, axis=2, keepdims=True),np.std(FGS1_cds_cleaned_compressed, axis=2, keepdims=True)], axis=2)\n",
    "    compressed_origi = np.concatenate([AIRS_cds_original_compressed,np.sum(FGS1_cds_original_compressed, axis=2, keepdims=True),np.mean(FGS1_cds_original_compressed, axis=2, keepdims=True),np.std(FGS1_cds_original_compressed, axis=2, keepdims=True)], axis=2)\n",
    "    compressed_plNor = np.concatenate([AIRS_cds_PlanetNorm_compressed,np.sum(FGS1_cds_PlanetNorm_compressed, axis=2, keepdims=True),np.mean(FGS1_cds_PlanetNorm_compressed, axis=2, keepdims=True),np.std(FGS1_cds_PlanetNorm_compressed, axis=2, keepdims=True)], axis=2)\n",
    "    compressed_waNor = np.concatenate([AIRS_cds_WaveLNorm_compressed,np.sum(FGS1_cds_WaveLNorm_compressed, axis=2, keepdims=True),np.mean(FGS1_cds_WaveLNorm_compressed, axis=2, keepdims=True),np.std(FGS1_cds_WaveLNorm_compressed, axis=2, keepdims=True)], axis=2)\n",
    "    \n",
    "\n",
    "    combined_array = np.stack([compressed_clean,compressed_origi,compressed_plNor,compressed_waNor], axis=-1)\n",
    "\n",
    "    np.savez('train/'+str(planet_id)+'/combined.npz', a=combined_array)\n",
    "    return combined_array\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

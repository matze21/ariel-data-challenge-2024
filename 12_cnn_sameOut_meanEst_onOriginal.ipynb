{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf0 = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf0 = labelDf0.set_index('planet_id')\n",
    "labelDf0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.4):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "meanLabels = np.mean(labelDf.mean())\n",
    "stdLabels = np.std(labelDf.std())\n",
    "maxLabels = np.max(labelDf.max())\n",
    "minLabels = np.min(labelDf.min())\n",
    "\n",
    "trainLabels = labelDf.loc[[int(star) for star in train_stars]]\n",
    "meanTrainLabels = np.mean(trainLabels.mean())\n",
    "stdTrainLabels = np.std(trainLabels.std())\n",
    "maxTrainLabels = np.max(trainLabels.max())\n",
    "minTrainLabels = np.min(trainLabels.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTrainLabels)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrain(train_stars):\n",
    "    i = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,:]\n",
    "            if i ==0:\n",
    "                mean = np.mean(x,axis=(0))\n",
    "                sumS = np.sum(x**2,axis=0)\n",
    "            else:\n",
    "                mean = mean + np.mean(x, axis=(0))\n",
    "                sumS += np.sum(x**2,axis=0)\n",
    "            i=i+1\n",
    "    meanTrain = mean / i\n",
    "    stdTrain = np.sqrt(sumS / (i*x.shape[0]) - meanTrain**2)    \n",
    "    return meanTrain, stdTrain\n",
    "meanTrain, stdTrain = calcMeanAndStdOfTrain(train_stars)\n",
    "\n",
    "def normalize_over_train(features, labels):\n",
    "    features = (features - meanTrain) / (stdTrain + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "# normalize over time per samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrainPerStar(x):\n",
    "    mean = np.mean(x,axis=(0))\n",
    "    sumS = np.sum(x**2,axis=0)\n",
    "    stdTrain = np.sqrt(sumS / (x.shape[0]) - mean**2)    \n",
    "    return mean, stdTrain\n",
    "def normalize_per_sample(features, labels):\n",
    "    m,s = calcMeanAndStdOfTrainPerStar(features)\n",
    "    features = (features) / (s + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            features = np.reshape(features,(-1,25,283,4))\n",
    "            features = np.mean(features,axis=1)\n",
    "            #features, labels = normalize_per_sample(features,labels)\n",
    "            features, labels = normalize_over_train(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([225, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283])))) #5625\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('helpers_origiData_meanPred.npz',meanTrain=meanTrain, stdTrain=stdTrain,meanLabels=meanLabels,stdLabels=stdLabels,maxTrainLabels=maxTrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_4  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,113</span> │ get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv1D</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_3 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ depthwise_conv1d… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_5  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">112</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,113</span> │ average_pooling1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv1D</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mean_of_wavelength… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">225</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ get_item_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">meanOfWavelengths</span>) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_4 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ depthwise_conv1d… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">206</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">168</span> │ mean_of_waveleng… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_6  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,113</span> │ average_pooling1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv1D</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">187</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,288</span> │ conv1d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_5 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ depthwise_conv1d… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">168</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,288</span> │ conv1d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_7  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">566</span>)   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">3,396</span> │ average_pooling1… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv1D</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">149</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,288</span> │ conv1d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">160,461</span> │ depthwise_conv1d… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,288</span> │ conv1d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape11_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape11</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">111</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,288</span> │ conv1d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1000</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">29,000</span> │ reshape11_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">888</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">100,100</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">88,900</span> │ flatten_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">283</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">101</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tile2_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">tile2</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">284</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tile2_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">tile2</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">284</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                     │                   │            │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape2_1          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">284</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ tile2_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape2</span>)          │                   │            │ tile2_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m225\u001b[0m, \u001b[38;5;34m283\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m4\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ get_item_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m225\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mGetItem\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_4  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m225\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │      \u001b[38;5;34m3,113\u001b[0m │ get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mDepthwiseConv1D\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_3 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ depthwise_conv1d… │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_5  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m112\u001b[0m, \u001b[38;5;34m283\u001b[0m)  │      \u001b[38;5;34m3,113\u001b[0m │ average_pooling1… │\n",
       "│ (\u001b[38;5;33mDepthwiseConv1D\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ mean_of_wavelength… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m225\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ get_item_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│ (\u001b[38;5;33mmeanOfWavelengths\u001b[0m) │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_4 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ depthwise_conv1d… │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_6 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m206\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │        \u001b[38;5;34m168\u001b[0m │ mean_of_waveleng… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_6  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m56\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │      \u001b[38;5;34m3,113\u001b[0m │ average_pooling1… │\n",
       "│ (\u001b[38;5;33mDepthwiseConv1D\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_7 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m187\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │      \u001b[38;5;34m1,288\u001b[0m │ conv1d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ average_pooling1d_5 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ depthwise_conv1d… │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)  │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_8 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m168\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │      \u001b[38;5;34m1,288\u001b[0m │ conv1d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ depthwise_conv1d_7  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m566\u001b[0m)   │      \u001b[38;5;34m3,396\u001b[0m │ average_pooling1… │\n",
       "│ (\u001b[38;5;33mDepthwiseConv1D\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_9 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m149\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │      \u001b[38;5;34m1,288\u001b[0m │ conv1d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m283\u001b[0m)   │    \u001b[38;5;34m160,461\u001b[0m │ depthwise_conv1d… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m130\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │      \u001b[38;5;34m1,288\u001b[0m │ conv1d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape11_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m28\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ dense_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mReshape11\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m111\u001b[0m, \u001b[38;5;34m8\u001b[0m)    │      \u001b[38;5;34m1,288\u001b[0m │ conv1d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m1000\u001b[0m) │     \u001b[38;5;34m29,000\u001b[0m │ reshape11_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m888\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m100\u001b[0m)  │    \u001b[38;5;34m100,100\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │     \u001b[38;5;34m88,900\u001b[0m │ flatten_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │        \u001b[38;5;34m101\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m101\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m283\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │        \u001b[38;5;34m101\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tile2_2 (\u001b[38;5;33mtile2\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m284\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ tile2_3 (\u001b[38;5;33mtile2\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m284\u001b[0m, \u001b[38;5;34m1\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ dense_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                     │                   │            │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ reshape2_1          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m284\u001b[0m, \u001b[38;5;34m2\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ tile2_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mReshape2\u001b[0m)          │                   │            │ tile2_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">398,107</span> (1.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m398,107\u001b[0m (1.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">398,107</span> (1.52 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m398,107\u001b[0m (1.52 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        return x\n",
    "    \n",
    "class reduce(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        mean = tf.expand_dims(mean, axis=-1)\n",
    "        return mean\n",
    "class reduce1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        return mean\n",
    "    \n",
    "class tile(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x,mean):\n",
    "        x = tf.concat([x,mean],axis=-1)\n",
    "        return x\n",
    "    \n",
    "class tile2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x,mean):\n",
    "        x = tf.concat([x,tf.expand_dims(mean,axis=-1)],axis=-2)\n",
    "        return x\n",
    "    \n",
    "class meanOfWavelengths(tf.keras.layers.Layer):\n",
    "    def __init__(self, concat=True,**kwargs):\n",
    "        self.concat=concat\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        m = tf.expand_dims(tf.reduce_mean(x,axis=-1),axis=-1)\n",
    "        x = tf.concat([x,m],axis=-1)\n",
    "        return x if self.concat else m\n",
    "\n",
    "\n",
    "timepoints = 225\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "\n",
    "def cnnM(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1]\n",
    "    x = meanOfWavelengths()(x)\n",
    "    \n",
    "    #x = Reshape11()(x)\n",
    "    dim = timepoints\n",
    "    for i in range(3):\n",
    "        # convolution with n_wavelengths of channels -> applying same operation across all channels\n",
    "        # after first convolution we have timepoints*wavelengths -> timepoints*284 filter outputs (* 1 channel)\n",
    "        x = tf.keras.layers.Conv1D(filters=284, kernel_size=(5), padding='valid')(x) \n",
    "        x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    mean = tf.keras.layers.Dense(100,activation='relu')(x)\n",
    "    mean = tf.keras.layers.Dense(50,activation='relu')(mean)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(mean)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_pred = tile()(x_pred,mean)\n",
    "    #x_pred = x_pred+mean\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_confidence = tile()(x_confidence,mean)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def cnnMeanOnly(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1]\n",
    "    x = meanOfWavelengths(False)(x) #\n",
    "    \n",
    "    #x = Reshape11()(x)\n",
    "    #dim = timepoints\n",
    "    for i in range(3):\n",
    "        x = tf.keras.layers.Conv1D(filters=40*(i+1), kernel_size=(5), padding='valid')(x)\n",
    "        x = tf.keras.layers.MaxPooling1D()(x)\n",
    "\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    #mean = tf.keras.layers.Dense(100,activation='relu')(x)\n",
    "    #mean = tf.keras.layers.Dense(50,activation='relu')(mean)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_pred = tile()(x_pred,mean)\n",
    "    #x_pred = x_pred+mean\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_confidence = tile()(x_confidence,mean)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def cnnDepthwise():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1]\n",
    "    x0 = meanOfWavelengths(False)(x)\n",
    "    # timepoints (225) * wavelengths (284)\n",
    "    \n",
    "    #x = Reshape11()(x)\n",
    "    dim = timepoints\n",
    "    for i in range(3):\n",
    "        # depthwise1d filter -> one filter per channel (=wavelength), depth_multiplier tells us how many filters per channel\n",
    "        x = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding='same', depth_multiplier=2,activation='relu')(x)\n",
    "        #x = tf.keras.layers.Conv1D(filters=284, kernel_size=(5), padding='valid')(x)\n",
    "        x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "        #x = tf.keras.layers.Dense(284)(x)\n",
    "\n",
    "    for i in range(4):\n",
    "        x0 = tf.keras.layers.Conv1D(filters=64*(i+1), kernel_size=(5), padding='valid')(x0)\n",
    "        x0 = tf.keras.layers.AveragePooling1D(2)(x0)\n",
    "    #x0 = tf.keras.layers.Dense(280)(x0)\n",
    "    x0 = Reshape11()(x0)\n",
    "    x0 = tf.keras.layers.Dense(1000)(x0)\n",
    "    mean = tf.keras.layers.Flatten()(x0)\n",
    "    mean = tf.keras.layers.Dense(1000)(mean)\n",
    "    mean = tf.keras.layers.Dense(1000)(mean)\n",
    "    mean = tf.keras.layers.Dense(50)(mean)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(mean)\n",
    "\n",
    "    x = tf.keras.layers.Dense(283)(x)\n",
    "    x = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding='same', depth_multiplier=2,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(283)(x)\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    \n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_pred = tile2()(x_pred,mean)\n",
    "    x_confidence = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_confidence = tile2()(x_confidence,mean)\n",
    "    x = Reshape2()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "def cnn2():\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1]\n",
    "    x0 = meanOfWavelengths(False)(x)\n",
    "    # timepoints (225) * wavelengths (284)\n",
    "    \n",
    "    for i in range(3*2):\n",
    "        x0 = tf.keras.layers.Conv1D(filters=8, kernel_size=(20), padding='valid')(x0)\n",
    "#\n",
    "    #x = tf.keras.layers.Dense(1000)(x)\n",
    "    x0 = tf.keras.layers.Flatten()(x0)\n",
    "    #x = tf.keras.layers.Dense(1000)(x)\n",
    "    x0 = tf.keras.layers.Dense(100,activation='relu')(x0)\n",
    "    #x = tf.keras.layers.Dense(50,activation='relu')(x)\n",
    "    mean = tf.keras.layers.Dense(1,activation='linear')(x0)\n",
    "\n",
    "    for i in range(3):\n",
    "        # depthwise1d filter -> one filter per channel (=wavelength), depth_multiplier tells us how many filters per channel\n",
    "        x = tf.keras.layers.DepthwiseConv1D(kernel_size=10,strides=1,padding='same', depth_multiplier=1,activation='relu')(x)\n",
    "        x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "        #x = tf.keras.layers.Dense(284)(x)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv1D(kernel_size=5,strides=1,padding='same', depth_multiplier=2,activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(283)(x)\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    \n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_pred = tile2()(x_pred,mean)\n",
    "    x_confidence = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_confidence = tile2()(x_confidence,mean)\n",
    "    x = Reshape2()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "#model = cnnDepthwise() \n",
    "#model = cnnM() \n",
    "model = cnn2() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tf.float64,\n",
       " tf.float32,\n",
       " tf.float32,\n",
       " TensorShape([64, 225, 283, 4]),\n",
       " TensorShape([64, 283]),\n",
       " TensorShape([64, 284, 2]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "test_batch = next(iter(test_dataset))\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=1.0>, 0.9999999999998052)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_likelihood_maxScaling(y_trueMax, y_predAll):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean = y_predAll[:,283,0:1]\n",
    "\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]+y_predMean\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = tf.math.log(sigma*sigma)# + tf.math.log(max)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square((y_true - y_pred0) / sigma))\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels**4) + tf.square((y_true - meanLabels)/(stdLabels*stdLabels)))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log((1e-5)**4)) * tf.ones_like(y_predMax)\n",
    "    #print(L_pred)\n",
    "    #print(L_ref)\n",
    "    #print(L_ideal)\n",
    "    #print(tf.reduce_sum(L_pred),tf.reduce_sum(L_ideal),tf.reduce_sum(L_ref))\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal) - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScaling_scipy(y_trueMax, y_predAll):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]+y_predMean\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "\n",
    "    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred0, scale=sigma))\n",
    "    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=(1e-10) * np.ones_like(y_true)))\n",
    "    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=meanLabels * np.ones_like(y_true), scale=(stdLabels*stdLabels) * np.ones_like(y_true)))\n",
    "\n",
    "    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n",
    "    #print(GLL_pred, GLL_true, GLL_mean)\n",
    "    \n",
    "    return submit_score\n",
    "\n",
    "log_likelihood_maxScaling(batch[1], out),log_likelihood_maxScaling_scipy(batch[1],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.35699797>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0061271787>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.63687444>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=17.720108>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.87735856>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combL_val_mse(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    lossMean = tf.square(y_predMean-y_trueMean)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossDiff2Mean = tf.square(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLossMean = tf.reduce_mean(lossMean)\n",
    "    rmLossDiff2Mean = tf.reduce_mean(lossDiff2Mean)\n",
    "    combinedLoss = 100*rmLossMean + rmLossDiff2Mean\n",
    "    return combinedLoss\n",
    "\n",
    "def combL_valVar_mse(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    lossMean = tf.square(y_predMean-y_trueMean)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossDiff2Mean = tf.square(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred =tf.square((y_predMean + y_predScaled) - y_trueScaled)\n",
    "    loss_2 = tf.square(lossPred-(logConfidence))\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLossMean = tf.reduce_mean(lossMean)\n",
    "    rmLossDiff2Mean = tf.reduce_mean(lossDiff2Mean)\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    combinedLoss = rmLossMean + rmLossDiff2Mean + rmLossLog\n",
    "    return combinedLoss\n",
    "\n",
    "def combL_valVarWeighted_mse(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    lossMean = tf.square(y_predMean-y_trueMean)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossDiff2Mean = tf.square(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred =tf.square((y_predMean + y_predScaled) - y_trueScaled)\n",
    "    loss_2 = tf.square(lossPred-(logConfidence))\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLossMean = tf.reduce_mean(lossMean)\n",
    "    rmLossDiff2Mean = tf.reduce_mean(lossDiff2Mean)\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    combinedLoss = 2*rmLossMean + rmLossDiff2Mean + rmLossLog*(0.001**2)\n",
    "    return combinedLoss\n",
    "\n",
    "\n",
    "def combL_onlyVar_mse(y_trueScaled, y_predAll):\n",
    "    \"\"\" freeze the gradients of mean and variance \"\"\"\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =mean = tf.stop_gradient(y_predAll[:,283,0:1]) #freeze gradient!!\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    lossMean = tf.square(y_predMean-y_trueMean)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossDiff2Mean = tf.square(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    logConfidence = tf.math.exp(tf.stop_gradient(y_pred[:, :,1])) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(10.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(10.0), logConfidence)\n",
    "\n",
    "    lossPred =tf.square((y_predMean + y_predScaled) - y_trueScaled)\n",
    "    loss_2 = tf.square(lossPred-(logConfidence))\n",
    "    #tf.print(lossMean.shape, lossDiff2Mean.shape, loss_2.shape)\n",
    "\n",
    "    rmLossMean = tf.reduce_mean(lossMean)\n",
    "    rmLossDiff2Mean = tf.reduce_mean(lossDiff2Mean)\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    combinedLoss = 2*rmLossMean + rmLossDiff2Mean + rmLossLog*(0.001**2)\n",
    "    return combinedLoss\n",
    "\n",
    "def mse(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "\n",
    "    y_predScaled = (y_pred[:, :,0] + y_predMean)\n",
    "\n",
    "    y_true = y_trueScaled*maxTrainLabels\n",
    "    y_pred = y_predScaled*maxTrainLabels\n",
    "    loss = tf.square(y_true-y_pred)\n",
    "\n",
    "    #combinedLoss = tf.reduce_sum(lossMean, axis=-1)\n",
    "    return tf.reduce_mean(loss,axis=-1)\n",
    "\n",
    "def mean_mae(y_trueScaled, y_predAll):\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    mae = tf.reduce_mean(tf.abs(y_predMean-y_trueMean))\n",
    "    return mae\n",
    "\n",
    "def deviation_mae(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_sigma = tf.stop_gradient(y_pred[:, :,1])\n",
    "    y_predMean = tf.stop_gradient(y_predAll[:,283,0:1])\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossPred = tf.abs(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    combinedLoss = tf.reduce_mean(lossPred)\n",
    "    return combinedLoss\n",
    "\n",
    "def deviation_mas(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_sigma = tf.stop_gradient(y_pred[:, :,1])\n",
    "    y_predMean = tf.stop_gradient(y_predAll[:,283,0:1])\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossPred = tf.abs(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    combinedLoss = tf.reduce_sum(lossPred)\n",
    "    return combinedLoss\n",
    "\n",
    "def logLoss_mae(y_trueScaled, y_predAll):\n",
    "    y_pred=y_predAll[:,0:283,:]\n",
    "    y_predMean =y_predAll[:,283,0:1]\n",
    "\n",
    "    y_trueMean = tf.expand_dims(tf.reduce_mean(y_trueScaled,axis=-1),axis=-1)\n",
    "    lossMean = tf.abs(y_predMean-y_trueMean)\n",
    "    y_predScaled = y_pred[:, :,0]\n",
    "    y_trueDiff2Mean = y_trueScaled - y_trueMean\n",
    "    lossDiff2Mean = tf.abs(y_trueDiff2Mean - y_predScaled)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(20.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(20.0), logConfidence)\n",
    "\n",
    "    lossPred =lossMean + lossDiff2Mean\n",
    "    loss_2 = tf.abs(lossPred-(logConfidence))\n",
    "    rmLossLog = tf.reduce_mean(loss_2)\n",
    "    return rmLossLog\n",
    "\n",
    "def log_loss_maxScaling(y_trueMax, y_pred):\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = tf.math.log(sigma*sigma)# + tf.math.log(max)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square((y_true - y_pred0) / sigma))\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels**4) + tf.square((y_true - meanLabels)/(stdLabels*stdLabels)))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log((1e-5)**4)) * tf.ones_like(y_predMax)\n",
    "    L = -((tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal) - tf.reduce_sum(L_ref)) -1)\n",
    "    return L\n",
    "\n",
    "#loss_mse(batch[1],out)\n",
    "#combined_loss_mse(batch[1],out)\n",
    "mean_mae(batch[1],out),deviation_mae(batch[1],out),logLoss_mae(batch[1],out),combL_val_mse(batch[1],out),combL_valVar_mse(batch[1],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0059 - log_loss_mae: 0.5987 - loss: 0.4031 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8484 - val_loss: 0.1540\n",
      "Epoch 2/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.8561 - loss: 0.1388 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8631 - val_loss: 0.1393\n",
      "Epoch 3/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.8614 - loss: 0.1342 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8538 - val_loss: 0.1484\n",
      "Epoch 4/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8730 - loss: 0.1214 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8654 - val_loss: 0.1361\n",
      "Epoch 5/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.8757 - loss: 0.1183 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8789 - val_loss: 0.1239\n",
      "Epoch 6/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.8763 - loss: 0.1175 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8730 - val_loss: 0.1279\n",
      "Epoch 7/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.8802 - loss: 0.1155 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8724 - val_loss: 0.1280\n",
      "Epoch 8/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.8837 - loss: 0.1116 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8757 - val_loss: 0.1264\n",
      "Epoch 9/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8756 - loss: 0.1185 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8760 - val_loss: 0.1275\n",
      "Epoch 10/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.8859 - loss: 0.1101 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8793 - val_loss: 0.1218\n",
      "Epoch 11/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.8879 - loss: 0.1063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8799 - val_loss: 0.1224\n",
      "Epoch 12/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.8898 - loss: 0.1053 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8768 - val_loss: 0.1263\n",
      "Epoch 13/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8883 - loss: 0.1071 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8800 - val_loss: 0.1214\n",
      "Epoch 14/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.8799 - loss: 0.1143 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8735 - val_loss: 0.1268\n",
      "Epoch 15/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.8847 - loss: 0.1103 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8758 - val_loss: 0.1266\n",
      "Epoch 16/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.8858 - loss: 0.1084 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8628 - val_loss: 0.1379\n",
      "Epoch 17/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8839 - loss: 0.1108 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8492 - val_loss: 0.1548\n",
      "Epoch 18/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.8727 - loss: 0.1218 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8642 - val_loss: 0.1367\n",
      "Epoch 19/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - deviation_mae: 0.0053 - log_loss_mae: 0.8762 - loss: 0.1195 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8657 - val_loss: 0.1349\n",
      "Epoch 20/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8776 - loss: 0.1178 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8439 - val_loss: 0.1607\n",
      "Epoch 21/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.8644 - loss: 0.1310 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8687 - val_loss: 0.1317\n",
      "Epoch 22/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.8776 - loss: 0.1166 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8771 - val_loss: 0.1235\n",
      "Epoch 23/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0059 - log_loss_mae: 0.8839 - loss: 0.1097 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8778 - val_loss: 0.1240\n",
      "Epoch 24/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8855 - loss: 0.1085 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8809 - val_loss: 0.1208\n",
      "Epoch 25/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.8914 - loss: 0.1041 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8813 - val_loss: 0.1201\n",
      "Epoch 26/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8893 - loss: 0.1055 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8815 - val_loss: 0.1198\n",
      "Epoch 27/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.8908 - loss: 0.1043 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8811 - val_loss: 0.1212\n",
      "Epoch 28/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.8895 - loss: 0.1062 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8778 - val_loss: 0.1234\n",
      "Epoch 29/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8869 - loss: 0.1078 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8820 - val_loss: 0.1194\n",
      "Epoch 30/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.8890 - loss: 0.1068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8834 - val_loss: 0.1195\n",
      "Epoch 31/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0055 - log_loss_mae: 0.8931 - loss: 0.1021 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8828 - val_loss: 0.1184\n",
      "Epoch 32/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.8866 - loss: 0.1075 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8846 - val_loss: 0.1165\n",
      "Epoch 33/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.8941 - loss: 0.0999 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8852 - val_loss: 0.1163\n",
      "Epoch 34/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8912 - loss: 0.1036 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8872 - val_loss: 0.1143\n",
      "Epoch 35/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.8976 - loss: 0.0970 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8884 - val_loss: 0.1126\n",
      "Epoch 36/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.8942 - loss: 0.0991 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8923 - val_loss: 0.1092\n",
      "Epoch 37/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.8996 - loss: 0.0950 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8946 - val_loss: 0.1064\n",
      "Epoch 38/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0061 - log_loss_mae: 0.9006 - loss: 0.0943 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9016 - val_loss: 0.0999\n",
      "Epoch 39/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9064 - loss: 0.0889 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9098 - val_loss: 0.0919\n",
      "Epoch 40/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9153 - loss: 0.0804 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9162 - val_loss: 0.0849\n",
      "Epoch 41/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9189 - loss: 0.0755 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9352 - val_loss: 0.0647\n",
      "Epoch 42/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9389 - loss: 0.0559 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9589 - val_loss: 0.0405\n",
      "Epoch 43/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9486 - loss: 0.0467 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9479 - val_loss: 0.0521\n",
      "Epoch 44/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9497 - loss: 0.0449 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9608 - val_loss: 0.0384\n",
      "Epoch 45/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9426 - loss: 0.0498 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9321 - val_loss: 0.0607\n",
      "Epoch 46/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9260 - loss: 0.0692 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9684 - val_loss: 0.0294\n",
      "Epoch 47/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9528 - loss: 0.0423 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9654 - val_loss: 0.0324\n",
      "Epoch 48/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9599 - loss: 0.0345 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9741 - val_loss: 0.0241\n",
      "Epoch 49/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9636 - loss: 0.0305 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9657 - val_loss: 0.0322\n",
      "Epoch 50/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9657 - loss: 0.0289 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9716 - val_loss: 0.0271\n",
      "Epoch 51/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9678 - loss: 0.0262 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9703 - val_loss: 0.0283\n",
      "Epoch 52/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9646 - loss: 0.0300 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9742 - val_loss: 0.0237\n",
      "Epoch 53/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9718 - loss: 0.0223 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9716 - val_loss: 0.0268\n",
      "Epoch 54/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9697 - loss: 0.0255 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9793 - val_loss: 0.0186\n",
      "Epoch 55/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9688 - loss: 0.0258 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9780 - val_loss: 0.0195\n",
      "Epoch 56/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9722 - loss: 0.0225 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9784 - val_loss: 0.0188\n",
      "Epoch 57/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9757 - loss: 0.0191 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9794 - val_loss: 0.0186\n",
      "Epoch 58/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9764 - loss: 0.0186 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9811 - val_loss: 0.0168\n",
      "Epoch 59/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9759 - loss: 0.0192 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9815 - val_loss: 0.0162\n",
      "Epoch 60/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9762 - loss: 0.0191 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9814 - val_loss: 0.0161\n",
      "Epoch 61/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9770 - loss: 0.0180 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9802 - val_loss: 0.0171\n",
      "Epoch 62/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0060 - log_loss_mae: 0.9792 - loss: 0.0150 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9817 - val_loss: 0.0158\n",
      "Epoch 63/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9787 - loss: 0.0156 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9824 - val_loss: 0.0153\n",
      "Epoch 64/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9795 - loss: 0.0152 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9825 - val_loss: 0.0150\n",
      "Epoch 65/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0046 - log_loss_mae: 0.9794 - loss: 0.0160 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9831 - val_loss: 0.0144\n",
      "Epoch 66/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9776 - loss: 0.0174 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9832 - val_loss: 0.0143\n",
      "Epoch 67/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9792 - loss: 0.0153 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9736 - val_loss: 0.0246\n",
      "Epoch 68/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9728 - loss: 0.0218 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9822 - val_loss: 0.0152\n",
      "Epoch 69/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9773 - loss: 0.0169 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9749 - val_loss: 0.0223\n",
      "Epoch 70/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9748 - loss: 0.0199 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9766 - val_loss: 0.0197\n",
      "Epoch 71/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9743 - loss: 0.0204 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9811 - val_loss: 0.0165\n",
      "Epoch 72/800\n",
      "\u001b[1m2/7\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 1s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9828 - loss: 0.0121 \n",
      "Epoch 72: saving model to C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/model-72.weights.h5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9793 - loss: 0.0157 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9821 - val_loss: 0.0155\n",
      "Epoch 73/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9798 - loss: 0.0145 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9834 - val_loss: 0.0139\n",
      "Epoch 74/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9762 - loss: 0.0188 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9727 - val_loss: 0.0256\n",
      "Epoch 75/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9756 - loss: 0.0189 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9823 - val_loss: 0.0153\n",
      "Epoch 76/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9796 - loss: 0.0152 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9832 - val_loss: 0.0142\n",
      "Epoch 77/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9791 - loss: 0.0162 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9775 - val_loss: 0.0201\n",
      "Epoch 78/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9757 - loss: 0.0191 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9781 - val_loss: 0.0191\n",
      "Epoch 79/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9780 - loss: 0.0170 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9750 - val_loss: 0.0231\n",
      "Epoch 80/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9763 - loss: 0.0187 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9851 - val_loss: 0.0123\n",
      "Epoch 81/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9792 - loss: 0.0151 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9728 - val_loss: 0.0240\n",
      "Epoch 82/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9723 - loss: 0.0228 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9703 - val_loss: 0.0258\n",
      "Epoch 83/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9753 - loss: 0.0197 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9828 - val_loss: 0.0143\n",
      "Epoch 84/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9825 - loss: 0.0127 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9837 - val_loss: 0.0132\n",
      "Epoch 85/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9822 - loss: 0.0124 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9856 - val_loss: 0.0114\n",
      "Epoch 86/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9818 - loss: 0.0129 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9806 - val_loss: 0.0169\n",
      "Epoch 87/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9820 - loss: 0.0127 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9832 - val_loss: 0.0141\n",
      "Epoch 88/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9824 - loss: 0.0121 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9857 - val_loss: 0.0109\n",
      "Epoch 89/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9831 - loss: 0.0115 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9843 - val_loss: 0.0128\n",
      "Epoch 90/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9838 - loss: 0.0112 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9855 - val_loss: 0.0112\n",
      "Epoch 91/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9832 - loss: 0.0120 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9864 - val_loss: 0.0104\n",
      "Epoch 92/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0060 - log_loss_mae: 0.9841 - loss: 0.0099 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9877 - val_loss: 0.0093\n",
      "Epoch 93/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9825 - loss: 0.0125 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9771 - val_loss: 0.0196\n",
      "Epoch 94/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9782 - loss: 0.0165 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9834 - val_loss: 0.0137\n",
      "Epoch 95/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9807 - loss: 0.0143 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9879 - val_loss: 0.0086\n",
      "Epoch 96/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9823 - loss: 0.0121 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9893 - val_loss: 0.0074\n",
      "Epoch 97/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9844 - loss: 0.0102 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9885 - val_loss: 0.0081\n",
      "Epoch 98/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9857 - loss: 0.0092 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9835 - val_loss: 0.0135\n",
      "Epoch 99/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9836 - loss: 0.0111 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9888 - val_loss: 0.0076\n",
      "Epoch 100/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9848 - loss: 0.0098 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9858 - val_loss: 0.0110\n",
      "Epoch 101/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9850 - loss: 0.0096 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9885 - val_loss: 0.0080\n",
      "Epoch 102/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9855 - loss: 0.0098 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9881 - val_loss: 0.0082\n",
      "Epoch 103/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9860 - loss: 0.0089 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0065\n",
      "Epoch 104/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9868 - loss: 0.0081 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0066\n",
      "Epoch 105/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9879 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9898 - val_loss: 0.0067\n",
      "Epoch 106/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9874 - loss: 0.0071 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9908 - val_loss: 0.0057\n",
      "Epoch 107/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9877 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9848 - val_loss: 0.0117\n",
      "Epoch 108/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9831 - loss: 0.0121 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9863 - val_loss: 0.0104\n",
      "Epoch 109/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9845 - loss: 0.0104 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9885 - val_loss: 0.0082\n",
      "Epoch 110/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9859 - loss: 0.0092 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9846 - val_loss: 0.0122\n",
      "Epoch 111/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9841 - loss: 0.0106 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9883 - val_loss: 0.0079\n",
      "Epoch 112/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9852 - loss: 0.0095 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9874 - val_loss: 0.0089\n",
      "Epoch 113/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9854 - loss: 0.0093 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9896 - val_loss: 0.0069\n",
      "Epoch 114/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9851 - loss: 0.0095 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9903 - val_loss: 0.0062\n",
      "Epoch 115/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9881 - loss: 0.0066 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9895 - val_loss: 0.0068\n",
      "Epoch 116/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9873 - loss: 0.0078 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9895 - val_loss: 0.0069\n",
      "Epoch 117/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9878 - loss: 0.0070 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9900 - val_loss: 0.0063\n",
      "Epoch 118/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9872 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9857 - val_loss: 0.0110\n",
      "Epoch 119/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9865 - loss: 0.0084 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9834 - val_loss: 0.0133\n",
      "Epoch 120/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9847 - loss: 0.0100 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9882 - val_loss: 0.0083\n",
      "Epoch 121/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9879 - loss: 0.0070 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9870 - val_loss: 0.0096\n",
      "Epoch 122/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9870 - loss: 0.0079 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9893 - val_loss: 0.0072\n",
      "Epoch 123/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0059 - log_loss_mae: 0.9874 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9874 - val_loss: 0.0093\n",
      "Epoch 124/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9878 - loss: 0.0070 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0065\n",
      "Epoch 125/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9875 - loss: 0.0071 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9888 - val_loss: 0.0078\n",
      "Epoch 126/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0046 - log_loss_mae: 0.9880 - loss: 0.0075 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9889 - val_loss: 0.0077\n",
      "Epoch 127/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9874 - loss: 0.0076 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0067\n",
      "Epoch 128/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9880 - loss: 0.0064 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0065\n",
      "Epoch 129/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9882 - loss: 0.0065 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9869 - val_loss: 0.0090\n",
      "Epoch 130/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9876 - loss: 0.0076 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9880 - val_loss: 0.0082\n",
      "Epoch 131/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9875 - loss: 0.0076 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9833 - val_loss: 0.0135\n",
      "Epoch 132/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9841 - loss: 0.0105 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9845 - val_loss: 0.0122\n",
      "Epoch 133/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9853 - loss: 0.0096 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9906 - val_loss: 0.0059\n",
      "Epoch 134/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9886 - loss: 0.0064 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9857 - val_loss: 0.0110\n",
      "Epoch 135/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9865 - loss: 0.0084 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9889 - val_loss: 0.0077\n",
      "Epoch 136/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9873 - loss: 0.0076 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9907 - val_loss: 0.0059\n",
      "Epoch 137/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9882 - loss: 0.0065 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9907 - val_loss: 0.0058\n",
      "Epoch 138/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9879 - loss: 0.0065 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0070\n",
      "Epoch 139/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9866 - loss: 0.0082 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9901 - val_loss: 0.0063\n",
      "Epoch 140/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9862 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9860 - val_loss: 0.0106\n",
      "Epoch 141/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9848 - loss: 0.0095 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9874 - val_loss: 0.0092\n",
      "Epoch 142/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9864 - loss: 0.0083 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9904 - val_loss: 0.0061\n",
      "Epoch 143/800\n",
      "\u001b[1m5/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 1s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9868 - loss: 0.0081\n",
      "Epoch 143: saving model to C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/model-143.weights.h5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9860 - loss: 0.0088 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9800 - val_loss: 0.0171\n",
      "Epoch 144/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9818 - loss: 0.0132 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9777 - val_loss: 0.0197\n",
      "Epoch 145/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9762 - loss: 0.0187 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9821 - val_loss: 0.0140\n",
      "Epoch 146/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9819 - loss: 0.0130 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9856 - val_loss: 0.0111\n",
      "Epoch 147/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9861 - loss: 0.0091 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9898 - val_loss: 0.0065\n",
      "Epoch 148/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0046 - log_loss_mae: 0.9869 - loss: 0.0086 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9897 - val_loss: 0.0069\n",
      "Epoch 149/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9872 - loss: 0.0079 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9840 - val_loss: 0.0129\n",
      "Epoch 150/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9852 - loss: 0.0096 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9868 - val_loss: 0.0096\n",
      "Epoch 151/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9879 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9901 - val_loss: 0.0064\n",
      "Epoch 152/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9880 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0066\n",
      "Epoch 153/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9886 - loss: 0.0057 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9879 - val_loss: 0.0081\n",
      "Epoch 154/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9875 - loss: 0.0070 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9867 - val_loss: 0.0096\n",
      "Epoch 155/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9860 - loss: 0.0086 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9829 - val_loss: 0.0134\n",
      "Epoch 156/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9848 - loss: 0.0102 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9841 - val_loss: 0.0125\n",
      "Epoch 157/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9832 - loss: 0.0115 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9863 - val_loss: 0.0103\n",
      "Epoch 158/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9855 - loss: 0.0093 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9876 - val_loss: 0.0088\n",
      "Epoch 159/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9863 - loss: 0.0089 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0066\n",
      "Epoch 160/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9882 - loss: 0.0062 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9898 - val_loss: 0.0069\n",
      "Epoch 161/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9887 - loss: 0.0063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9909 - val_loss: 0.0055\n",
      "Epoch 162/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9877 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9830 - val_loss: 0.0137\n",
      "Epoch 163/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9820 - loss: 0.0125 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9887 - val_loss: 0.0075\n",
      "Epoch 164/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9820 - loss: 0.0127 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9773 - val_loss: 0.0200\n",
      "Epoch 165/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9775 - loss: 0.0175 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9758 - val_loss: 0.0208\n",
      "Epoch 166/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9793 - loss: 0.0159 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9840 - val_loss: 0.0127\n",
      "Epoch 167/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9841 - loss: 0.0106 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9890 - val_loss: 0.0076\n",
      "Epoch 168/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9823 - loss: 0.0127 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9756 - val_loss: 0.0215\n",
      "Epoch 169/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9771 - loss: 0.0179 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9729 - val_loss: 0.0241\n",
      "Epoch 170/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9771 - loss: 0.0174 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9867 - val_loss: 0.0099\n",
      "Epoch 171/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9789 - loss: 0.0158 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9852 - val_loss: 0.0107\n",
      "Epoch 172/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9830 - loss: 0.0115 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9753 - val_loss: 0.0219\n",
      "Epoch 173/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9733 - loss: 0.0209 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9081 - val_loss: 0.0896\n",
      "Epoch 174/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8510 - loss: 0.1463 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9494 - val_loss: 0.0446\n",
      "Epoch 175/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.8715 - loss: 0.1248 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.8852 - val_loss: 0.1168\n",
      "Epoch 176/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9237 - loss: 0.0723 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9817 - val_loss: 0.0157\n",
      "Epoch 177/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.8836 - loss: 0.1132 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9342 - val_loss: 0.0591\n",
      "Epoch 178/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9411 - loss: 0.0531 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9288 - val_loss: 0.0670\n",
      "Epoch 179/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9380 - loss: 0.0575 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9619 - val_loss: 0.0340\n",
      "Epoch 180/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9588 - loss: 0.0357 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9501 - val_loss: 0.0470\n",
      "Epoch 181/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9598 - loss: 0.0350 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9690 - val_loss: 0.0254\n",
      "Epoch 182/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9701 - loss: 0.0245 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9709 - val_loss: 0.0246\n",
      "Epoch 183/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9761 - loss: 0.0185 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9737 - val_loss: 0.0230\n",
      "Epoch 184/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9740 - loss: 0.0205 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9592 - val_loss: 0.0389\n",
      "Epoch 185/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9749 - loss: 0.0199 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9886 - val_loss: 0.0082\n",
      "Epoch 186/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9868 - loss: 0.0082 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9870 - val_loss: 0.0095\n",
      "Epoch 187/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9871 - loss: 0.0077 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9858 - val_loss: 0.0111\n",
      "Epoch 188/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9836 - loss: 0.0114 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9883 - val_loss: 0.0083\n",
      "Epoch 189/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9822 - loss: 0.0121 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9842 - val_loss: 0.0125\n",
      "Epoch 190/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9851 - loss: 0.0094 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9878 - val_loss: 0.0084\n",
      "Epoch 191/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9872 - loss: 0.0079 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9881 - val_loss: 0.0085\n",
      "Epoch 192/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9881 - loss: 0.0069 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9875 - val_loss: 0.0091\n",
      "Epoch 193/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9878 - loss: 0.0072 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9893 - val_loss: 0.0071\n",
      "Epoch 194/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9871 - loss: 0.0077 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9850 - val_loss: 0.0118\n",
      "Epoch 195/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9849 - loss: 0.0097 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9896 - val_loss: 0.0069\n",
      "Epoch 196/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9886 - loss: 0.0064 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0072\n",
      "Epoch 197/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9878 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9852 - val_loss: 0.0115\n",
      "Epoch 198/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9865 - loss: 0.0087 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9896 - val_loss: 0.0070\n",
      "Epoch 199/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9889 - loss: 0.0063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9901 - val_loss: 0.0065\n",
      "Epoch 200/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9878 - loss: 0.0074 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9828 - val_loss: 0.0138\n",
      "Epoch 201/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9850 - loss: 0.0101 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9849 - val_loss: 0.0119\n",
      "Epoch 202/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9856 - loss: 0.0098 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9889 - val_loss: 0.0076\n",
      "Epoch 203/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9879 - loss: 0.0065 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9903 - val_loss: 0.0063\n",
      "Epoch 204/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9889 - loss: 0.0063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9905 - val_loss: 0.0061\n",
      "Epoch 205/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9884 - loss: 0.0063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9896 - val_loss: 0.0069\n",
      "Epoch 206/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9882 - loss: 0.0064 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9887 - val_loss: 0.0079\n",
      "Epoch 207/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9887 - loss: 0.0061 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9904 - val_loss: 0.0061\n",
      "Epoch 208/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9862 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9897 - val_loss: 0.0067\n",
      "Epoch 209/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0046 - log_loss_mae: 0.9865 - loss: 0.0088 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9815 - val_loss: 0.0149\n",
      "Epoch 210/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9835 - loss: 0.0110 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9808 - val_loss: 0.0161\n",
      "Epoch 211/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9827 - loss: 0.0118 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9874 - val_loss: 0.0087\n",
      "Epoch 212/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9863 - loss: 0.0086 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9823 - val_loss: 0.0148\n",
      "Epoch 213/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9815 - loss: 0.0133 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9823 - val_loss: 0.0146\n",
      "Epoch 214/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9844 - loss: 0.0107 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9900 - val_loss: 0.0066\n",
      "Epoch 215/800\n",
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 1s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9874 - loss: 0.0072\n",
      "Epoch 215: saving model to C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/model-215.weights.h5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9864 - loss: 0.0083 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9891 - val_loss: 0.0075\n",
      "Epoch 216/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9876 - loss: 0.0076 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9858 - val_loss: 0.0109\n",
      "Epoch 217/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9855 - loss: 0.0091 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9820 - val_loss: 0.0142\n",
      "Epoch 218/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9820 - loss: 0.0125 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0073\n",
      "Epoch 219/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9856 - loss: 0.0092 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9821 - val_loss: 0.0146\n",
      "Epoch 220/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9838 - loss: 0.0112 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9808 - val_loss: 0.0157\n",
      "Epoch 221/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9813 - loss: 0.0132 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9858 - val_loss: 0.0111\n",
      "Epoch 222/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9848 - loss: 0.0103 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9855 - val_loss: 0.0110\n",
      "Epoch 223/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9858 - loss: 0.0093 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9842 - val_loss: 0.0123\n",
      "Epoch 224/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9837 - loss: 0.0109 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9854 - val_loss: 0.0113\n",
      "Epoch 225/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9863 - loss: 0.0087 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9873 - val_loss: 0.0089\n",
      "Epoch 226/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9865 - loss: 0.0081 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9893 - val_loss: 0.0073\n",
      "Epoch 227/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9872 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9875 - val_loss: 0.0086\n",
      "Epoch 228/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9871 - loss: 0.0073 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9890 - val_loss: 0.0075\n",
      "Epoch 229/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9872 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9873 - val_loss: 0.0091\n",
      "Epoch 230/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9868 - loss: 0.0081 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9875 - val_loss: 0.0091\n",
      "Epoch 231/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9883 - loss: 0.0066 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9894 - val_loss: 0.0071\n",
      "Epoch 232/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9882 - loss: 0.0069 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9878 - val_loss: 0.0087\n",
      "Epoch 233/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9880 - loss: 0.0069 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9883 - val_loss: 0.0083\n",
      "Epoch 234/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9881 - loss: 0.0063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9864 - val_loss: 0.0101\n",
      "Epoch 235/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9856 - loss: 0.0091 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9813 - val_loss: 0.0159\n",
      "Epoch 236/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9825 - loss: 0.0124 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9821 - val_loss: 0.0148\n",
      "Epoch 237/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9835 - loss: 0.0114 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9863 - val_loss: 0.0102\n",
      "Epoch 238/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9866 - loss: 0.0082 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9877 - val_loss: 0.0088\n",
      "Epoch 239/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9889 - loss: 0.0061 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9842 - val_loss: 0.0126\n",
      "Epoch 240/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0047 - log_loss_mae: 0.9860 - loss: 0.0093 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9841 - val_loss: 0.0127\n",
      "Epoch 241/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9865 - loss: 0.0086 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9882 - val_loss: 0.0084\n",
      "Epoch 242/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9871 - loss: 0.0076 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9907 - val_loss: 0.0059\n",
      "Epoch 243/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9883 - loss: 0.0062 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9893 - val_loss: 0.0070\n",
      "Epoch 244/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9885 - loss: 0.0062 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9814 - val_loss: 0.0148\n",
      "Epoch 245/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9829 - loss: 0.0115 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9905 - val_loss: 0.0061\n",
      "Epoch 246/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9873 - loss: 0.0069 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9822 - val_loss: 0.0142\n",
      "Epoch 247/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9836 - loss: 0.0111 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9901 - val_loss: 0.0063\n",
      "Epoch 248/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9887 - loss: 0.0065 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9890 - val_loss: 0.0073\n",
      "Epoch 249/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0059 - log_loss_mae: 0.9882 - loss: 0.0059 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0074\n",
      "Epoch 250/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9887 - loss: 0.0057 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0073\n",
      "Epoch 251/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9889 - loss: 0.0058 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9884 - val_loss: 0.0077\n",
      "Epoch 252/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9879 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9886 - val_loss: 0.0074\n",
      "Epoch 253/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9875 - loss: 0.0069 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9870 - val_loss: 0.0095\n",
      "Epoch 254/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9879 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9888 - val_loss: 0.0075\n",
      "Epoch 255/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9871 - loss: 0.0074 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9902 - val_loss: 0.0064\n",
      "Epoch 256/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9886 - loss: 0.0059 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9903 - val_loss: 0.0063\n",
      "Epoch 257/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9890 - loss: 0.0061 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9878 - val_loss: 0.0089\n",
      "Epoch 258/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9861 - loss: 0.0084 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9858 - val_loss: 0.0102\n",
      "Epoch 259/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9867 - loss: 0.0079 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9844 - val_loss: 0.0124\n",
      "Epoch 260/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9867 - loss: 0.0083 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9838 - val_loss: 0.0128\n",
      "Epoch 261/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9856 - loss: 0.0090 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0075\n",
      "Epoch 262/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9874 - loss: 0.0071 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9869 - val_loss: 0.0097\n",
      "Epoch 263/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9872 - loss: 0.0078 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9857 - val_loss: 0.0102\n",
      "Epoch 264/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9863 - loss: 0.0083 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9892 - val_loss: 0.0072\n",
      "Epoch 265/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9895 - loss: 0.0055 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9906 - val_loss: 0.0059\n",
      "Epoch 266/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9880 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9865 - val_loss: 0.0102\n",
      "Epoch 267/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9879 - loss: 0.0072 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9907 - val_loss: 0.0058\n",
      "Epoch 268/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9870 - loss: 0.0075 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9854 - val_loss: 0.0105\n",
      "Epoch 269/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9862 - loss: 0.0082 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9888 - val_loss: 0.0076\n",
      "Epoch 270/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9878 - loss: 0.0073 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9906 - val_loss: 0.0060\n",
      "Epoch 271/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9891 - loss: 0.0059 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9898 - val_loss: 0.0067\n",
      "Epoch 272/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9878 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9828 - val_loss: 0.0132\n",
      "Epoch 273/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9841 - loss: 0.0108 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9816 - val_loss: 0.0155\n",
      "Epoch 274/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9833 - loss: 0.0117 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9884 - val_loss: 0.0079\n",
      "Epoch 275/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9877 - loss: 0.0073 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9896 - val_loss: 0.0069\n",
      "Epoch 276/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0058 - log_loss_mae: 0.9874 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9850 - val_loss: 0.0119\n",
      "Epoch 277/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9864 - loss: 0.0084 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9854 - val_loss: 0.0111\n",
      "Epoch 278/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9854 - loss: 0.0094 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9774 - val_loss: 0.0199\n",
      "Epoch 279/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9812 - loss: 0.0138 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9829 - val_loss: 0.0140\n",
      "Epoch 280/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9856 - loss: 0.0094 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9869 - val_loss: 0.0092\n",
      "Epoch 281/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9863 - loss: 0.0086 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9881 - val_loss: 0.0079\n",
      "Epoch 282/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9876 - loss: 0.0070 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9882 - val_loss: 0.0085\n",
      "Epoch 283/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9880 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0067\n",
      "Epoch 284/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9890 - loss: 0.0055 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9869 - val_loss: 0.0098\n",
      "Epoch 285/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9869 - loss: 0.0078 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9859 - val_loss: 0.0103\n",
      "Epoch 286/800\n",
      "\u001b[1m4/7\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2s/step - deviation_mae: 0.0060 - log_loss_mae: 0.9838 - loss: 0.0102\n",
      "Epoch 286: saving model to C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/model-286.weights.h5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9851 - loss: 0.0093 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0062\n",
      "Epoch 287/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9893 - loss: 0.0056 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9855 - val_loss: 0.0114\n",
      "Epoch 288/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9859 - loss: 0.0094 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9873 - val_loss: 0.0093\n",
      "Epoch 289/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9865 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9877 - val_loss: 0.0091\n",
      "Epoch 290/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9887 - loss: 0.0062 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9882 - val_loss: 0.0078\n",
      "Epoch 291/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9872 - loss: 0.0074 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9903 - val_loss: 0.0060\n",
      "Epoch 292/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9871 - loss: 0.0075 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9834 - val_loss: 0.0134\n",
      "Epoch 293/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9850 - loss: 0.0095 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9833 - val_loss: 0.0130\n",
      "Epoch 294/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9822 - loss: 0.0124 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9859 - val_loss: 0.0102\n",
      "Epoch 295/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9824 - loss: 0.0121 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9829 - val_loss: 0.0140\n",
      "Epoch 296/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9841 - loss: 0.0107 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9883 - val_loss: 0.0077\n",
      "Epoch 297/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9864 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9906 - val_loss: 0.0058\n",
      "Epoch 298/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9854 - loss: 0.0095 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9771 - val_loss: 0.0200\n",
      "Epoch 299/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9806 - loss: 0.0143 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9894 - val_loss: 0.0070\n",
      "Epoch 300/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9869 - loss: 0.0080 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9791 - val_loss: 0.0183\n",
      "Epoch 301/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9808 - loss: 0.0146 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9825 - val_loss: 0.0142\n",
      "Epoch 302/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9848 - loss: 0.0101 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9905 - val_loss: 0.0059\n",
      "Epoch 303/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9877 - loss: 0.0072 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9796 - val_loss: 0.0175\n",
      "Epoch 304/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9817 - loss: 0.0129 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9866 - val_loss: 0.0102\n",
      "Epoch 305/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9802 - loss: 0.0146 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9882 - val_loss: 0.0083\n",
      "Epoch 306/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9851 - loss: 0.0098 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9891 - val_loss: 0.0073\n",
      "Epoch 307/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9875 - loss: 0.0073 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9879 - val_loss: 0.0084\n",
      "Epoch 308/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0051 - log_loss_mae: 0.9874 - loss: 0.0074 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9876 - val_loss: 0.0090\n",
      "Epoch 309/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0048 - log_loss_mae: 0.9891 - loss: 0.0061 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9906 - val_loss: 0.0058\n",
      "Epoch 310/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0053 - log_loss_mae: 0.9895 - loss: 0.0053 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9907 - val_loss: 0.0058\n",
      "Epoch 311/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9897 - loss: 0.0053 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9871 - val_loss: 0.0095\n",
      "Epoch 312/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9881 - loss: 0.0068 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9881 - val_loss: 0.0086\n",
      "Epoch 313/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0055 - log_loss_mae: 0.9872 - loss: 0.0073 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9863 - val_loss: 0.0098\n",
      "Epoch 314/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9867 - loss: 0.0082 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9837 - val_loss: 0.0133\n",
      "Epoch 315/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9853 - loss: 0.0100 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9802 - val_loss: 0.0161\n",
      "Epoch 316/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9823 - loss: 0.0124 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9887 - val_loss: 0.0077\n",
      "Epoch 317/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9858 - loss: 0.0084 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9833 - val_loss: 0.0134\n",
      "Epoch 318/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9846 - loss: 0.0103 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9829 - val_loss: 0.0137\n",
      "Epoch 319/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0057 - log_loss_mae: 0.9824 - loss: 0.0119 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9874 - val_loss: 0.0092\n",
      "Epoch 320/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2s/step - deviation_mae: 0.0050 - log_loss_mae: 0.9883 - loss: 0.0067 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9899 - val_loss: 0.0063\n",
      "Epoch 321/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9884 - loss: 0.0063 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9792 - val_loss: 0.0171\n",
      "Epoch 322/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - deviation_mae: 0.0054 - log_loss_mae: 0.9814 - loss: 0.0131 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9897 - val_loss: 0.0070\n",
      "Epoch 323/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0049 - log_loss_mae: 0.9881 - loss: 0.0070 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9888 - val_loss: 0.0077\n",
      "Epoch 324/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 2s/step - deviation_mae: 0.0056 - log_loss_mae: 0.9866 - loss: 0.0077 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9867 - val_loss: 0.0095\n",
      "Epoch 325/800\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2s/step - deviation_mae: 0.0052 - log_loss_mae: 0.9875 - loss: 0.0073 - val_deviation_mae: 0.0038 - val_log_loss_mae: 0.9900 - val_loss: 0.0064\n",
      "Epoch 326/800\n",
      "\u001b[1m1/7\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 1s/step - deviation_mae: 0.0043 - log_loss_mae: 0.9911 - loss: 0.0046"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 20\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mmean_mae\u001b[38;5;66;03m#combL_val_mse            \u001b[39;00m\n\u001b[0;32m     14\u001b[0m               \u001b[38;5;66;03m#,metrics=[log_likelihood_maxScaling]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m               ,metrics\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m     16\u001b[0m                   \u001b[38;5;66;03m#mean_mae,\u001b[39;00m\n\u001b[0;32m     17\u001b[0m                   deviation_mae,logLoss_mae]\n\u001b[0;32m     18\u001b[0m               , optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m---> 20\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, \n\u001b[0;32m     21\u001b[0m                     \u001b[38;5;66;03m#batch[0],batch[1], #verbose=2,\u001b[39;00m\n\u001b[0;32m     22\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m     23\u001b[0m                     \u001b[38;5;66;03m#validation_data=(test_batch[0],test_batch[1]),\u001b[39;00m\n\u001b[0;32m     24\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     25\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback]\n\u001b[0;32m     26\u001b[0m                     \u001b[38;5;66;03m#callbacks=[lr_callback]\u001b[39;00m\n\u001b[0;32m     27\u001b[0m                     )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    319\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 320\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    321\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    322\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/model-{epoch:02d}.weights.h5\",\n",
    "    save_weights_only=True,  # Set to False if you want to save the entire model\n",
    "    save_freq=100 * 5,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#fit values first and in second stage fit for variance\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=mean_mae#combL_val_mse            \n",
    "              #,metrics=[log_likelihood_maxScaling]\n",
    "              ,metrics=[\n",
    "                  #mean_mae,\n",
    "                  deviation_mae,logLoss_mae]\n",
    "              , optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    #validation_data=(test_batch[0],test_batch[1]),\n",
    "                    epochs=800, batch_size=batch_size,\n",
    "                    callbacks=[checkpoint_callback]\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('325_meanOnly_0_07-0_06.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - deviation_mae: 0.0061 - log_loss_mae: 0.9871 - loss: 110.9748 - mean_mae: 0.0068 - val_deviation_mae: 0.0558 - val_log_loss_mae: 0.9340 - val_loss: 1010.2534 - val_mean_mae: 0.0070\n",
      "Epoch 2/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0568 - log_loss_mae: 0.9331 - loss: 1029.3176 - mean_mae: 0.0068 - val_deviation_mae: 0.0115 - val_log_loss_mae: 0.9798 - val_loss: 208.0411 - val_mean_mae: 0.0070\n",
      "Epoch 3/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0128 - log_loss_mae: 0.9787 - loss: 231.6100 - mean_mae: 0.0068 - val_deviation_mae: 0.0359 - val_log_loss_mae: 0.9573 - val_loss: 650.6017 - val_mean_mae: 0.0070\n",
      "Epoch 4/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0372 - log_loss_mae: 0.9562 - loss: 674.3026 - mean_mae: 0.0068 - val_deviation_mae: 0.0345 - val_log_loss_mae: 0.9584 - val_loss: 624.9545 - val_mean_mae: 0.0070\n",
      "Epoch 5/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0358 - log_loss_mae: 0.9573 - loss: 648.7146 - mean_mae: 0.0068 - val_deviation_mae: 0.0178 - val_log_loss_mae: 0.9740 - val_loss: 321.7739 - val_mean_mae: 0.0070\n",
      "Epoch 6/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - deviation_mae: 0.0191 - log_loss_mae: 0.9729 - loss: 345.9440 - mean_mae: 0.0068 - val_deviation_mae: 0.0091 - val_log_loss_mae: 0.9813 - val_loss: 164.5125 - val_mean_mae: 0.0070\n",
      "Epoch 7/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0104 - log_loss_mae: 0.9801 - loss: 188.8195 - mean_mae: 0.0068 - val_deviation_mae: 0.0158 - val_log_loss_mae: 0.9741 - val_loss: 285.8477 - val_mean_mae: 0.0070\n",
      "Epoch 8/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0173 - log_loss_mae: 0.9727 - loss: 314.0250 - mean_mae: 0.0068 - val_deviation_mae: 0.0128 - val_log_loss_mae: 0.9774 - val_loss: 232.0235 - val_mean_mae: 0.0070\n",
      "Epoch 9/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0143 - log_loss_mae: 0.9760 - loss: 259.3196 - mean_mae: 0.0068 - val_deviation_mae: 0.0057 - val_log_loss_mae: 0.9851 - val_loss: 103.9649 - val_mean_mae: 0.0070\n",
      "Epoch 10/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0070 - log_loss_mae: 0.9841 - loss: 126.2941 - mean_mae: 0.0068 - val_deviation_mae: 0.0105 - val_log_loss_mae: 0.9812 - val_loss: 189.7143 - val_mean_mae: 0.0070\n",
      "Epoch 11/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0118 - log_loss_mae: 0.9800 - loss: 213.7703 - mean_mae: 0.0068 - val_deviation_mae: 0.0138 - val_log_loss_mae: 0.9782 - val_loss: 250.5507 - val_mean_mae: 0.0070\n",
      "Epoch 12/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0152 - log_loss_mae: 0.9771 - loss: 274.9420 - mean_mae: 0.0068 - val_deviation_mae: 0.0126 - val_log_loss_mae: 0.9795 - val_loss: 228.9978 - val_mean_mae: 0.0070\n",
      "Epoch 13/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0140 - log_loss_mae: 0.9784 - loss: 253.3223 - mean_mae: 0.0068 - val_deviation_mae: 0.0085 - val_log_loss_mae: 0.9835 - val_loss: 153.9303 - val_mean_mae: 0.0070\n",
      "Epoch 14/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0098 - log_loss_mae: 0.9825 - loss: 176.8809 - mean_mae: 0.0068 - val_deviation_mae: 0.0051 - val_log_loss_mae: 0.9866 - val_loss: 93.2296 - val_mean_mae: 0.0070\n",
      "Epoch 15/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0063 - log_loss_mae: 0.9855 - loss: 114.9574 - mean_mae: 0.0068 - val_deviation_mae: 0.0082 - val_log_loss_mae: 0.9834 - val_loss: 147.7343 - val_mean_mae: 0.0070\n",
      "Epoch 16/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0095 - log_loss_mae: 0.9822 - loss: 171.5104 - mean_mae: 0.0068 - val_deviation_mae: 0.0089 - val_log_loss_mae: 0.9827 - val_loss: 161.7874 - val_mean_mae: 0.0070\n",
      "Epoch 17/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0103 - log_loss_mae: 0.9815 - loss: 186.0210 - mean_mae: 0.0068 - val_deviation_mae: 0.0073 - val_log_loss_mae: 0.9847 - val_loss: 132.0210 - val_mean_mae: 0.0070\n",
      "Epoch 18/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0086 - log_loss_mae: 0.9836 - loss: 155.3212 - mean_mae: 0.0068 - val_deviation_mae: 0.0050 - val_log_loss_mae: 0.9874 - val_loss: 90.6290 - val_mean_mae: 0.0070\n",
      "Epoch 19/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0062 - log_loss_mae: 0.9865 - loss: 111.5393 - mean_mae: 0.0068 - val_deviation_mae: 0.0068 - val_log_loss_mae: 0.9861 - val_loss: 122.7022 - val_mean_mae: 0.0070\n",
      "Epoch 20/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0080 - log_loss_mae: 0.9851 - loss: 144.1354 - mean_mae: 0.0068 - val_deviation_mae: 0.0080 - val_log_loss_mae: 0.9852 - val_loss: 144.0399 - val_mean_mae: 0.0070\n",
      "Epoch 21/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0092 - log_loss_mae: 0.9841 - loss: 166.4292 - mean_mae: 0.0068 - val_deviation_mae: 0.0075 - val_log_loss_mae: 0.9855 - val_loss: 136.5973 - val_mean_mae: 0.0070\n",
      "Epoch 22/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0088 - log_loss_mae: 0.9845 - loss: 158.6753 - mean_mae: 0.0068 - val_deviation_mae: 0.0060 - val_log_loss_mae: 0.9868 - val_loss: 108.9449 - val_mean_mae: 0.0070\n",
      "Epoch 23/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0072 - log_loss_mae: 0.9859 - loss: 129.6780 - mean_mae: 0.0068 - val_deviation_mae: 0.0051 - val_log_loss_mae: 0.9874 - val_loss: 92.2881 - val_mean_mae: 0.0070\n",
      "Epoch 24/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0063 - log_loss_mae: 0.9864 - loss: 113.8589 - mean_mae: 0.0068 - val_deviation_mae: 0.0062 - val_log_loss_mae: 0.9860 - val_loss: 113.0151 - val_mean_mae: 0.0070\n",
      "Epoch 25/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0075 - log_loss_mae: 0.9850 - loss: 135.7811 - mean_mae: 0.0068 - val_deviation_mae: 0.0065 - val_log_loss_mae: 0.9858 - val_loss: 117.7262 - val_mean_mae: 0.0070\n",
      "Epoch 26/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0078 - log_loss_mae: 0.9847 - loss: 140.6830 - mean_mae: 0.0068 - val_deviation_mae: 0.0057 - val_log_loss_mae: 0.9868 - val_loss: 103.5989 - val_mean_mae: 0.0070\n",
      "Epoch 27/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0070 - log_loss_mae: 0.9857 - loss: 126.0411 - mean_mae: 0.0068 - val_deviation_mae: 0.0049 - val_log_loss_mae: 0.9880 - val_loss: 88.7362 - val_mean_mae: 0.0070\n",
      "Epoch 28/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9870 - loss: 109.1735 - mean_mae: 0.0068 - val_deviation_mae: 0.0057 - val_log_loss_mae: 0.9875 - val_loss: 102.7560 - val_mean_mae: 0.0070\n",
      "Epoch 29/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0068 - log_loss_mae: 0.9865 - loss: 123.5918 - mean_mae: 0.0068 - val_deviation_mae: 0.0060 - val_log_loss_mae: 0.9872 - val_loss: 108.6226 - val_mean_mae: 0.0070\n",
      "Epoch 30/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0072 - log_loss_mae: 0.9863 - loss: 129.9287 - mean_mae: 0.0068 - val_deviation_mae: 0.0055 - val_log_loss_mae: 0.9876 - val_loss: 99.9443 - val_mean_mae: 0.0070\n",
      "Epoch 31/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0067 - log_loss_mae: 0.9866 - loss: 120.9470 - mean_mae: 0.0068 - val_deviation_mae: 0.0048 - val_log_loss_mae: 0.9879 - val_loss: 87.4208 - val_mean_mae: 0.0070\n",
      "Epoch 32/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9870 - loss: 108.0688 - mean_mae: 0.0068 - val_deviation_mae: 0.0053 - val_log_loss_mae: 0.9872 - val_loss: 95.5110 - val_mean_mae: 0.0070\n",
      "Epoch 33/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0065 - log_loss_mae: 0.9861 - loss: 117.7510 - mean_mae: 0.0068 - val_deviation_mae: 0.0055 - val_log_loss_mae: 0.9869 - val_loss: 99.2692 - val_mean_mae: 0.0070\n",
      "Epoch 34/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0067 - log_loss_mae: 0.9858 - loss: 121.7952 - mean_mae: 0.0068 - val_deviation_mae: 0.0050 - val_log_loss_mae: 0.9875 - val_loss: 90.9338 - val_mean_mae: 0.0070\n",
      "Epoch 35/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - deviation_mae: 0.0062 - log_loss_mae: 0.9865 - loss: 112.7221 - mean_mae: 0.0068 - val_deviation_mae: 0.0048 - val_log_loss_mae: 0.9881 - val_loss: 86.7261 - val_mean_mae: 0.0070\n",
      "Epoch 36/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9871 - loss: 107.9910 - mean_mae: 0.0068 - val_deviation_mae: 0.0052 - val_log_loss_mae: 0.9880 - val_loss: 93.6516 - val_mean_mae: 0.0070\n",
      "Epoch 37/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0064 - log_loss_mae: 0.9869 - loss: 115.7887 - mean_mae: 0.0068 - val_deviation_mae: 0.0051 - val_log_loss_mae: 0.9880 - val_loss: 92.7786 - val_mean_mae: 0.0070\n",
      "Epoch 38/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0064 - log_loss_mae: 0.9869 - loss: 115.1326 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9881 - val_loss: 85.7582 - val_mean_mae: 0.0070\n",
      "Epoch 39/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9871 - loss: 107.8040 - mean_mae: 0.0068 - val_deviation_mae: 0.0048 - val_log_loss_mae: 0.9878 - val_loss: 86.5961 - val_mean_mae: 0.0070\n",
      "Epoch 40/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 176ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9868 - loss: 108.2768 - mean_mae: 0.0068 - val_deviation_mae: 0.0050 - val_log_loss_mae: 0.9874 - val_loss: 91.2805 - val_mean_mae: 0.0070\n",
      "Epoch 41/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0062 - log_loss_mae: 0.9863 - loss: 113.1824 - mean_mae: 0.0068 - val_deviation_mae: 0.0049 - val_log_loss_mae: 0.9876 - val_loss: 88.3759 - val_mean_mae: 0.0070\n",
      "Epoch 42/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0061 - log_loss_mae: 0.9866 - loss: 110.1068 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.0272 - val_mean_mae: 0.0070\n",
      "Epoch 43/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.9194 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9881 - val_loss: 86.0073 - val_mean_mae: 0.0070\n",
      "Epoch 44/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9871 - loss: 108.3758 - mean_mae: 0.0068 - val_deviation_mae: 0.0049 - val_log_loss_mae: 0.9881 - val_loss: 87.8679 - val_mean_mae: 0.0070\n",
      "Epoch 45/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0061 - log_loss_mae: 0.9870 - loss: 110.1717 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9881 - val_loss: 84.7633 - val_mean_mae: 0.0070\n",
      "Epoch 46/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9871 - loss: 106.5165 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.2078 - val_mean_mae: 0.0070\n",
      "Epoch 47/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.4749 - mean_mae: 0.0068 - val_deviation_mae: 0.0048 - val_log_loss_mae: 0.9877 - val_loss: 87.1725 - val_mean_mae: 0.0070\n",
      "Epoch 48/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0060 - log_loss_mae: 0.9867 - loss: 108.6051 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9878 - val_loss: 86.0141 - val_mean_mae: 0.0070\n",
      "Epoch 49/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9868 - loss: 107.2547 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.0543 - val_mean_mae: 0.0070\n",
      "Epoch 50/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9871 - loss: 105.0512 - mean_mae: 0.0068 - val_deviation_mae: 0.0048 - val_log_loss_mae: 0.9880 - val_loss: 86.2329 - val_mean_mae: 0.0070\n",
      "Epoch 51/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9870 - loss: 107.3198 - mean_mae: 0.0068 - val_deviation_mae: 0.0048 - val_log_loss_mae: 0.9880 - val_loss: 86.1714 - val_mean_mae: 0.0070\n",
      "Epoch 52/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9870 - loss: 107.2630 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.1143 - val_mean_mae: 0.0070\n",
      "Epoch 53/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9871 - loss: 105.0931 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9879 - val_loss: 85.1367 - val_mean_mae: 0.0070\n",
      "Epoch 54/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9869 - loss: 106.2431 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9878 - val_loss: 85.6327 - val_mean_mae: 0.0070\n",
      "Epoch 55/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9868 - loss: 106.8409 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.1203 - val_mean_mae: 0.0070\n",
      "Epoch 56/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.2813 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9880 - val_loss: 84.2898 - val_mean_mae: 0.0070\n",
      "Epoch 57/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9871 - loss: 105.6247 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9880 - val_loss: 84.9809 - val_mean_mae: 0.0070\n",
      "Epoch 58/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0059 - log_loss_mae: 0.9870 - loss: 106.4881 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9880 - val_loss: 84.2263 - val_mean_mae: 0.0070\n",
      "Epoch 59/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.5658 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9875 - val_mean_mae: 0.0070\n",
      "Epoch 60/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.1678 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9879 - val_loss: 84.8054 - val_mean_mae: 0.0070\n",
      "Epoch 61/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9869 - loss: 105.9486 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9879 - val_loss: 84.4866 - val_mean_mae: 0.0070\n",
      "Epoch 62/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9869 - loss: 105.5573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9646 - val_mean_mae: 0.0070\n",
      "Epoch 63/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9703 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9880 - val_loss: 84.5798 - val_mean_mae: 0.0070\n",
      "Epoch 64/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.5964 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9880 - val_loss: 84.4887 - val_mean_mae: 0.0070\n",
      "Epoch 65/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.4739 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9789 - val_mean_mae: 0.0070\n",
      "Epoch 66/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9441 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9879 - val_loss: 84.3443 - val_mean_mae: 0.0070\n",
      "Epoch 67/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9869 - loss: 105.3470 - mean_mae: 0.0068 - val_deviation_mae: 0.0047 - val_log_loss_mae: 0.9879 - val_loss: 84.2943 - val_mean_mae: 0.0070\n",
      "Epoch 68/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9869 - loss: 105.3300 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8244 - val_mean_mae: 0.0070\n",
      "Epoch 69/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9140 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9818 - val_mean_mae: 0.0070\n",
      "Epoch 70/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.1732 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.0245 - val_mean_mae: 0.0070\n",
      "Epoch 71/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.2576 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.7538 - val_mean_mae: 0.0070\n",
      "Epoch 72/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9353 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.9447 - val_mean_mae: 0.0070\n",
      "Epoch 73/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.0737 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 84.0942 - val_mean_mae: 0.0070\n",
      "Epoch 74/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9869 - loss: 105.1778 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8508 - val_mean_mae: 0.0070\n",
      "Epoch 75/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9132 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9609 - val_mean_mae: 0.0070\n",
      "Epoch 76/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.0066 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 84.0911 - val_mean_mae: 0.0070\n",
      "Epoch 77/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.1124 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9164 - val_mean_mae: 0.0070\n",
      "Epoch 78/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9127 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.9919 - val_mean_mae: 0.0070\n",
      "Epoch 79/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9826 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 84.0363 - val_mean_mae: 0.0070\n",
      "Epoch 80/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 105.0447 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8394 - val_mean_mae: 0.0070\n",
      "Epoch 81/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8885 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8522 - val_mean_mae: 0.0070\n",
      "Epoch 82/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9603 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8591 - val_mean_mae: 0.0070\n",
      "Epoch 83/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9977 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.7690 - val_mean_mae: 0.0070\n",
      "Epoch 84/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8861 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8681 - val_mean_mae: 0.0070\n",
      "Epoch 85/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9526 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8929 - val_mean_mae: 0.0070\n",
      "Epoch 86/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9515 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8339 - val_mean_mae: 0.0070\n",
      "Epoch 87/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8708 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9193 - val_mean_mae: 0.0070\n",
      "Epoch 88/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9434 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.9109 - val_mean_mae: 0.0070\n",
      "Epoch 89/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9088 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8977 - val_mean_mae: 0.0070\n",
      "Epoch 90/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8831 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.9462 - val_mean_mae: 0.0070\n",
      "Epoch 91/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9357 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8706 - val_mean_mae: 0.0070\n",
      "Epoch 92/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8800 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8515 - val_mean_mae: 0.0070\n",
      "Epoch 93/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8876 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8539 - val_mean_mae: 0.0070\n",
      "Epoch 94/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9102 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8222 - val_mean_mae: 0.0070\n",
      "Epoch 95/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8658 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8678 - val_mean_mae: 0.0070\n",
      "Epoch 96/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9002 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8626 - val_mean_mae: 0.0070\n",
      "Epoch 97/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8804 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8662 - val_mean_mae: 0.0070\n",
      "Epoch 98/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8688 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8978 - val_mean_mae: 0.0070\n",
      "Epoch 99/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8930 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8819 - val_mean_mae: 0.0070\n",
      "Epoch 100/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8632 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8965 - val_mean_mae: 0.0070\n",
      "Epoch 101/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8815 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8789 - val_mean_mae: 0.0070\n",
      "Epoch 102/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8725 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8584 - val_mean_mae: 0.0070\n",
      "Epoch 103/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8631 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8590 - val_mean_mae: 0.0070\n",
      "Epoch 104/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8798 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8477 - val_mean_mae: 0.0070\n",
      "Epoch 105/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8595 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8663 - val_mean_mae: 0.0070\n",
      "Epoch 106/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8749 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8630 - val_mean_mae: 0.0070\n",
      "Epoch 107/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8620 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8721 - val_mean_mae: 0.0070\n",
      "Epoch 108/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8668 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8763 - val_mean_mae: 0.0070\n",
      "Epoch 109/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8649 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8761 - val_mean_mae: 0.0070\n",
      "Epoch 110/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8609 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8758 - val_mean_mae: 0.0070\n",
      "Epoch 111/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8654 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8610 - val_mean_mae: 0.0070\n",
      "Epoch 112/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8574 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8566 - val_mean_mae: 0.0070\n",
      "Epoch 113/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8657 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8523 - val_mean_mae: 0.0070\n",
      "Epoch 114/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8571 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8610 - val_mean_mae: 0.0070\n",
      "Epoch 115/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8634 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8587 - val_mean_mae: 0.0070\n",
      "Epoch 116/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8638 - val_mean_mae: 0.0070\n",
      "Epoch 117/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8624 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8672 - val_mean_mae: 0.0070\n",
      "Epoch 118/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8730 - val_mean_mae: 0.0070\n",
      "Epoch 119/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8612 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8646 - val_mean_mae: 0.0070\n",
      "Epoch 120/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8574 - val_mean_mae: 0.0070\n",
      "Epoch 121/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8603 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8551 - val_mean_mae: 0.0070\n",
      "Epoch 122/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8563 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8594 - val_mean_mae: 0.0070\n",
      "Epoch 123/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8597 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8573 - val_mean_mae: 0.0070\n",
      "Epoch 124/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8601 - val_mean_mae: 0.0070\n",
      "Epoch 125/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8593 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8643 - val_mean_mae: 0.0070\n",
      "Epoch 126/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8563 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8685 - val_mean_mae: 0.0070\n",
      "Epoch 127/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8585 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8630 - val_mean_mae: 0.0070\n",
      "Epoch 128/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8594 - val_mean_mae: 0.0070\n",
      "Epoch 129/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8576 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8596 - val_mean_mae: 0.0070\n",
      "Epoch 130/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8570 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8597 - val_mean_mae: 0.0070\n",
      "Epoch 131/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8572 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8583 - val_mean_mae: 0.0070\n",
      "Epoch 132/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8570 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8624 - val_mean_mae: 0.0070\n",
      "Epoch 133/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8563 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8669 - val_mean_mae: 0.0070\n",
      "Epoch 134/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8668 - val_mean_mae: 0.0070\n",
      "Epoch 135/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8639 - val_mean_mae: 0.0070\n",
      "Epoch 136/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8563 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8602 - val_mean_mae: 0.0070\n",
      "Epoch 137/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8587 - val_mean_mae: 0.0070\n",
      "Epoch 138/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8577 - val_mean_mae: 0.0070\n",
      "Epoch 139/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8588 - val_mean_mae: 0.0070\n",
      "Epoch 140/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8627 - val_mean_mae: 0.0070\n",
      "Epoch 141/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8655 - val_mean_mae: 0.0070\n",
      "Epoch 142/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8648 - val_mean_mae: 0.0070\n",
      "Epoch 143/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8622 - val_mean_mae: 0.0070\n",
      "Epoch 144/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8598 - val_mean_mae: 0.0070\n",
      "Epoch 145/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8576 - val_mean_mae: 0.0070\n",
      "Epoch 146/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8575 - val_mean_mae: 0.0070\n",
      "Epoch 147/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8601 - val_mean_mae: 0.0070\n",
      "Epoch 148/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8623 - val_mean_mae: 0.0070\n",
      "Epoch 149/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8631 - val_mean_mae: 0.0070\n",
      "Epoch 150/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8626 - val_mean_mae: 0.0070\n",
      "Epoch 151/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8607 - val_mean_mae: 0.0070\n",
      "Epoch 152/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8590 - val_mean_mae: 0.0070\n",
      "Epoch 153/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8593 - val_mean_mae: 0.0070\n",
      "Epoch 154/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8609 - val_mean_mae: 0.0070\n",
      "Epoch 155/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8612 - val_mean_mae: 0.0070\n",
      "Epoch 156/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8620 - val_mean_mae: 0.0070\n",
      "Epoch 157/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8615 - val_mean_mae: 0.0070\n",
      "Epoch 158/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8591 - val_mean_mae: 0.0070\n",
      "Epoch 159/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8581 - val_mean_mae: 0.0070\n",
      "Epoch 160/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8588 - val_mean_mae: 0.0070\n",
      "Epoch 161/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8598 - val_mean_mae: 0.0070\n",
      "Epoch 162/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8608 - val_mean_mae: 0.0070\n",
      "Epoch 163/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 128ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8617 - val_mean_mae: 0.0070\n",
      "Epoch 164/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8610 - val_mean_mae: 0.0070\n",
      "Epoch 165/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8596 - val_mean_mae: 0.0070\n",
      "Epoch 166/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8602 - val_mean_mae: 0.0070\n",
      "Epoch 167/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8602 - val_mean_mae: 0.0070\n",
      "Epoch 168/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8605 - val_mean_mae: 0.0070\n",
      "Epoch 169/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8621 - val_mean_mae: 0.0070\n",
      "Epoch 170/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8639 - val_mean_mae: 0.0070\n",
      "Epoch 171/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8628 - val_mean_mae: 0.0070\n",
      "Epoch 172/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8610 - val_mean_mae: 0.0070\n",
      "Epoch 173/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8605 - val_mean_mae: 0.0070\n",
      "Epoch 174/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8592 - val_mean_mae: 0.0070\n",
      "Epoch 175/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8596 - val_mean_mae: 0.0070\n",
      "Epoch 176/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8617 - val_mean_mae: 0.0070\n",
      "Epoch 177/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8631 - val_mean_mae: 0.0070\n",
      "Epoch 178/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8622 - val_mean_mae: 0.0070\n",
      "Epoch 179/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8607 - val_mean_mae: 0.0070\n",
      "Epoch 180/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8581 - val_mean_mae: 0.0070\n",
      "Epoch 181/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8583 - val_mean_mae: 0.0070\n",
      "Epoch 182/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8600 - val_mean_mae: 0.0070\n",
      "Epoch 183/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8558 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8623 - val_mean_mae: 0.0070\n",
      "Epoch 184/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8618 - val_mean_mae: 0.0070\n",
      "Epoch 185/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8607 - val_mean_mae: 0.0070\n",
      "Epoch 186/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8558 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8599 - val_mean_mae: 0.0070\n",
      "Epoch 187/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8585 - val_mean_mae: 0.0070\n",
      "Epoch 188/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8593 - val_mean_mae: 0.0070\n",
      "Epoch 189/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8606 - val_mean_mae: 0.0070\n",
      "Epoch 190/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8558 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8614 - val_mean_mae: 0.0070\n",
      "Epoch 191/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8624 - val_mean_mae: 0.0070\n",
      "Epoch 192/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8613 - val_mean_mae: 0.0070\n",
      "Epoch 193/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8558 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8596 - val_mean_mae: 0.0070\n",
      "Epoch 194/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8597 - val_mean_mae: 0.0070\n",
      "Epoch 195/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8609 - val_mean_mae: 0.0070\n",
      "Epoch 196/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8633 - val_mean_mae: 0.0070\n",
      "Epoch 197/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8647 - val_mean_mae: 0.0070\n",
      "Epoch 198/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8631 - val_mean_mae: 0.0070\n",
      "Epoch 199/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8611 - val_mean_mae: 0.0070\n",
      "Epoch 200/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8606 - val_mean_mae: 0.0070\n",
      "Epoch 201/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8596 - val_mean_mae: 0.0070\n",
      "Epoch 202/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8606 - val_mean_mae: 0.0070\n",
      "Epoch 203/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8635 - val_mean_mae: 0.0070\n",
      "Epoch 204/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8632 - val_mean_mae: 0.0070\n",
      "Epoch 205/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8618 - val_mean_mae: 0.0070\n",
      "Epoch 206/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8610 - val_mean_mae: 0.0070\n",
      "Epoch 207/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8590 - val_mean_mae: 0.0070\n",
      "Epoch 208/800\n",
      "\n",
      "Epoch 208: saving model to C:/Users/uic33116/Documents/documents/ariel-data-challenge-2024/training_full_model/model-208.weights.h5\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8589 - val_mean_mae: 0.0070\n",
      "Epoch 209/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8623 - val_mean_mae: 0.0070\n",
      "Epoch 210/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8620 - val_mean_mae: 0.0070\n",
      "Epoch 211/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8615 - val_mean_mae: 0.0070\n",
      "Epoch 212/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8620 - val_mean_mae: 0.0070\n",
      "Epoch 213/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8565 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8600 - val_mean_mae: 0.0070\n",
      "Epoch 214/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8584 - val_mean_mae: 0.0070\n",
      "Epoch 215/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8650 - val_mean_mae: 0.0070\n",
      "Epoch 216/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8632 - val_mean_mae: 0.0070\n",
      "Epoch 217/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8635 - val_mean_mae: 0.0070\n",
      "Epoch 218/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8565 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8635 - val_mean_mae: 0.0070\n",
      "Epoch 219/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8579 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8576 - val_mean_mae: 0.0070\n",
      "Epoch 220/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8557 - val_mean_mae: 0.0070\n",
      "Epoch 221/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8599 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8660 - val_mean_mae: 0.0070\n",
      "Epoch 222/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8620 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8628 - val_mean_mae: 0.0070\n",
      "Epoch 223/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8671 - val_mean_mae: 0.0070\n",
      "Epoch 224/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8638 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8737 - val_mean_mae: 0.0070\n",
      "Epoch 225/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8627 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8646 - val_mean_mae: 0.0070\n",
      "Epoch 226/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8575 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8591 - val_mean_mae: 0.0070\n",
      "Epoch 227/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8642 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8615 - val_mean_mae: 0.0070\n",
      "Epoch 228/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8595 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8618 - val_mean_mae: 0.0070\n",
      "Epoch 229/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8584 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8611 - val_mean_mae: 0.0070\n",
      "Epoch 230/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8615 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8617 - val_mean_mae: 0.0070\n",
      "Epoch 231/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8574 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8696 - val_mean_mae: 0.0070\n",
      "Epoch 232/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8637 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8603 - val_mean_mae: 0.0070\n",
      "Epoch 233/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8589 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8583 - val_mean_mae: 0.0070\n",
      "Epoch 234/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8605 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8630 - val_mean_mae: 0.0070\n",
      "Epoch 235/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8610 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8578 - val_mean_mae: 0.0070\n",
      "Epoch 236/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8577 - val_mean_mae: 0.0070\n",
      "Epoch 237/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8597 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8632 - val_mean_mae: 0.0070\n",
      "Epoch 238/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8588 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8617 - val_mean_mae: 0.0070\n",
      "Epoch 239/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8603 - val_mean_mae: 0.0070\n",
      "Epoch 240/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8581 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8621 - val_mean_mae: 0.0070\n",
      "Epoch 241/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8575 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8585 - val_mean_mae: 0.0070\n",
      "Epoch 242/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8581 - val_mean_mae: 0.0070\n",
      "Epoch 243/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8574 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8639 - val_mean_mae: 0.0070\n",
      "Epoch 244/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8630 - val_mean_mae: 0.0070\n",
      "Epoch 245/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8560 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8612 - val_mean_mae: 0.0070\n",
      "Epoch 246/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8570 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8637 - val_mean_mae: 0.0070\n",
      "Epoch 247/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8590 - val_mean_mae: 0.0070\n",
      "Epoch 248/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8608 - val_mean_mae: 0.0070\n",
      "Epoch 249/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8559 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8625 - val_mean_mae: 0.0070\n",
      "Epoch 250/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8625 - val_mean_mae: 0.0070\n",
      "Epoch 251/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8565 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8609 - val_mean_mae: 0.0070\n",
      "Epoch 252/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8549 - val_mean_mae: 0.0070\n",
      "Epoch 253/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8561 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8547 - val_mean_mae: 0.0070\n",
      "Epoch 254/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8578 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8663 - val_mean_mae: 0.0070\n",
      "Epoch 255/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8618 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8634 - val_mean_mae: 0.0070\n",
      "Epoch 256/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8638 - val_mean_mae: 0.0070\n",
      "Epoch 257/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8626 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8705 - val_mean_mae: 0.0070\n",
      "Epoch 258/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8671 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8555 - val_mean_mae: 0.0070\n",
      "Epoch 259/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8567 - val_mean_mae: 0.0070\n",
      "Epoch 260/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8711 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8687 - val_mean_mae: 0.0070\n",
      "Epoch 261/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8703 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8616 - val_mean_mae: 0.0070\n",
      "Epoch 262/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8574 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8716 - val_mean_mae: 0.0070\n",
      "Epoch 263/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8768 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8728 - val_mean_mae: 0.0070\n",
      "Epoch 264/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8645 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8716 - val_mean_mae: 0.0070\n",
      "Epoch 265/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8655 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8626 - val_mean_mae: 0.0070\n",
      "Epoch 266/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8709 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8558 - val_mean_mae: 0.0070\n",
      "Epoch 267/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8589 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8704 - val_mean_mae: 0.0070\n",
      "Epoch 268/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8721 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8573 - val_mean_mae: 0.0070\n",
      "Epoch 269/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8620 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8611 - val_mean_mae: 0.0070\n",
      "Epoch 270/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8659 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8676 - val_mean_mae: 0.0070\n",
      "Epoch 271/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8635 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8662 - val_mean_mae: 0.0070\n",
      "Epoch 272/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8613 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8631 - val_mean_mae: 0.0070\n",
      "Epoch 273/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8643 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8590 - val_mean_mae: 0.0070\n",
      "Epoch 274/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8578 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8613 - val_mean_mae: 0.0070\n",
      "Epoch 275/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8619 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8546 - val_mean_mae: 0.0070\n",
      "Epoch 276/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8582 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8561 - val_mean_mae: 0.0070\n",
      "Epoch 277/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8585 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8654 - val_mean_mae: 0.0070\n",
      "Epoch 278/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8623 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8601 - val_mean_mae: 0.0070\n",
      "Epoch 279/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8595 - val_mean_mae: 0.0070\n",
      "Epoch 280/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8597 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8740 - val_mean_mae: 0.0070\n",
      "Epoch 281/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8714 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8628 - val_mean_mae: 0.0070\n",
      "Epoch 282/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8568 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8617 - val_mean_mae: 0.0070\n",
      "Epoch 283/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8629 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8841 - val_mean_mae: 0.0070\n",
      "Epoch 284/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8845 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8587 - val_mean_mae: 0.0070\n",
      "Epoch 285/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8568 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8690 - val_mean_mae: 0.0070\n",
      "Epoch 286/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8793 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8924 - val_mean_mae: 0.0070\n",
      "Epoch 287/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8900 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8660 - val_mean_mae: 0.0070\n",
      "Epoch 288/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8588 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8921 - val_mean_mae: 0.0070\n",
      "Epoch 289/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9081 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8782 - val_mean_mae: 0.0070\n",
      "Epoch 290/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8785 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8749 - val_mean_mae: 0.0070\n",
      "Epoch 291/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8805 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8722 - val_mean_mae: 0.0070\n",
      "Epoch 292/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8930 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8542 - val_mean_mae: 0.0070\n",
      "Epoch 293/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8598 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8774 - val_mean_mae: 0.0070\n",
      "Epoch 294/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 188ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8790 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8565 - val_mean_mae: 0.0070\n",
      "Epoch 295/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8618 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8587 - val_mean_mae: 0.0070\n",
      "Epoch 296/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8736 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8529 - val_mean_mae: 0.0070\n",
      "Epoch 297/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8649 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8567 - val_mean_mae: 0.0070\n",
      "Epoch 298/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8683 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8514 - val_mean_mae: 0.0070\n",
      "Epoch 299/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8645 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8522 - val_mean_mae: 0.0070\n",
      "Epoch 300/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8608 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8627 - val_mean_mae: 0.0070\n",
      "Epoch 301/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8661 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8527 - val_mean_mae: 0.0070\n",
      "Epoch 302/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8579 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8516 - val_mean_mae: 0.0070\n",
      "Epoch 303/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8640 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8590 - val_mean_mae: 0.0070\n",
      "Epoch 304/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8627 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8529 - val_mean_mae: 0.0070\n",
      "Epoch 305/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8528 - val_mean_mae: 0.0070\n",
      "Epoch 306/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8614 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8631 - val_mean_mae: 0.0070\n",
      "Epoch 307/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8622 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8551 - val_mean_mae: 0.0070\n",
      "Epoch 308/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8579 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8555 - val_mean_mae: 0.0070\n",
      "Epoch 309/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8596 - val_mean_mae: 0.0070\n",
      "Epoch 310/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8592 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8533 - val_mean_mae: 0.0070\n",
      "Epoch 311/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8580 - val_mean_mae: 0.0070\n",
      "Epoch 312/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8593 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8542 - val_mean_mae: 0.0070\n",
      "Epoch 313/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8558 - val_mean_mae: 0.0070\n",
      "Epoch 314/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8565 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8567 - val_mean_mae: 0.0070\n",
      "Epoch 315/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8568 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8549 - val_mean_mae: 0.0070\n",
      "Epoch 316/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8520 - val_mean_mae: 0.0070\n",
      "Epoch 317/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8511 - val_mean_mae: 0.0070\n",
      "Epoch 318/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8511 - val_mean_mae: 0.0070\n",
      "Epoch 319/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8557 - val_mean_mae: 0.0070\n",
      "Epoch 320/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8536 - val_mean_mae: 0.0070\n",
      "Epoch 321/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8591 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8624 - val_mean_mae: 0.0070\n",
      "Epoch 322/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8641 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8531 - val_mean_mae: 0.0070\n",
      "Epoch 323/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8600 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8571 - val_mean_mae: 0.0070\n",
      "Epoch 324/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8581 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8522 - val_mean_mae: 0.0070\n",
      "Epoch 325/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8517 - val_mean_mae: 0.0070\n",
      "Epoch 326/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8567 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8562 - val_mean_mae: 0.0070\n",
      "Epoch 327/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8578 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8547 - val_mean_mae: 0.0070\n",
      "Epoch 328/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8588 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8603 - val_mean_mae: 0.0070\n",
      "Epoch 329/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8544 - val_mean_mae: 0.0070\n",
      "Epoch 330/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8500 - val_mean_mae: 0.0070\n",
      "Epoch 331/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8513 - val_mean_mae: 0.0070\n",
      "Epoch 332/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8568 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8541 - val_mean_mae: 0.0070\n",
      "Epoch 333/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8562 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8584 - val_mean_mae: 0.0070\n",
      "Epoch 334/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8563 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8592 - val_mean_mae: 0.0070\n",
      "Epoch 335/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8526 - val_mean_mae: 0.0070\n",
      "Epoch 336/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8577 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8570 - val_mean_mae: 0.0070\n",
      "Epoch 337/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8600 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8535 - val_mean_mae: 0.0070\n",
      "Epoch 338/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8609 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8647 - val_mean_mae: 0.0070\n",
      "Epoch 339/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8620 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8572 - val_mean_mae: 0.0070\n",
      "Epoch 340/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8592 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8564 - val_mean_mae: 0.0070\n",
      "Epoch 341/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8573 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8522 - val_mean_mae: 0.0070\n",
      "Epoch 342/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8524 - val_mean_mae: 0.0070\n",
      "Epoch 343/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8577 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8595 - val_mean_mae: 0.0070\n",
      "Epoch 344/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8562 - val_mean_mae: 0.0070\n",
      "Epoch 345/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8575 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8546 - val_mean_mae: 0.0070\n",
      "Epoch 346/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8517 - val_mean_mae: 0.0070\n",
      "Epoch 347/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8567 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8504 - val_mean_mae: 0.0070\n",
      "Epoch 348/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8583 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8616 - val_mean_mae: 0.0070\n",
      "Epoch 349/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8604 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8611 - val_mean_mae: 0.0070\n",
      "Epoch 350/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 158ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8585 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8593 - val_mean_mae: 0.0070\n",
      "Epoch 351/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8565 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8538 - val_mean_mae: 0.0070\n",
      "Epoch 352/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8567 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8492 - val_mean_mae: 0.0070\n",
      "Epoch 353/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8607 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8670 - val_mean_mae: 0.0070\n",
      "Epoch 354/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8654 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8621 - val_mean_mae: 0.0070\n",
      "Epoch 355/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8606 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8620 - val_mean_mae: 0.0070\n",
      "Epoch 356/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8574 - val_mean_mae: 0.0070\n",
      "Epoch 357/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8578 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8491 - val_mean_mae: 0.0070\n",
      "Epoch 358/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8647 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8667 - val_mean_mae: 0.0070\n",
      "Epoch 359/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8703 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8598 - val_mean_mae: 0.0070\n",
      "Epoch 360/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8612 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8585 - val_mean_mae: 0.0070\n",
      "Epoch 361/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8577 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8643 - val_mean_mae: 0.0070\n",
      "Epoch 362/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8665 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8483 - val_mean_mae: 0.0070\n",
      "Epoch 363/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8603 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8489 - val_mean_mae: 0.0070\n",
      "Epoch 364/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8572 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8586 - val_mean_mae: 0.0070\n",
      "Epoch 365/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 130ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8610 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8592 - val_mean_mae: 0.0070\n",
      "Epoch 366/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8564 - val_mean_mae: 0.0070\n",
      "Epoch 367/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8519 - val_mean_mae: 0.0070\n",
      "Epoch 368/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 164ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8502 - val_mean_mae: 0.0070\n",
      "Epoch 369/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8577 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8627 - val_mean_mae: 0.0070\n",
      "Epoch 370/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8650 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8583 - val_mean_mae: 0.0070\n",
      "Epoch 371/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8602 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8673 - val_mean_mae: 0.0070\n",
      "Epoch 372/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8631 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8601 - val_mean_mae: 0.0070\n",
      "Epoch 373/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8566 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8551 - val_mean_mae: 0.0070\n",
      "Epoch 374/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8581 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8666 - val_mean_mae: 0.0070\n",
      "Epoch 375/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8701 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8559 - val_mean_mae: 0.0070\n",
      "Epoch 376/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8648 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8667 - val_mean_mae: 0.0070\n",
      "Epoch 377/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8652 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8609 - val_mean_mae: 0.0070\n",
      "Epoch 378/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8575 - val_mean_mae: 0.0070\n",
      "Epoch 379/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 152ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8636 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8746 - val_mean_mae: 0.0070\n",
      "Epoch 380/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8823 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8567 - val_mean_mae: 0.0070\n",
      "Epoch 381/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8708 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8613 - val_mean_mae: 0.0070\n",
      "Epoch 382/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8618 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8638 - val_mean_mae: 0.0070\n",
      "Epoch 383/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8599 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8667 - val_mean_mae: 0.0070\n",
      "Epoch 384/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8663 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8591 - val_mean_mae: 0.0070\n",
      "Epoch 385/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8606 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8539 - val_mean_mae: 0.0070\n",
      "Epoch 386/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8595 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8539 - val_mean_mae: 0.0070\n",
      "Epoch 387/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8633 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8565 - val_mean_mae: 0.0070\n",
      "Epoch 388/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8582 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8628 - val_mean_mae: 0.0070\n",
      "Epoch 389/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8591 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8649 - val_mean_mae: 0.0070\n",
      "Epoch 390/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8738 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8804 - val_mean_mae: 0.0070\n",
      "Epoch 391/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8778 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8627 - val_mean_mae: 0.0070\n",
      "Epoch 392/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8608 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8614 - val_mean_mae: 0.0070\n",
      "Epoch 393/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8613 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8758 - val_mean_mae: 0.0070\n",
      "Epoch 394/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8741 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8634 - val_mean_mae: 0.0070\n",
      "Epoch 395/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8694 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8652 - val_mean_mae: 0.0070\n",
      "Epoch 396/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8603 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8572 - val_mean_mae: 0.0070\n",
      "Epoch 397/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8578 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8491 - val_mean_mae: 0.0070\n",
      "Epoch 398/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8619 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8540 - val_mean_mae: 0.0070\n",
      "Epoch 399/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8623 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8551 - val_mean_mae: 0.0070\n",
      "Epoch 400/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8594 - val_mean_mae: 0.0070\n",
      "Epoch 401/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8582 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8603 - val_mean_mae: 0.0070\n",
      "Epoch 402/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8611 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8524 - val_mean_mae: 0.0070\n",
      "Epoch 403/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8593 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8516 - val_mean_mae: 0.0070\n",
      "Epoch 404/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8592 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8498 - val_mean_mae: 0.0070\n",
      "Epoch 405/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8567 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8566 - val_mean_mae: 0.0070\n",
      "Epoch 406/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8604 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8856 - val_mean_mae: 0.0070\n",
      "Epoch 407/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8879 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8690 - val_mean_mae: 0.0070\n",
      "Epoch 408/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8742 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8699 - val_mean_mae: 0.0070\n",
      "Epoch 409/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8659 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8572 - val_mean_mae: 0.0070\n",
      "Epoch 410/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8567 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8563 - val_mean_mae: 0.0070\n",
      "Epoch 411/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8686 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8926 - val_mean_mae: 0.0070\n",
      "Epoch 412/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.9000 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8721 - val_mean_mae: 0.0070\n",
      "Epoch 413/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8750 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8700 - val_mean_mae: 0.0070\n",
      "Epoch 414/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8604 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8651 - val_mean_mae: 0.0070\n",
      "Epoch 415/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8616 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8606 - val_mean_mae: 0.0070\n",
      "Epoch 416/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8710 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8665 - val_mean_mae: 0.0070\n",
      "Epoch 417/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8712 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8577 - val_mean_mae: 0.0070\n",
      "Epoch 418/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8591 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8604 - val_mean_mae: 0.0070\n",
      "Epoch 419/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8660 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8568 - val_mean_mae: 0.0070\n",
      "Epoch 420/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 155ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8621 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8530 - val_mean_mae: 0.0070\n",
      "Epoch 421/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8581 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8552 - val_mean_mae: 0.0070\n",
      "Epoch 422/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8614 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8593 - val_mean_mae: 0.0070\n",
      "Epoch 423/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8615 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8562 - val_mean_mae: 0.0070\n",
      "Epoch 424/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8604 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8557 - val_mean_mae: 0.0070\n",
      "Epoch 425/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8568 - val_mean_mae: 0.0070\n",
      "Epoch 426/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8574 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8572 - val_mean_mae: 0.0070\n",
      "Epoch 427/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8663 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8830 - val_mean_mae: 0.0070\n",
      "Epoch 428/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8813 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8762 - val_mean_mae: 0.0070\n",
      "Epoch 429/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8796 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8792 - val_mean_mae: 0.0070\n",
      "Epoch 430/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8707 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8631 - val_mean_mae: 0.0070\n",
      "Epoch 431/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8616 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8561 - val_mean_mae: 0.0070\n",
      "Epoch 432/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8572 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8536 - val_mean_mae: 0.0070\n",
      "Epoch 433/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8609 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8558 - val_mean_mae: 0.0070\n",
      "Epoch 434/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8712 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8855 - val_mean_mae: 0.0070\n",
      "Epoch 435/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8803 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8778 - val_mean_mae: 0.0070\n",
      "Epoch 436/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8649 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8672 - val_mean_mae: 0.0070\n",
      "Epoch 437/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8594 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8586 - val_mean_mae: 0.0070\n",
      "Epoch 438/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8688 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8403 - val_mean_mae: 0.0070\n",
      "Epoch 439/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8633 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8493 - val_mean_mae: 0.0070\n",
      "Epoch 440/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8608 - val_mean_mae: 0.0070\n",
      "Epoch 441/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8590 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8637 - val_mean_mae: 0.0070\n",
      "Epoch 442/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8609 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8571 - val_mean_mae: 0.0070\n",
      "Epoch 443/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8608 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8467 - val_mean_mae: 0.0070\n",
      "Epoch 444/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8582 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8478 - val_mean_mae: 0.0070\n",
      "Epoch 445/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8569 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8540 - val_mean_mae: 0.0070\n",
      "Epoch 446/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8563 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8609 - val_mean_mae: 0.0070\n",
      "Epoch 447/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8564 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8627 - val_mean_mae: 0.0070\n",
      "Epoch 448/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8570 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8563 - val_mean_mae: 0.0070\n",
      "Epoch 449/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8598 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8676 - val_mean_mae: 0.0070\n",
      "Epoch 450/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8738 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8629 - val_mean_mae: 0.0070\n",
      "Epoch 451/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8822 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8865 - val_mean_mae: 0.0070\n",
      "Epoch 452/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8854 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8736 - val_mean_mae: 0.0070\n",
      "Epoch 453/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8671 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8701 - val_mean_mae: 0.0070\n",
      "Epoch 454/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8589 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8608 - val_mean_mae: 0.0070\n",
      "Epoch 455/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8584 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8525 - val_mean_mae: 0.0070\n",
      "Epoch 456/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8619 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8600 - val_mean_mae: 0.0070\n",
      "Epoch 457/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8623 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8615 - val_mean_mae: 0.0070\n",
      "Epoch 458/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8602 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8632 - val_mean_mae: 0.0070\n",
      "Epoch 459/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8576 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8666 - val_mean_mae: 0.0070\n",
      "Epoch 460/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8607 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8627 - val_mean_mae: 0.0070\n",
      "Epoch 461/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8662 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8722 - val_mean_mae: 0.0070\n",
      "Epoch 462/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8735 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8599 - val_mean_mae: 0.0070\n",
      "Epoch 463/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8663 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8692 - val_mean_mae: 0.0070\n",
      "Epoch 464/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8645 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8641 - val_mean_mae: 0.0070\n",
      "Epoch 465/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8606 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8577 - val_mean_mae: 0.0070\n",
      "Epoch 466/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8575 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8522 - val_mean_mae: 0.0070\n",
      "Epoch 467/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8586 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8496 - val_mean_mae: 0.0070\n",
      "Epoch 468/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8645 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8809 - val_mean_mae: 0.0070\n",
      "Epoch 469/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8804 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8824 - val_mean_mae: 0.0070\n",
      "Epoch 470/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8846 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8945 - val_mean_mae: 0.0070\n",
      "Epoch 471/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8863 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8622 - val_mean_mae: 0.0070\n",
      "Epoch 472/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8663 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8445 - val_mean_mae: 0.0070\n",
      "Epoch 473/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8585 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8434 - val_mean_mae: 0.0070\n",
      "Epoch 474/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8642 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8497 - val_mean_mae: 0.0070\n",
      "Epoch 475/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8702 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8800 - val_mean_mae: 0.0070\n",
      "Epoch 476/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8745 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8787 - val_mean_mae: 0.0070\n",
      "Epoch 477/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 173ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8612 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8712 - val_mean_mae: 0.0070\n",
      "Epoch 478/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8692 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9879 - val_loss: 83.8726 - val_mean_mae: 0.0070\n",
      "Epoch 479/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8884 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8486 - val_mean_mae: 0.0070\n",
      "Epoch 480/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8744 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8661 - val_mean_mae: 0.0070\n",
      "Epoch 481/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8645 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8741 - val_mean_mae: 0.0070\n",
      "Epoch 482/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8595 - mean_mae: 0.0068 - val_deviation_mae: 0.0046 - val_log_loss_mae: 0.9880 - val_loss: 83.8734 - val_mean_mae: 0.0070\n",
      "Epoch 483/800\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - deviation_mae: 0.0058 - log_loss_mae: 0.9870 - loss: 104.8663 - mean_mae: 0.0068"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 8\u001b[0m\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39mdeviation_mas\u001b[38;5;66;03m#combL_onlyVar_mse#combL_valVarWeighted_mse #combL_valVar_mse            \u001b[39;00m\n\u001b[0;32m      4\u001b[0m               \u001b[38;5;66;03m#,metrics=[log_likelihood_maxScaling]\u001b[39;00m\n\u001b[0;32m      5\u001b[0m               ,metrics\u001b[38;5;241m=\u001b[39m[mean_mae,deviation_mae,logLoss_mae]\n\u001b[0;32m      6\u001b[0m               , optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m----> 8\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;66;03m#train_dataset, \u001b[39;00m\n\u001b[0;32m      9\u001b[0m                     batch[\u001b[38;5;241m0\u001b[39m],batch[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;66;03m#verbose=2,\u001b[39;00m\n\u001b[0;32m     10\u001b[0m                     \u001b[38;5;66;03m#validation_data=test_dataset,\u001b[39;00m\n\u001b[0;32m     11\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m(test_batch[\u001b[38;5;241m0\u001b[39m],test_batch[\u001b[38;5;241m1\u001b[39m]),\n\u001b[0;32m     12\u001b[0m                     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     13\u001b[0m                     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback]\n\u001b[0;32m     14\u001b[0m                     \u001b[38;5;66;03m#callbacks=[lr_callback]\u001b[39;00m\n\u001b[0;32m     15\u001b[0m                     )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:345\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    336\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    337\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    343\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[1;32m--> 345\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m    346\u001b[0m     x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    347\u001b[0m     y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m    348\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39mval_sample_weight,\n\u001b[0;32m    349\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mvalidation_batch_size \u001b[38;5;129;01mor\u001b[39;00m batch_size,\n\u001b[0;32m    350\u001b[0m     steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[0;32m    351\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[0;32m    352\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    353\u001b[0m     _use_cached_eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m )\n\u001b[0;32m    355\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    357\u001b[0m }\n\u001b[0;32m    358\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:433\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    432\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[1;32m--> 433\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_function(iterator)\n\u001b[0;32m    434\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    435\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1552\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1550\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1552\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1553\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1554\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1555\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1556\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1557\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1558\u001b[0m   )\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1560\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1561\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1562\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1566\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1567\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\uic33116\\AppData\\Local\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# use trained model for values as an init point\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss=deviation_mas#combL_onlyVar_mse#combL_valVarWeighted_mse #combL_valVar_mse            \n",
    "              #,metrics=[log_likelihood_maxScaling]\n",
    "              ,metrics=[mean_mae,deviation_mae,logLoss_mae]\n",
    "              , optimizer=optimizer)\n",
    "\n",
    "history = model.fit(#train_dataset, \n",
    "                    batch[0],batch[1], #verbose=2,\n",
    "                    #validation_data=test_dataset,\n",
    "                    validation_data=(test_batch[0],test_batch[1]),\n",
    "                    epochs=800, batch_size=batch_size,\n",
    "                    callbacks=[checkpoint_callback]\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('originalData_fullM_866_epochs_blowup59_63.keras')\n",
    "# Save weights\n",
    "model.save_weights('325_meanOnly_0_07-0_06.weights.h5')\n",
    "\n",
    "# Load weights\n",
    "#loaded_weights = model.load_weights('170_epochs_accLoss_reluActivation_23_23.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('120_epochs_accLoss31_30.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try couldn't fit the values, just predicted mean if I kept the shape (output layer of shape 1 - tensor 283x100 -> 283x1)\n",
    "# having a flatten layer between converges\n",
    "\n",
    "# flatten layer and 12 samples -> predict the same for all 12 samples, maybe not enough filters\n",
    "\n",
    "# PROBLEM why we can't fit multiple targets: layer normalization!! use batch norm instead\n",
    "\n",
    "#---- with batch norm\n",
    "# cnn model + mean estimation, loss ~80, but predicting differnt mean\n",
    "# fcn model, loss ~81\n",
    "# fcn model / min scaling -> loss 0.8 / 27 (lots of negative predictions)\n",
    "# fcn model / max scaling / relu activation -> 3.1/8 (lots of 0 predictions) / with scale of 100, loss =14.9/43691\n",
    "# cnn model / max scaling / mean pred -> 6.0/inf\n",
    "# cnn ... no layer norm in beginning -> 15\n",
    "\n",
    "# loss function for every output (batch,283) / 100 epochs\n",
    "# cnn 1.5 loss\n",
    "# cnn with smaller LR 0.22(also after 200 epochs)\n",
    "# cnn with separate mean prediction loss 20.5 (lr0.0001) vs 3.5(lr0.0005) / can't even fit 2 samples (0.5 for lr 0.0005)\n",
    "\n",
    "# cnn without mean prediction (2 samples, lr0.0005) 22.4   / lr0.001 0.4 loss, but targets still fit badly / only fitting target noVar 0.08 still bad\n",
    "\n",
    "# difference between train / test = batch norm has significant effect here\n",
    "#fcn + mean, 2 samples LR0.0005 -> \n",
    "#fcn + mean, 2 samples only loss on target -> \n",
    "#fcn + mean, 1 sample, only loss on target -> 0.03 targets are far off\n",
    "#fcn, 1 sample, only loss on target -> 0.4 targets are far off\n",
    "# -> train data was not normalized!!\n",
    "\n",
    "# with regularization / without regularization doesn't matter that much as long as sample is normalized\n",
    "# normalization per sample -> predict the same for all targets ~0.0978\n",
    "# norm per sample + bis estimation -> predict same for all targets (besides 1) ~0.0978\n",
    "\n",
    "# with learning rate schedule -> 0.06 lots more possible to not get stuck in local minima\n",
    "\n",
    "#cnn / norm over train / bias estimation / lr0.01 / only target -> ~45 sum loss\n",
    "#cnn / norm over train / bias estimation / lf0.01 / target + loss2 -> ~47 after 95 epochs (15 after~150epochs)\n",
    "\n",
    "#cnn / norm over train / bais est / lr0.01 / target + loss / activation function relu instead of linear (conf + bias / still nan bc stddev =0, log(0) = nan)\n",
    "# 39/40 but training seems to be a lot more stable\n",
    "# after 170 epochs 23.7/23.5\n",
    "# after 220 epochs 11/19 (but already went down to 14/16)\n",
    "# after 250 epochs 12/16 (but already 16/15)\n",
    "# after 300 epochs 12/17\n",
    "\n",
    "\n",
    "# Assuming 'history' is your model's training history\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "#plt.plot(epochs, test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = model.predict(normData)\n",
    "def calcStats(b, plot=True, display=False):\n",
    "    outputs = model.predict(b[0])\n",
    "    pred = outputs[:,0:283,:]\n",
    "    pred[:,:,0] = pred[:,:,0]+outputs[:,283,0:1]\n",
    "    if display:\n",
    "        print(pred[0:10:,0:2,0], b[1][0:10:,0:2] ,np.exp(pred[0:10:,0:2,1]))\n",
    "        print(pred[0:10:,0:2,0]*maxLabels, b[1][0:10:,0:2]*maxLabels ,np.exp(pred[0:10:,0:2,1])*maxLabels)\n",
    "\n",
    "    mae = np.sum(np.abs(pred[:,:,0]*maxLabels-b[1]*maxLabels)) / pred.shape[0] / pred.shape[1]\n",
    "    mse = np.sum(np.abs(pred[:,:,0]*maxLabels-b[1]*maxLabels)**2) / pred.shape[0] / pred.shape[1]\n",
    "    sigma = np.exp(pred[:,:,1]) * maxLabels #we predict the squared loss\n",
    "    logLikelihood = -0.5*(np.log(2*np.pi) +np.log(sigma) + (np.abs(pred[:,:,0]*maxLabels-b[1]*maxLabels)**2) / sigma)\n",
    "    l_ideal = -0.5*(np.log(2*np.pi) + np.log(1e-10))*b[1].shape[0]*b[1].shape[1]\n",
    "    l_ref = -0.5*(np.log(2*np.pi) + np.log(stdLabels**2) + ((b[1]-meanLabels)/stdLabels)**2)\n",
    "    sumLogL = np.sum(logLikelihood)\n",
    "    sumLref = np.sum(l_ref)\n",
    "    score = (sumLogL - sumLref) / (l_ideal - sumLref)\n",
    "\n",
    "    print('mae',mae,'mse', mse, 'mean sigma', np.mean(sigma))\n",
    "    print('scaled: mae',mae/maxLabels,'mse', mse/maxLabels/maxLabels, 'mean sigma', np.mean(sigma)/maxLabels)\n",
    "    #print(logLikelihood.shape, logLikelihood)\n",
    "    #print(l_ideal, l_ref.shape, l_ref)\n",
    "    print(score, sumLogL, sumLref, l_ideal)\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for i in range(10): #range(12):# \n",
    "        fig.add_trace(go.Scatter(y=b[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "        fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step \n",
      "mae 8.286210599720688e-05 mse 2.7572209241326123e-08 mean sigma 0.007988296\n",
      "scaled: mae 0.010368195576441695 mse 0.00043168404069469587 mean sigma 0.9995427814800002\n",
      "0.9999999834448179 27094.63 -11590225000000.0 191878.29130813776\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_0",
         "type": "scatter",
         "y": [
          0.2296084612607956,
          0.22958338260650635,
          0.2295285016298294,
          0.22937124967575073,
          0.22939619421958923,
          0.22936667501926422,
          0.22937291860580444,
          0.22946715354919434,
          0.22954288125038147,
          0.22963352501392365,
          0.22975148260593414,
          0.22993376851081848,
          0.2301895022392273,
          0.23046840727329254,
          0.23073527216911316,
          0.2309660017490387,
          0.23126205801963806,
          0.2315950095653534,
          0.23189403116703033,
          0.23223331570625305,
          0.23259702324867249,
          0.23298057913780212,
          0.23337683081626892,
          0.2337350994348526,
          0.23410513997077942,
          0.2345173954963684,
          0.23498719930648804,
          0.23545926809310913,
          0.23591899871826172,
          0.23652298748493195,
          0.23717932403087616,
          0.2374751716852188,
          0.23739759624004364,
          0.23726370930671692,
          0.2372979074716568,
          0.2375108152627945,
          0.23764467239379883,
          0.23771613836288452,
          0.23795171082019806,
          0.2382248044013977,
          0.23835758864879608,
          0.23836393654346466,
          0.23823297023773193,
          0.23808728158473969,
          0.23824143409729004,
          0.23868153989315033,
          0.2390424609184265,
          0.23913216590881348,
          0.23893314599990845,
          0.2386670857667923,
          0.23868459463119507,
          0.23886914551258087,
          0.23875422775745392,
          0.2382686883211136,
          0.23783832788467407,
          0.2377011924982071,
          0.23761576414108276,
          0.23745448887348175,
          0.2374354600906372,
          0.23746684193611145,
          0.2373162806034088,
          0.23706679046154022,
          0.23682527244091034,
          0.23664280772209167,
          0.2364984005689621,
          0.23636701703071594,
          0.23625138401985168,
          0.2360791265964508,
          0.23583678901195526,
          0.23561985790729523,
          0.23548246920108795,
          0.23536454141139984,
          0.23519942164421082,
          0.2350316196680069,
          0.2349412441253662,
          0.2349414825439453,
          0.2349461317062378,
          0.2347760796546936,
          0.23441797494888306,
          0.23413346707820892,
          0.23401936888694763,
          0.23393027484416962,
          0.23375238478183746,
          0.23348866403102875,
          0.23326776921749115,
          0.2331411987543106,
          0.23299837112426758,
          0.23276005685329437,
          0.23249343037605286,
          0.23229098320007324,
          0.23213453590869904,
          0.23206134140491486,
          0.23203502595424652,
          0.23183543980121613,
          0.23152688145637512,
          0.23135298490524292,
          0.23129869997501373,
          0.23124194145202637,
          0.23115397989749908,
          0.23117132484912872,
          0.23139937222003937,
          0.2316066026687622,
          0.2315475046634674,
          0.23133662343025208,
          0.23120222985744476,
          0.23123042285442352,
          0.2314542979001999,
          0.23179349303245544,
          0.23196381330490112,
          0.23189538717269897,
          0.23186950385570526,
          0.23199445009231567,
          0.2321418821811676,
          0.23228314518928528,
          0.2324322909116745,
          0.232581228017807,
          0.23276183009147644,
          0.23297646641731262,
          0.23320971429347992,
          0.23346976935863495,
          0.23371192812919617,
          0.23386001586914062,
          0.23395512998104095,
          0.2341179996728897,
          0.234357088804245,
          0.23458397388458252,
          0.23478522896766663,
          0.23500767350196838,
          0.23524163663387299,
          0.23547884821891785,
          0.23577840626239777,
          0.23614349961280823,
          0.23652566969394684,
          0.2369575798511505,
          0.2374516874551773,
          0.23787078261375427,
          0.23816777765750885,
          0.23856866359710693,
          0.23920053243637085,
          0.23981812596321106,
          0.24021798372268677,
          0.24054275453090668,
          0.2409537136554718,
          0.24137893319129944,
          0.24168133735656738,
          0.24183392524719238,
          0.24196168780326843,
          0.24222268164157867,
          0.24258597195148468,
          0.24288494884967804,
          0.24309250712394714,
          0.24335291981697083,
          0.24377746880054474,
          0.24426686763763428,
          0.24456925690174103,
          0.2446109801530838,
          0.24456873536109924,
          0.2445489466190338,
          0.24446246027946472,
          0.24426253139972687,
          0.2440418154001236,
          0.24391986429691315,
          0.24394212663173676,
          0.24401132762432098,
          0.24397557973861694,
          0.24384868144989014,
          0.24373358488082886,
          0.2435961812734604,
          0.24336719512939453,
          0.24315539002418518,
          0.2430826872587204,
          0.24306553602218628,
          0.24296371638774872,
          0.24280232191085815,
          0.24267442524433136,
          0.24259909987449646,
          0.2425733208656311,
          0.2426825612783432,
          0.24312730133533478,
          0.24406427145004272,
          0.2453494369983673,
          0.24654188752174377,
          0.24726474285125732,
          0.24746748805046082,
          0.24728630483150482,
          0.24684125185012817,
          0.24622008204460144,
          0.2455115020275116,
          0.24479909241199493,
          0.24416445195674896,
          0.24366509914398193,
          0.24329602718353271,
          0.2430150955915451,
          0.24284161627292633,
          0.24283643066883087,
          0.24297352135181427,
          0.24312107264995575,
          0.24319018423557281,
          0.2432023584842682,
          0.24319270253181458,
          0.24316523969173431,
          0.24313420057296753,
          0.24309177696704865,
          0.2430008202791214,
          0.24287642538547516,
          0.2428130954504013,
          0.2428533136844635,
          0.2429163008928299,
          0.24290528893470764,
          0.24283123016357422,
          0.24277637898921967,
          0.24277232587337494,
          0.24279284477233887,
          0.2428014725446701,
          0.24279576539993286,
          0.24279823899269104,
          0.2428111732006073,
          0.24281512200832367,
          0.2427908182144165,
          0.24273870885372162,
          0.24268050491809845,
          0.2426290661096573,
          0.2425711750984192,
          0.24246655404567719,
          0.24229052662849426,
          0.24208126962184906,
          0.24190261960029602,
          0.24176840484142303,
          0.24164210259914398,
          0.2415013462305069,
          0.2413608729839325,
          0.2412649691104889,
          0.24124108254909515,
          0.24124674499034882,
          0.24119508266448975,
          0.24106262624263763,
          0.24090944230556488,
          0.24077200889587402,
          0.24061515927314758,
          0.24042105674743652,
          0.24024856090545654,
          0.2401522994041443,
          0.2401142120361328,
          0.24007555842399597,
          0.23999358713626862,
          0.23985499143600464,
          0.23966962099075317,
          0.23946940898895264,
          0.2392936497926712,
          0.2391567975282669,
          0.23904310166835785,
          0.23892787098884583,
          0.2387976050376892,
          0.2386522740125656,
          0.23850540816783905,
          0.2383842021226883,
          0.23831042647361755,
          0.23827612400054932,
          0.2382374256849289,
          0.23815016448497772,
          0.23801566660404205,
          0.23787707090377808,
          0.23776642978191376,
          0.2376777082681656,
          0.23759174346923828,
          0.23750749230384827,
          0.23743677139282227,
          0.23737338185310364,
          0.23729008436203003,
          0.23717442154884338,
          0.23705247044563293,
          0.2369569092988968,
          0.2368900328874588,
          0.236829474568367,
          0.23675836622714996,
          0.23667961359024048,
          0.23661397397518158,
          0.23658259212970734,
          0.2365744262933731,
          0.23654110729694366,
          0.23644523322582245,
          0.23630641400814056,
          0.236175537109375
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_0",
         "type": "scatter",
         "y": [
          0.24733637273311615,
          0.24947147071361542,
          0.2493615746498108,
          0.2493150234222412,
          0.24930322170257568,
          0.24895279109477997,
          0.24915216863155365,
          0.2492833137512207,
          0.2491675615310669,
          0.2489250898361206,
          0.2489289939403534,
          0.24880030751228333,
          0.24872136116027832,
          0.24866658449172974,
          0.24802562594413757,
          0.2477467656135559,
          0.24767310917377472,
          0.24752286076545715,
          0.24745380878448486,
          0.24753165245056152,
          0.24746477603912354,
          0.24748817086219788,
          0.24726951122283936,
          0.24725548923015594,
          0.24731409549713135,
          0.2473040223121643,
          0.24732159078121185,
          0.2474435567855835,
          0.24765434861183167,
          0.24818214774131775,
          0.2488207221031189,
          0.2488558292388916,
          0.24874244630336761,
          0.24852779507637024,
          0.24852682650089264,
          0.24881939589977264,
          0.248846173286438,
          0.24883127212524414,
          0.24902495741844177,
          0.24916818737983704,
          0.24930942058563232,
          0.24932613968849182,
          0.24930799007415771,
          0.24917572736740112,
          0.24932880699634552,
          0.24941545724868774,
          0.24962866306304932,
          0.24964851140975952,
          0.24959680438041687,
          0.24951466917991638,
          0.2495921552181244,
          0.2496136873960495,
          0.24960680305957794,
          0.2493993490934372,
          0.24923501908779144,
          0.24916721880435944,
          0.24908505380153656,
          0.24912889301776886,
          0.24932484328746796,
          0.2493133246898651,
          0.24926580488681793,
          0.24921539425849915,
          0.24910464882850647,
          0.24896904826164246,
          0.24893996119499207,
          0.24916496872901917,
          0.2492782473564148,
          0.24952958524227142,
          0.24959665536880493,
          0.2494228184223175,
          0.24924910068511963,
          0.24908670783042908,
          0.24911795556545258,
          0.24917736649513245,
          0.2492513656616211,
          0.24931477010250092,
          0.2494686245918274,
          0.24947845935821533,
          0.2493518441915512,
          0.24930430948734283,
          0.24941281974315643,
          0.24942290782928467,
          0.24933721125125885,
          0.2490592896938324,
          0.24925848841667175,
          0.24956680834293365,
          0.24990661442279816,
          0.250476211309433,
          0.2505127489566803,
          0.2505100965499878,
          0.25063028931617737,
          0.2508273720741272,
          0.25100576877593994,
          0.25092366337776184,
          0.2505851089954376,
          0.2505658268928528,
          0.2507079243659973,
          0.2507496476173401,
          0.25062936544418335,
          0.25059106945991516,
          0.2508893311023712,
          0.2511734962463379,
          0.25120922923088074,
          0.2509051263332367,
          0.2505262792110443,
          0.2504175901412964,
          0.2507096827030182,
          0.2511197328567505,
          0.2512190341949463,
          0.25081902742385864,
          0.2504858374595642,
          0.2504872977733612,
          0.2504819631576538,
          0.25037673115730286,
          0.25032907724380493,
          0.2503136992454529,
          0.25034308433532715,
          0.2503928542137146,
          0.25045862793922424,
          0.25055989623069763,
          0.25068825483322144,
          0.2506157159805298,
          0.2503316104412079,
          0.2501227557659149,
          0.2502032518386841,
          0.25047364830970764,
          0.25036513805389404,
          0.2501364052295685,
          0.2500695288181305,
          0.24993577599525452,
          0.24994701147079468,
          0.24994751811027527,
          0.249892458319664,
          0.24989762902259827,
          0.2500419318675995,
          0.25027433037757874,
          0.25036418437957764,
          0.25052890181541443,
          0.25064805150032043,
          0.2506590783596039,
          0.25067269802093506,
          0.2505514919757843,
          0.2506842613220215,
          0.2511596381664276,
          0.2509209215641022,
          0.2507334053516388,
          0.2507651448249817,
          0.2508293390274048,
          0.25067999958992004,
          0.2506850063800812,
          0.2506893277168274,
          0.25074535608291626,
          0.25105226039886475,
          0.25164350867271423,
          0.25184252858161926,
          0.2519581913948059,
          0.2520006597042084,
          0.2521703839302063,
          0.2521872818470001,
          0.25213244557380676,
          0.25183865427970886,
          0.25171276926994324,
          0.2517896592617035,
          0.2519303560256958,
          0.2519059479236603,
          0.25165069103240967,
          0.25145822763442993,
          0.25142771005630493,
          0.2513429522514343,
          0.25111210346221924,
          0.250760942697525,
          0.25072181224823,
          0.2506650984287262,
          0.25061577558517456,
          0.2506018280982971,
          0.25050267577171326,
          0.25046607851982117,
          0.25049951672554016,
          0.2510267496109009,
          0.2519665062427521,
          0.25281578302383423,
          0.2533333897590637,
          0.2535184323787689,
          0.25359630584716797,
          0.25318625569343567,
          0.25247782468795776,
          0.25182780623435974,
          0.2513304352760315,
          0.250966876745224,
          0.2508027255535126,
          0.250720739364624,
          0.2506543695926666,
          0.250611275434494,
          0.25064849853515625,
          0.251114159822464,
          0.2512209415435791,
          0.25135326385498047,
          0.2511502802371979,
          0.2508436441421509,
          0.25068554282188416,
          0.2506617307662964,
          0.25067800283432007,
          0.2507498264312744,
          0.25087398290634155,
          0.2508760094642639,
          0.25076377391815186,
          0.2507614195346832,
          0.25083687901496887,
          0.2507905960083008,
          0.2506617605686188,
          0.2505813539028168,
          0.25052833557128906,
          0.2504972517490387,
          0.2504969537258148,
          0.25050148367881775,
          0.25048771500587463,
          0.2505115568637848,
          0.25050610303878784,
          0.2504977881908417,
          0.25045886635780334,
          0.25038444995880127,
          0.2503325641155243,
          0.25031760334968567,
          0.25031572580337524,
          0.2502949833869934,
          0.2502506375312805,
          0.2502259314060211,
          0.25019749999046326,
          0.25012531876564026,
          0.2500653564929962,
          0.2500212490558624,
          0.24999016523361206,
          0.24996840953826904,
          0.2499808520078659,
          0.24996212124824524,
          0.24991865456104279,
          0.24983961880207062,
          0.24982184171676636,
          0.24976642429828644,
          0.2496451884508133,
          0.24960173666477203,
          0.24957415461540222,
          0.2495705932378769,
          0.2495754063129425,
          0.2495567798614502,
          0.24953030049800873,
          0.24946685135364532,
          0.24937576055526733,
          0.24928312003612518,
          0.2491706907749176,
          0.2490786463022232,
          0.24903604388237,
          0.2489130049943924,
          0.24864105880260468,
          0.24871022999286652,
          0.24852895736694336,
          0.2485312968492508,
          0.24865615367889404,
          0.2484993189573288,
          0.24854858219623566,
          0.24849361181259155,
          0.24834491312503815,
          0.248223215341568,
          0.24815058708190918,
          0.24802973866462708,
          0.2479534149169922,
          0.24788972735404968,
          0.24783125519752502,
          0.24774771928787231,
          0.24764001369476318,
          0.2476213425397873,
          0.24758587777614594,
          0.24761521816253662,
          0.2476378232240677,
          0.24760806560516357,
          0.24753892421722412,
          0.2475210726261139,
          0.2475435435771942,
          0.2475573718547821,
          0.24758103489875793,
          0.24757659435272217,
          0.2475316971540451,
          0.24745115637779236
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_1",
         "type": "scatter",
         "y": [
          0.29442042112350464,
          0.29765620827674866,
          0.29684415459632874,
          0.2960548400878906,
          0.2969377338886261,
          0.2963954210281372,
          0.29555177688598633,
          0.29548725485801697,
          0.2949698865413666,
          0.29466643929481506,
          0.29448410868644714,
          0.2942022383213043,
          0.2941615581512451,
          0.293944776058197,
          0.29368457198143005,
          0.29349884390830994,
          0.2933084964752197,
          0.2932308614253998,
          0.2931618392467499,
          0.2930842638015747,
          0.29299843311309814,
          0.29319119453430176,
          0.29326051473617554,
          0.2932564318180084,
          0.2933630049228668,
          0.2932206988334656,
          0.2929743826389313,
          0.29288727045059204,
          0.2928887903690338,
          0.292992502450943,
          0.29322436451911926,
          0.29330578446388245,
          0.2932997941970825,
          0.2933661937713623,
          0.2933613955974579,
          0.2933119535446167,
          0.2933116853237152,
          0.2932620942592621,
          0.29330936074256897,
          0.2935129404067993,
          0.2934841811656952,
          0.293476939201355,
          0.29392939805984497,
          0.2942907512187958,
          0.2941884994506836,
          0.2940869629383087,
          0.29414603114128113,
          0.294058620929718,
          0.2939571440219879,
          0.2940780222415924,
          0.2942523658275604,
          0.29461437463760376,
          0.2948059141635895,
          0.29462870955467224,
          0.29464074969291687,
          0.29465004801750183,
          0.2944744825363159,
          0.29458364844322205,
          0.29490503668785095,
          0.2948719263076782,
          0.2949451506137848,
          0.2954602837562561,
          0.295823335647583,
          0.2958466112613678,
          0.29598069190979004,
          0.2966415286064148,
          0.2975199818611145,
          0.2982293963432312,
          0.2986050546169281,
          0.29845088720321655,
          0.29787397384643555,
          0.29743829369544983,
          0.297703355550766,
          0.29829150438308716,
          0.29838648438453674,
          0.29829075932502747,
          0.29857316613197327,
          0.2987265884876251,
          0.2985267639160156,
          0.2984190881252289,
          0.29837653040885925,
          0.29834288358688354,
          0.2981882393360138,
          0.2977984547615051,
          0.29768437147140503,
          0.29807573556900024,
          0.29845190048217773,
          0.2985862195491791,
          0.2987992763519287,
          0.2990720868110657,
          0.299079030752182,
          0.2991107702255249,
          0.2993377447128296,
          0.29911285638809204,
          0.29845842719078064,
          0.2982342541217804,
          0.2986510396003723,
          0.2989940345287323,
          0.298707515001297,
          0.29842981696128845,
          0.2989775538444519,
          0.2998904883861542,
          0.3001832365989685,
          0.2994977533817291,
          0.2984323799610138,
          0.2981027662754059,
          0.29899242520332336,
          0.3002244830131531,
          0.3002966642379761,
          0.2992006242275238,
          0.29840779304504395,
          0.2983487844467163,
          0.2983594834804535,
          0.29824966192245483,
          0.2980504631996155,
          0.2978385388851166,
          0.2979030907154083,
          0.2982397973537445,
          0.29859471321105957,
          0.2989047169685364,
          0.29922518134117126,
          0.2993747293949127,
          0.29914337396621704,
          0.298917293548584,
          0.2991330325603485,
          0.2993163466453552,
          0.2990773320198059,
          0.29890623688697815,
          0.2989290654659271,
          0.2987827956676483,
          0.29851776361465454,
          0.2981151342391968,
          0.2975049316883087,
          0.2972949743270874,
          0.297949880361557,
          0.2988850176334381,
          0.29921847581863403,
          0.2988300323486328,
          0.29836058616638184,
          0.29817941784858704,
          0.2979714274406433,
          0.29770955443382263,
          0.2976682782173157,
          0.2974323034286499,
          0.29672083258628845,
          0.2962799370288849,
          0.29644152522087097,
          0.2964646518230438,
          0.2960878014564514,
          0.29586827754974365,
          0.29593127965927124,
          0.29601168632507324,
          0.29616230726242065,
          0.29640528559684753,
          0.2965271472930908,
          0.29653826355934143,
          0.2966497838497162,
          0.29689255356788635,
          0.2970944344997406,
          0.297085165977478,
          0.2968248724937439,
          0.29657217860221863,
          0.29665493965148926,
          0.2968623638153076,
          0.296661376953125,
          0.29611852765083313,
          0.29581204056739807,
          0.29592618346214294,
          0.2960870862007141,
          0.2959565818309784,
          0.2955837547779083,
          0.2952698767185211,
          0.29526472091674805,
          0.29548731446266174,
          0.2956054210662842,
          0.29546305537223816,
          0.29531845450401306,
          0.29552730917930603,
          0.296100914478302,
          0.2967430055141449,
          0.2971996068954468,
          0.2973838448524475,
          0.29728442430496216,
          0.29697227478027344,
          0.2966148555278778,
          0.29631364345550537,
          0.29597559571266174,
          0.2955358028411865,
          0.2952134609222412,
          0.29519712924957275,
          0.2953225374221802,
          0.2953421175479889,
          0.2953495383262634,
          0.29562506079673767,
          0.29613205790519714,
          0.29647186398506165,
          0.29634204506874084,
          0.2958621680736542,
          0.29537707567214966,
          0.29506662487983704,
          0.2949451208114624,
          0.2950616180896759,
          0.29539984464645386,
          0.2956974506378174,
          0.2957373261451721,
          0.29561692476272583,
          0.29555177688598633,
          0.29558509588241577,
          0.29556792974472046,
          0.2953720688819885,
          0.29505455493927,
          0.29480060935020447,
          0.29471394419670105,
          0.2946918308734894,
          0.29461634159088135,
          0.2945207357406616,
          0.294486403465271,
          0.2944723665714264,
          0.2943803369998932,
          0.29421254992485046,
          0.29406389594078064,
          0.29399460554122925,
          0.2939605414867401,
          0.29388830065727234,
          0.2937939465045929,
          0.2937491536140442,
          0.29377034306526184,
          0.2937849462032318,
          0.2937369644641876,
          0.2936456501483917,
          0.29356786608695984,
          0.29355278611183167,
          0.2936110496520996,
          0.29367244243621826,
          0.29363951086997986,
          0.2935163676738739,
          0.29340896010398865,
          0.2933659553527832,
          0.2933275103569031,
          0.2932600677013397,
          0.29322466254234314,
          0.2932531237602234,
          0.29328399896621704,
          0.29327112436294556,
          0.2932387590408325,
          0.29320618510246277,
          0.2931498885154724,
          0.29308080673217773,
          0.2930590510368347,
          0.2931068539619446,
          0.29319077730178833,
          0.2932802438735962,
          0.2933488190174103,
          0.2933444082736969,
          0.2932499647140503,
          0.29314321279525757,
          0.29310375452041626,
          0.2931152880191803,
          0.2931215763092041,
          0.29312002658843994,
          0.29312556982040405,
          0.29311466217041016,
          0.29308435320854187,
          0.29308605194091797,
          0.29314255714416504,
          0.2931956946849823,
          0.29318538308143616,
          0.29313144087791443,
          0.2930903732776642,
          0.29306936264038086,
          0.2930386960506439,
          0.29299166798591614,
          0.29294806718826294,
          0.29292038083076477,
          0.2929069995880127,
          0.29289671778678894,
          0.29288020730018616,
          0.29288196563720703,
          0.2929450273513794,
          0.29306015372276306,
          0.293142169713974,
          0.2931174039840698,
          0.2930176556110382
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_1",
         "type": "scatter",
         "y": [
          0.2971686124801636,
          0.29930374026298523,
          0.2991938292980194,
          0.29914727807044983,
          0.2991354763507843,
          0.2987850308418274,
          0.29898443818092346,
          0.2991155683994293,
          0.2989998161792755,
          0.2987573444843292,
          0.298761248588562,
          0.29863256216049194,
          0.29855361580848694,
          0.29849883913993835,
          0.2978578805923462,
          0.2975790202617645,
          0.29750537872314453,
          0.29735511541366577,
          0.2972860634326935,
          0.29736390709877014,
          0.29729703068733215,
          0.2973204255104065,
          0.297101765871048,
          0.29708775877952576,
          0.29714635014533997,
          0.2971362769603729,
          0.2971538305282593,
          0.2972758114337921,
          0.2974866032600403,
          0.29801440238952637,
          0.2986529767513275,
          0.2986880838871002,
          0.2985747158527374,
          0.29836004972457886,
          0.29835906624794006,
          0.29865166544914246,
          0.2986784279346466,
          0.29866352677345276,
          0.2988572120666504,
          0.29900044202804565,
          0.29914167523384094,
          0.29915839433670044,
          0.29914024472236633,
          0.29900798201560974,
          0.29916107654571533,
          0.29924771189689636,
          0.29946091771125793,
          0.29948076605796814,
          0.2994290590286255,
          0.299346923828125,
          0.299424409866333,
          0.2994459271430969,
          0.29943907260894775,
          0.299231618642807,
          0.29906728863716125,
          0.29899945855140686,
          0.29891732335090637,
          0.2989611327648163,
          0.2991570830345154,
          0.29914557933807373,
          0.29909804463386536,
          0.29904764890670776,
          0.2989369034767151,
          0.2988013029098511,
          0.2987722158432007,
          0.2989972233772278,
          0.2991105020046234,
          0.29936182498931885,
          0.29942891001701355,
          0.2992550730705261,
          0.29908135533332825,
          0.2989189624786377,
          0.2989502251148224,
          0.29900962114334106,
          0.2990836203098297,
          0.29914700984954834,
          0.299300879240036,
          0.29931071400642395,
          0.29918408393859863,
          0.29913657903671265,
          0.29924505949020386,
          0.2992551624774933,
          0.2991694509983063,
          0.298891544342041,
          0.29909074306488037,
          0.29939907789230347,
          0.2997388541698456,
          0.3003084659576416,
          0.3003450036048889,
          0.3003423511981964,
          0.300462543964386,
          0.3006596267223358,
          0.30083802342414856,
          0.30075591802597046,
          0.30041736364364624,
          0.3003980815410614,
          0.30054017901420593,
          0.3005819022655487,
          0.30046162009239197,
          0.3004233241081238,
          0.30072158575057983,
          0.3010057508945465,
          0.30104148387908936,
          0.3007373809814453,
          0.30035853385925293,
          0.300249844789505,
          0.3005419373512268,
          0.3009519875049591,
          0.3010512888431549,
          0.30065128207206726,
          0.3003180921077728,
          0.3003195524215698,
          0.3003142178058624,
          0.3002089858055115,
          0.30016133189201355,
          0.3001459538936615,
          0.30017533898353577,
          0.3002251088619232,
          0.30029088258743286,
          0.30039215087890625,
          0.30052050948143005,
          0.3004479706287384,
          0.3001638650894165,
          0.29995501041412354,
          0.3000355064868927,
          0.30030590295791626,
          0.30019739270210266,
          0.2999686598777771,
          0.2999017834663391,
          0.29976803064346313,
          0.2997792661190033,
          0.2997797727584839,
          0.2997246980667114,
          0.2997298836708069,
          0.2998741865158081,
          0.30010658502578735,
          0.30019643902778625,
          0.30036115646362305,
          0.30048030614852905,
          0.3004913330078125,
          0.3005049526691437,
          0.3003837466239929,
          0.3005165159702301,
          0.30099189281463623,
          0.3007531762123108,
          0.3005656599998474,
          0.3005973994731903,
          0.3006615936756134,
          0.30051225423812866,
          0.3005172610282898,
          0.300521582365036,
          0.3005776107311249,
          0.30088451504707336,
          0.30147576332092285,
          0.3016747832298279,
          0.3017904460430145,
          0.301832914352417,
          0.3020026385784149,
          0.30201953649520874,
          0.3019647002220154,
          0.3016709089279175,
          0.30154502391815186,
          0.3016219139099121,
          0.3017626106739044,
          0.3017382025718689,
          0.3014829456806183,
          0.30129048228263855,
          0.30125996470451355,
          0.30117520689964294,
          0.30094435811042786,
          0.30059319734573364,
          0.3005540668964386,
          0.3004973530769348,
          0.3004480302333832,
          0.30043408274650574,
          0.3003349304199219,
          0.3002983331680298,
          0.3003317713737488,
          0.3008590042591095,
          0.3017987608909607,
          0.30264803767204285,
          0.30316561460494995,
          0.30335068702697754,
          0.3034285604953766,
          0.3030185103416443,
          0.3023100793361664,
          0.30166006088256836,
          0.3011626899242401,
          0.3007991313934326,
          0.3006349802017212,
          0.30055299401283264,
          0.30048662424087524,
          0.30044353008270264,
          0.30048075318336487,
          0.3009464144706726,
          0.3010531961917877,
          0.3011855185031891,
          0.3009825348854065,
          0.3006758987903595,
          0.3005177974700928,
          0.300493985414505,
          0.3005102574825287,
          0.30058208107948303,
          0.30070623755455017,
          0.30070826411247253,
          0.3005960285663605,
          0.30059367418289185,
          0.3006691336631775,
          0.3006228506565094,
          0.3004940152168274,
          0.3004136085510254,
          0.3003605902194977,
          0.3003295063972473,
          0.30032920837402344,
          0.30033373832702637,
          0.30031996965408325,
          0.3003438115119934,
          0.30033835768699646,
          0.3003300428390503,
          0.30029112100601196,
          0.3002167046070099,
          0.3001648187637329,
          0.3001498579978943,
          0.30014798045158386,
          0.300127238035202,
          0.30008289217948914,
          0.30005818605422974,
          0.3000297546386719,
          0.2999575734138489,
          0.29989761114120483,
          0.29985350370407104,
          0.2998224198818207,
          0.29980066418647766,
          0.2998131215572357,
          0.29979437589645386,
          0.2997508943080902,
          0.29967188835144043,
          0.299654096364975,
          0.29959866404533386,
          0.2994774580001831,
          0.29943400621414185,
          0.29940640926361084,
          0.2994028329849243,
          0.2994076609611511,
          0.2993890345096588,
          0.29936254024505615,
          0.29929912090301514,
          0.29920801520347595,
          0.299115389585495,
          0.2990029454231262,
          0.29891088604927063,
          0.2988682985305786,
          0.2987452745437622,
          0.2984733283519745,
          0.29854249954223633,
          0.298361212015152,
          0.2983635663986206,
          0.29848840832710266,
          0.2983315587043762,
          0.2983808219432831,
          0.29832586646080017,
          0.29817715287208557,
          0.2980554699897766,
          0.2979828417301178,
          0.2978619933128357,
          0.2977856695652008,
          0.2977219820022583,
          0.29766350984573364,
          0.29757997393608093,
          0.2974722683429718,
          0.2974536120891571,
          0.29741811752319336,
          0.29744747281074524,
          0.2974700927734375,
          0.2974403202533722,
          0.29737117886543274,
          0.2973533272743225,
          0.29737579822540283,
          0.2973896265029907,
          0.29741328954696655,
          0.2974088490009308,
          0.2973639667034149,
          0.297283411026001
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_2",
         "type": "scatter",
         "y": [
          0.20937946438789368,
          0.21270684897899628,
          0.21276558935642242,
          0.21288716793060303,
          0.21266670525074005,
          0.2123548835515976,
          0.21252532303333282,
          0.21302901208400726,
          0.21312695741653442,
          0.21309694647789001,
          0.21320432424545288,
          0.21337993443012238,
          0.21367312967777252,
          0.2137221395969391,
          0.21353888511657715,
          0.21337617933750153,
          0.21326962113380432,
          0.21309827268123627,
          0.21296465396881104,
          0.21292538940906525,
          0.21277564764022827,
          0.21260510385036469,
          0.21248140931129456,
          0.21234862506389618,
          0.21215084195137024,
          0.2119922935962677,
          0.2120293527841568,
          0.21208679676055908,
          0.21197709441184998,
          0.211770161986351,
          0.21160079538822174,
          0.21139954030513763,
          0.21120400726795197,
          0.2110298126935959,
          0.21086536347866058,
          0.21076111495494843,
          0.21064619719982147,
          0.21050940454006195,
          0.2104647308588028,
          0.21049951016902924,
          0.21052958071231842,
          0.21053682267665863,
          0.2105133980512619,
          0.21048055589199066,
          0.2105269730091095,
          0.2106582373380661,
          0.2107633799314499,
          0.21077556908130646,
          0.2106987088918686,
          0.21060864627361298,
          0.21060383319854736,
          0.2106650471687317,
          0.21065261960029602,
          0.21052606403827667,
          0.21040938794612885,
          0.21035625040531158,
          0.21031735837459564,
          0.2103249430656433,
          0.21037626266479492,
          0.21035107970237732,
          0.21028855443000793,
          0.2102784365415573,
          0.21027825772762299,
          0.21026471257209778,
          0.21026797592639923,
          0.21031419932842255,
          0.21045039594173431,
          0.21071971952915192,
          0.21095553040504456,
          0.21099524199962616,
          0.21094784140586853,
          0.2109212875366211,
          0.2109771966934204,
          0.21118074655532837,
          0.2113688737154007,
          0.21140190958976746,
          0.211490660905838,
          0.21167168021202087,
          0.21173107624053955,
          0.21182496845722198,
          0.21205967664718628,
          0.21210983395576477,
          0.21193742752075195,
          0.21190759539604187,
          0.212327778339386,
          0.21333268284797668,
          0.21461591124534607,
          0.21559303998947144,
          0.2160952091217041,
          0.21638156473636627,
          0.21670639514923096,
          0.2170601487159729,
          0.2172286957502365,
          0.21718953549861908,
          0.2171466201543808,
          0.21716858446598053,
          0.21716436743736267,
          0.21708635985851288,
          0.21702121198177338,
          0.21709173917770386,
          0.21719233691692352,
          0.2170967012643814,
          0.21685762703418732,
          0.216689333319664,
          0.2166423499584198,
          0.21663402020931244,
          0.21661525964736938,
          0.21659965813159943,
          0.21656304597854614,
          0.21651212871074677,
          0.21653413772583008,
          0.21658866107463837,
          0.21655413508415222,
          0.2164510190486908,
          0.21635405719280243,
          0.2162933349609375,
          0.21628707647323608,
          0.2163095325231552,
          0.21631403267383575,
          0.2162965089082718,
          0.21624980866909027,
          0.21614159643650055,
          0.21598130464553833,
          0.2158392369747162,
          0.21575234830379486,
          0.21567575633525848,
          0.21557462215423584,
          0.21546848118305206,
          0.21534597873687744,
          0.21516837179660797,
          0.21496856212615967,
          0.21480098366737366,
          0.21465271711349487,
          0.2145107090473175,
          0.21439231932163239,
          0.21427521109580994,
          0.21412983536720276,
          0.21398204565048218,
          0.2138512283563614,
          0.21370571851730347,
          0.21352924406528473,
          0.21335676312446594,
          0.21320506930351257,
          0.21303732693195343,
          0.21284136176109314,
          0.21266815066337585,
          0.21255339682102203,
          0.2124803513288498,
          0.21243400871753693,
          0.2124185413122177,
          0.2124311327934265,
          0.21247582137584686,
          0.2125716656446457,
          0.21269507706165314,
          0.2127740979194641,
          0.21278385818004608,
          0.21276900172233582,
          0.21275751292705536,
          0.2127249836921692,
          0.21265903115272522,
          0.21258722245693207,
          0.2125452607870102,
          0.21254487335681915,
          0.2125554382801056,
          0.21253551542758942,
          0.21249333024024963,
          0.21245846152305603,
          0.21241816878318787,
          0.21235013008117676,
          0.21228259801864624,
          0.2122504562139511,
          0.21223677694797516,
          0.21220815181732178,
          0.21217012405395508,
          0.2121438980102539,
          0.21213234961032867,
          0.2121349275112152,
          0.21217598021030426,
          0.2123107612133026,
          0.2125820368528366,
          0.2129524052143097,
          0.21330392360687256,
          0.21353210508823395,
          0.21361729502677917,
          0.2135905921459198,
          0.21347971260547638,
          0.21330375969409943,
          0.2130860984325409,
          0.21285606920719147,
          0.21264494955539703,
          0.21247504651546478,
          0.21234801411628723,
          0.2122528851032257,
          0.2121935933828354,
          0.21218395233154297,
          0.21221384406089783,
          0.21224595606327057,
          0.21225719153881073,
          0.2122553139925003,
          0.21224984526634216,
          0.21224035322666168,
          0.21223127841949463,
          0.2122209370136261,
          0.21219539642333984,
          0.21215513348579407,
          0.21212828159332275,
          0.21213093400001526,
          0.2121412605047226,
          0.21213209629058838,
          0.21210967004299164,
          0.21209973096847534,
          0.21210788190364838,
          0.2121201902627945,
          0.21212267875671387,
          0.21211551129817963,
          0.2121090441942215,
          0.21210907399654388,
          0.21211250126361847,
          0.21211154758930206,
          0.21210229396820068,
          0.21208949387073517,
          0.21207793056964874,
          0.21206378936767578,
          0.2120337188243866,
          0.21198038756847382,
          0.2119164764881134,
          0.21186110377311707,
          0.21181720495224,
          0.21177391707897186,
          0.21172624826431274,
          0.211680069565773,
          0.2116485983133316,
          0.2116391956806183,
          0.2116386890411377,
          0.21162092685699463,
          0.21157878637313843,
          0.21152961254119873,
          0.21148346364498138,
          0.21143032610416412,
          0.2113664597272873,
          0.21131058037281036,
          0.2112787663936615,
          0.2112644910812378,
          0.21124978363513947,
          0.21122200787067413,
          0.21117670834064484,
          0.21111595630645752,
          0.21104979515075684,
          0.2109917253255844,
          0.21094724535942078,
          0.21091118454933167,
          0.2108747363090515,
          0.21083220839500427,
          0.2107822746038437,
          0.2107286900281906,
          0.2106814980506897,
          0.21064972877502441,
          0.21063244342803955,
          0.21061751246452332,
          0.21059183776378632,
          0.21055297553539276,
          0.21050935983657837,
          0.21046926081180573,
          0.21043358743190765,
          0.21039904654026031,
          0.21036532521247864,
          0.2103361338376999,
          0.21031181514263153,
          0.21028579771518707,
          0.2102517932653427,
          0.21021312475204468,
          0.21017806231975555,
          0.2101500928401947,
          0.21012632548809052,
          0.21010245382785797,
          0.21007636189460754,
          0.21005018055438995,
          0.2100297063589096,
          0.210016667842865,
          0.21000263094902039,
          0.20997776091098785,
          0.20994466543197632,
          0.2099159061908722
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_2",
         "type": "scatter",
         "y": [
          0.21031345427036285,
          0.21244855225086212,
          0.2123386561870575,
          0.21229210495948792,
          0.2122803032398224,
          0.21192987263202667,
          0.21212925016880035,
          0.2122603952884674,
          0.2121446430683136,
          0.2119021713733673,
          0.2119060754776001,
          0.21177738904953003,
          0.21169844269752502,
          0.21164366602897644,
          0.21100270748138428,
          0.2107238471508026,
          0.21065019071102142,
          0.21049994230270386,
          0.21043089032173157,
          0.21050873398780823,
          0.21044185757637024,
          0.21046525239944458,
          0.21024659276008606,
          0.21023257076740265,
          0.21029117703437805,
          0.210281103849411,
          0.21029867231845856,
          0.2104206383228302,
          0.21063143014907837,
          0.21115922927856445,
          0.2117978036403656,
          0.2118329107761383,
          0.21171952784061432,
          0.21150487661361694,
          0.21150390803813934,
          0.21179647743701935,
          0.2118232548236847,
          0.21180835366249084,
          0.21200203895568848,
          0.21214526891708374,
          0.21228650212287903,
          0.21230322122573853,
          0.21228507161140442,
          0.21215280890464783,
          0.21230588853359222,
          0.21239253878593445,
          0.21260574460029602,
          0.21262559294700623,
          0.21257388591766357,
          0.21249175071716309,
          0.2125692367553711,
          0.2125907689332962,
          0.21258388459682465,
          0.2123764306306839,
          0.21221210062503815,
          0.21214430034160614,
          0.21206213533878326,
          0.21210597455501556,
          0.21230192482471466,
          0.21229040622711182,
          0.21224288642406464,
          0.21219247579574585,
          0.21208173036575317,
          0.21194612979888916,
          0.21191704273223877,
          0.21214205026626587,
          0.2122553288936615,
          0.21250666677951813,
          0.21257373690605164,
          0.2123998999595642,
          0.21222618222236633,
          0.21206378936767578,
          0.21209503710269928,
          0.21215444803237915,
          0.2122284471988678,
          0.21229185163974762,
          0.2124457061290741,
          0.21245554089546204,
          0.2123289257287979,
          0.21228139102458954,
          0.21238990128040314,
          0.21239998936653137,
          0.21231429278850555,
          0.2120363712310791,
          0.21223556995391846,
          0.21254388988018036,
          0.21288369596004486,
          0.2134532779455185,
          0.213489830493927,
          0.2134871780872345,
          0.21360738575458527,
          0.2138044685125351,
          0.21398283541202545,
          0.21390075981616974,
          0.21356220543384552,
          0.2135429084300995,
          0.21368500590324402,
          0.21372674405574799,
          0.21360644698143005,
          0.21356815099716187,
          0.21386639773845673,
          0.2141505628824234,
          0.21418631076812744,
          0.2138822078704834,
          0.21350336074829102,
          0.2133946716785431,
          0.2136867791414261,
          0.2140968143939972,
          0.2141961008310318,
          0.21379610896110535,
          0.2134629338979721,
          0.2134643793106079,
          0.2134590446949005,
          0.21335379779338837,
          0.21330615878105164,
          0.21329078078269958,
          0.21332016587257385,
          0.2133699357509613,
          0.21343569457530975,
          0.21353697776794434,
          0.21366532146930695,
          0.21359281241893768,
          0.21330870687961578,
          0.21309982240200043,
          0.2131803184747696,
          0.21345072984695435,
          0.21334221959114075,
          0.21311350166797638,
          0.213046595454216,
          0.21291285753250122,
          0.21292409300804138,
          0.21292459964752197,
          0.2128695398569107,
          0.21287471055984497,
          0.2130190134048462,
          0.21325139701366425,
          0.21334126591682434,
          0.21350598335266113,
          0.21362514793872833,
          0.21363615989685059,
          0.21364976465702057,
          0.213528573513031,
          0.213661327958107,
          0.21413670480251312,
          0.21389801800251007,
          0.2137105017900467,
          0.2137422263622284,
          0.21380643546581268,
          0.21365708112716675,
          0.21366208791732788,
          0.2136664241552353,
          0.21372243762016296,
          0.21402934193611145,
          0.21462059020996094,
          0.21481962502002716,
          0.2149352729320526,
          0.21497774124145508,
          0.215147465467453,
          0.21516436338424683,
          0.21510952711105347,
          0.21481573581695557,
          0.21468985080718994,
          0.2147667407989502,
          0.2149074524641037,
          0.21488302946090698,
          0.21462778747081757,
          0.21443532407283783,
          0.21440479159355164,
          0.21432001888751984,
          0.21408917009830475,
          0.21373802423477173,
          0.2136988788843155,
          0.2136421799659729,
          0.21359287202358246,
          0.21357890963554382,
          0.21347974240779877,
          0.21344317495822906,
          0.21347661316394806,
          0.2140038162469864,
          0.21494358777999878,
          0.21579286456108093,
          0.21631045639514923,
          0.21649549901485443,
          0.21657337248325348,
          0.21616333723068237,
          0.21545489132404327,
          0.21480487287044525,
          0.214307501912117,
          0.2139439731836319,
          0.21377980709075928,
          0.21369780600070953,
          0.21363146603107452,
          0.21358835697174072,
          0.21362556517124176,
          0.2140912562608719,
          0.2141980230808258,
          0.21433033049106598,
          0.21412737667560577,
          0.21382072567939758,
          0.21366262435913086,
          0.2136388123035431,
          0.21365509927272797,
          0.2137269228696823,
          0.21385107934474945,
          0.21385307610034943,
          0.21374087035655975,
          0.21373850107192993,
          0.21381397545337677,
          0.21376767754554749,
          0.21363884210586548,
          0.21355842053890228,
          0.21350540220737457,
          0.2134743332862854,
          0.21347403526306152,
          0.21347858011722565,
          0.21346481144428253,
          0.2134886384010315,
          0.21348318457603455,
          0.21347488462924957,
          0.21343596279621124,
          0.21336153149604797,
          0.213309645652771,
          0.21329468488693237,
          0.21329282224178314,
          0.2132720649242401,
          0.21322771906852722,
          0.21320299804210663,
          0.21317458152770996,
          0.21310240030288696,
          0.21304243803024292,
          0.21299831569194794,
          0.21296724677085876,
          0.21294549107551575,
          0.2129579335451126,
          0.21293920278549194,
          0.2128957360982895,
          0.21281670033931732,
          0.21279892325401306,
          0.21274350583553314,
          0.21262226998806,
          0.21257881820201874,
          0.21255123615264893,
          0.2125476747751236,
          0.2125524878501892,
          0.2125338613986969,
          0.21250738203525543,
          0.21244393289089203,
          0.21235284209251404,
          0.2122602015733719,
          0.2121477723121643,
          0.2120557278394699,
          0.2120131254196167,
          0.2118900865316391,
          0.21161814033985138,
          0.21168731153011322,
          0.21150603890419006,
          0.2115083783864975,
          0.21163323521614075,
          0.2114764004945755,
          0.21152566373348236,
          0.21147069334983826,
          0.21132199466228485,
          0.2112002968788147,
          0.21112766861915588,
          0.21100682020187378,
          0.2109304964542389,
          0.2108668088912964,
          0.21080833673477173,
          0.21072480082511902,
          0.2106170952320099,
          0.210598424077034,
          0.21056295931339264,
          0.21059229969978333,
          0.2106149047613144,
          0.21058514714241028,
          0.21051600575447083,
          0.2104981541633606,
          0.21052062511444092,
          0.2105344533920288,
          0.21055811643600464,
          0.21055367588996887,
          0.2105087786912918,
          0.21042823791503906
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_3",
         "type": "scatter",
         "y": [
          0.09744498133659363,
          0.09989896416664124,
          0.09962798655033112,
          0.09931004792451859,
          0.09992910176515579,
          0.09959284216165543,
          0.09926897287368774,
          0.09945615381002426,
          0.09917041659355164,
          0.0990181565284729,
          0.09898605197668076,
          0.0990997701883316,
          0.09926386177539825,
          0.09929615259170532,
          0.09908174723386765,
          0.09876061975955963,
          0.09885131567716599,
          0.09884524345397949,
          0.09864109754562378,
          0.098611980676651,
          0.09864815324544907,
          0.09884203970432281,
          0.09896646440029144,
          0.09897900372743607,
          0.09908618032932281,
          0.09919840097427368,
          0.09931352734565735,
          0.09946652501821518,
          0.09965619444847107,
          0.09992136061191559,
          0.10021951049566269,
          0.10034468024969101,
          0.10031456500291824,
          0.10029047727584839,
          0.10031753033399582,
          0.10038963705301285,
          0.1004423201084137,
          0.10046503692865372,
          0.1005653589963913,
          0.10070974379777908,
          0.1007688045501709,
          0.10080006718635559,
          0.10084427148103714,
          0.10083096474409103,
          0.10086682438850403,
          0.1010463535785675,
          0.10120910406112671,
          0.10121849179267883,
          0.10109691321849823,
          0.10098075866699219,
          0.10100000351667404,
          0.10112963616847992,
          0.10114472359418869,
          0.10097558796405792,
          0.10083205252885818,
          0.10075098276138306,
          0.10067740827798843,
          0.10076609253883362,
          0.10092686116695404,
          0.10084135085344315,
          0.10071200877428055,
          0.10076673328876495,
          0.10080862790346146,
          0.10072388499975204,
          0.10068471729755402,
          0.10087656229734421,
          0.10128206014633179,
          0.10174211114645004,
          0.101913683116436,
          0.10165568441152573,
          0.10122445970773697,
          0.10089916735887527,
          0.10086523741483688,
          0.10105661302804947,
          0.10120704770088196,
          0.10137004405260086,
          0.10158651322126389,
          0.10146992653608322,
          0.1010880246758461,
          0.10089614242315292,
          0.10088106244802475,
          0.10092337429523468,
          0.10086741298437119,
          0.10064863413572311,
          0.1005924642086029,
          0.1008167490363121,
          0.10105165094137192,
          0.1011379212141037,
          0.10117176920175552,
          0.10122384876012802,
          0.10123535990715027,
          0.1013244092464447,
          0.10151419788599014,
          0.10149677097797394,
          0.10130449384450912,
          0.10131753981113434,
          0.10158735513687134,
          0.10174092650413513,
          0.10159112513065338,
          0.10156460851430893,
          0.10194284468889236,
          0.10234057903289795,
          0.10235082358121872,
          0.10194491595029831,
          0.10144399106502533,
          0.1013006791472435,
          0.10168369114398956,
          0.1022452786564827,
          0.10229215770959854,
          0.10173578560352325,
          0.10134619474411011,
          0.10141521692276001,
          0.10143794119358063,
          0.10125550627708435,
          0.10114322602748871,
          0.10119149088859558,
          0.10132606327533722,
          0.10149836540222168,
          0.10167347639799118,
          0.10187528282403946,
          0.10211895406246185,
          0.10219237208366394,
          0.10196468979120255,
          0.10180819034576416,
          0.1020287275314331,
          0.10225860029459,
          0.1021692305803299,
          0.10198826342821121,
          0.10190272331237793,
          0.10193273425102234,
          0.10213430225849152,
          0.10225966572761536,
          0.1020626649260521,
          0.10189234465360641,
          0.10216701030731201,
          0.10270006209611893,
          0.10300257802009583,
          0.10295098274946213,
          0.1028788834810257,
          0.102971151471138,
          0.10302417725324631,
          0.10304922610521317,
          0.10327817499637604,
          0.10353746265172958,
          0.10354273021221161,
          0.10345186293125153,
          0.10347927361726761,
          0.10353665798902512,
          0.10353885591030121,
          0.10355817526578903,
          0.10362706333398819,
          0.1037280261516571,
          0.10389108955860138,
          0.10409232974052429,
          0.10422389209270477,
          0.10425509512424469,
          0.10425502061843872,
          0.10426075011491776,
          0.10422426462173462,
          0.10411424934864044,
          0.10398201644420624,
          0.10391158610582352,
          0.10392635315656662,
          0.10395051538944244,
          0.10390224307775497,
          0.10381335765123367,
          0.10375544428825378,
          0.10370852053165436,
          0.10361513495445251,
          0.10349981486797333,
          0.10342317819595337,
          0.1033836379647255,
          0.10334256291389465,
          0.10329858213663101,
          0.1032654196023941,
          0.10323681682348251,
          0.10322227329015732,
          0.10328271239995956,
          0.10350453108549118,
          0.1039249449968338,
          0.10446935147047043,
          0.10497651249170303,
          0.1053122878074646,
          0.1054472103714943,
          0.10541503876447678,
          0.10525091737508774,
          0.10498254746198654,
          0.10464633256196976,
          0.10429274290800095,
          0.10397345572710037,
          0.10372290015220642,
          0.10354216396808624,
          0.10341425985097885,
          0.10334464907646179,
          0.10334758460521698,
          0.10339614748954773,
          0.10343029350042343,
          0.10342636704444885,
          0.10340841114521027,
          0.10339140892028809,
          0.10337552428245544,
          0.10337325185537338,
          0.10337591171264648,
          0.10334159433841705,
          0.10326649248600006,
          0.10321101546287537,
          0.10321281105279922,
          0.1032344400882721,
          0.10322435945272446,
          0.10318739712238312,
          0.10316510498523712,
          0.10317249596118927,
          0.1031913310289383,
          0.10319514572620392,
          0.10318032652139664,
          0.10316601395606995,
          0.10316506028175354,
          0.10317077487707138,
          0.10316836088895798,
          0.10315336287021637,
          0.10313466191291809,
          0.10311733186244965,
          0.10309277474880219,
          0.10304279625415802,
          0.1029626727104187,
          0.10287334769964218,
          0.10279747098684311,
          0.10273326188325882,
          0.10266600549221039,
          0.10259319841861725,
          0.10252596437931061,
          0.10248441994190216,
          0.10247889161109924,
          0.1024843156337738,
          0.10245607793331146,
          0.10238614678382874,
          0.10230790823698044,
          0.10223809629678726,
          0.10215803980827332,
          0.1020616963505745,
          0.10198007524013519,
          0.10193571448326111,
          0.10191471129655838,
          0.10189176350831985,
          0.1018516793847084,
          0.10178562253713608,
          0.10169117152690887,
          0.10158471763134003,
          0.1014941856265068,
          0.10143204778432846,
          0.10138913989067078,
          0.10134818404912949,
          0.1012933999300003,
          0.10121307522058487,
          0.10111259669065475,
          0.10102390497922897,
          0.1009773388504982,
          0.10096633434295654,
          0.10095193982124329,
          0.10090522468090057,
          0.10083422809839249,
          0.1007654070854187,
          0.10071162134408951,
          0.1006653755903244,
          0.10061702132225037,
          0.10057035088539124,
          0.10053600370883942,
          0.10051197558641434,
          0.10047894716262817,
          0.10042038559913635,
          0.10034456849098206,
          0.10027443617582321,
          0.10022251307964325,
          0.10018524527549744,
          0.10015465319156647,
          0.10012452304363251,
          0.10009147971868515,
          0.10006040334701538,
          0.10003890842199326,
          0.10002078115940094,
          0.0999857559800148,
          0.09992215037345886,
          0.09984491765499115
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_3",
         "type": "scatter",
         "y": [
          0.09570275247097015,
          0.09783785790205002,
          0.0977279469370842,
          0.09768140316009521,
          0.09766960144042969,
          0.09731916338205338,
          0.09751854836940765,
          0.09764969348907471,
          0.0975339412689209,
          0.09729146957397461,
          0.0972953736782074,
          0.09716668725013733,
          0.09708773344755173,
          0.09703296422958374,
          0.09639200568199158,
          0.09611314535140991,
          0.09603948891162872,
          0.09588923305273056,
          0.09582018852233887,
          0.09589802473783493,
          0.09583114832639694,
          0.09585455805063248,
          0.09563589096069336,
          0.09562187641859055,
          0.09568046778440475,
          0.09567039459943771,
          0.09568797051906586,
          0.0958099365234375,
          0.09602072834968567,
          0.09654852747917175,
          0.0971871018409729,
          0.0972222089767456,
          0.09710882604122162,
          0.09689416736364365,
          0.09689320623874664,
          0.09718577563762665,
          0.09721255302429199,
          0.09719764441251755,
          0.09739133715629578,
          0.09753457456827164,
          0.09767580777406693,
          0.09769251942634583,
          0.09767436236143112,
          0.09754210710525513,
          0.09769519418478012,
          0.09778183698654175,
          0.09799503535032272,
          0.09801488369703293,
          0.09796317666769028,
          0.09788104891777039,
          0.0979585349559784,
          0.0979800671339035,
          0.09797318279743195,
          0.0977657288312912,
          0.09760139882564545,
          0.09753359854221344,
          0.09745143353939056,
          0.09749527275562286,
          0.09769121557474136,
          0.09767971187829971,
          0.09763217717409134,
          0.09758177399635315,
          0.09747102856636047,
          0.09733542799949646,
          0.09730634093284607,
          0.09753134101629257,
          0.0976446345448494,
          0.09789595752954483,
          0.09796302765607834,
          0.0977892056107521,
          0.09761548787355423,
          0.09745308756828308,
          0.09748434275388718,
          0.09754374623298645,
          0.0976177379488945,
          0.09768114238977432,
          0.0978350043296814,
          0.09784483909606934,
          0.09771822392940521,
          0.09767069667577744,
          0.09777919948101044,
          0.09778928756713867,
          0.09770359098911285,
          0.0974256694316864,
          0.09762486815452576,
          0.09793319553136826,
          0.09827299416065216,
          0.09884258359670639,
          0.0988791361451149,
          0.0988764762878418,
          0.09899667650461197,
          0.0991937592625618,
          0.09937213361263275,
          0.09929005801677704,
          0.09895149618387222,
          0.09893219918012619,
          0.09907430410385132,
          0.09911604225635529,
          0.09899575263261795,
          0.09895745664834976,
          0.09925570338964462,
          0.0995398685336113,
          0.09957560896873474,
          0.0992715060710907,
          0.09889265894889832,
          0.09878396987915039,
          0.09907606989145279,
          0.09948612004518509,
          0.0995853990316391,
          0.09918541461229324,
          0.0988522320985794,
          0.09885367751121521,
          0.09884834289550781,
          0.09874310344457626,
          0.09869545698165894,
          0.09868007153272629,
          0.09870946407318115,
          0.0987592339515686,
          0.09882500022649765,
          0.09892626851797104,
          0.09905461966991425,
          0.09898210316896439,
          0.09869799762964249,
          0.09848912805318832,
          0.09856962412595749,
          0.09884002804756165,
          0.09873151779174805,
          0.09850279241800308,
          0.0984359011054039,
          0.09830214828252792,
          0.09831339120864868,
          0.09831389784812927,
          0.098258838057518,
          0.09826400876045227,
          0.09840831905603409,
          0.09864069521427155,
          0.09873056411743164,
          0.09889528155326843,
          0.09901443868875504,
          0.09902545809745789,
          0.09903907030820847,
          0.0989178791642189,
          0.09905063360929489,
          0.09952600300312042,
          0.09928731620311737,
          0.0990997925400734,
          0.09913153201341629,
          0.09919573366641998,
          0.09904637932777405,
          0.09905139356851578,
          0.099055714905262,
          0.09911174327135086,
          0.09941864013671875,
          0.10000988095998764,
          0.10020892322063446,
          0.10032457113265991,
          0.10036704689264297,
          0.1005367636680603,
          0.10055366158485413,
          0.10049882531166077,
          0.10020502656698227,
          0.10007914900779724,
          0.1001560389995575,
          0.1002967432141304,
          0.10027233511209488,
          0.10001708567142487,
          0.09982461482286453,
          0.09979408979415894,
          0.09970931708812714,
          0.09947846829891205,
          0.09912732988595963,
          0.09908817708492279,
          0.0990314781665802,
          0.09898217022418976,
          0.09896820783615112,
          0.09886904060840607,
          0.09883246570825577,
          0.09886591136455536,
          0.09939312189817429,
          0.10033288598060608,
          0.10118216276168823,
          0.10169975459575653,
          0.10188480466604233,
          0.10196267068386078,
          0.10155262798070908,
          0.10084419697523117,
          0.10019417107105255,
          0.0996968075633049,
          0.0993332713842392,
          0.09916911274194717,
          0.09908711165189743,
          0.09902075678110123,
          0.09897765517234802,
          0.09901487082242966,
          0.09948054701089859,
          0.0995873212814331,
          0.09971962869167328,
          0.09951667487621307,
          0.09921001642942429,
          0.09905192255973816,
          0.09902811795473099,
          0.09904439002275467,
          0.09911621361970901,
          0.09924037009477615,
          0.09924238175153732,
          0.09913016855716705,
          0.09912779182195663,
          0.09920327365398407,
          0.09915698319673538,
          0.09902813285589218,
          0.09894772619009018,
          0.09889470040798187,
          0.0988636314868927,
          0.09886332601308823,
          0.09886787086725235,
          0.09885410219430923,
          0.0988779291510582,
          0.09887249022722244,
          0.09886418282985687,
          0.09882526099681854,
          0.09875082969665527,
          0.0986989438533783,
          0.09868398308753967,
          0.09868212044239044,
          0.09866137057542801,
          0.09861702471971512,
          0.09859230369329453,
          0.09856388717889786,
          0.09849169850349426,
          0.09843173623085022,
          0.09838761389255524,
          0.09835653752088547,
          0.09833478182554245,
          0.09834723174571991,
          0.09832850098609924,
          0.09828502684831619,
          0.09820600599050522,
          0.09818822145462036,
          0.09813280403614044,
          0.0980115756392479,
          0.09796811640262604,
          0.09794052690267563,
          0.0979369729757309,
          0.09794178605079651,
          0.0979231521487236,
          0.09789668023586273,
          0.09783323109149933,
          0.09774213284254074,
          0.09764949977397919,
          0.0975370705127716,
          0.09744502604007721,
          0.097402423620224,
          0.097279392182827,
          0.09700744599103928,
          0.09707660973072052,
          0.09689533710479736,
          0.0968976765871048,
          0.09702254086732864,
          0.0968656986951828,
          0.09691496193408966,
          0.09685999155044556,
          0.09671129286289215,
          0.096589595079422,
          0.09651697427034378,
          0.09639612585306168,
          0.09631979465484619,
          0.09625610709190369,
          0.09619763493537903,
          0.09611409157514572,
          0.09600639343261719,
          0.0959877222776413,
          0.09595225751399994,
          0.09598159044981003,
          0.09600420296192169,
          0.09597443789243698,
          0.09590530395507812,
          0.09588745981454849,
          0.09590992331504822,
          0.09592375159263611,
          0.09594740718603134,
          0.09594297409057617,
          0.09589807689189911,
          0.09581753611564636
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_4",
         "type": "scatter",
         "y": [
          0.33118218183517456,
          0.33213403820991516,
          0.33190250396728516,
          0.33159804344177246,
          0.33186373114585876,
          0.33169665932655334,
          0.33146825432777405,
          0.3314823508262634,
          0.331325888633728,
          0.3312625586986542,
          0.33124545216560364,
          0.33116552233695984,
          0.3311583697795868,
          0.33112233877182007,
          0.33106884360313416,
          0.3310450613498688,
          0.3310258090496063,
          0.331022709608078,
          0.33103036880493164,
          0.3310207724571228,
          0.3309997320175171,
          0.33102110028266907,
          0.33102595806121826,
          0.33102551102638245,
          0.331041544675827,
          0.331022709608078,
          0.3309914767742157,
          0.3309805393218994,
          0.3309806287288666,
          0.3309914767742157,
          0.3310159742832184,
          0.3310207426548004,
          0.3310209810733795,
          0.33103665709495544,
          0.3310280442237854,
          0.3310090899467468,
          0.3310219943523407,
          0.3310275673866272,
          0.33103105425834656,
          0.3310626149177551,
          0.33104774355888367,
          0.3310610055923462,
          0.33121567964553833,
          0.3313126564025879,
          0.3312477171421051,
          0.3311956226825714,
          0.3311927914619446,
          0.3311549425125122,
          0.3311278522014618,
          0.3311717212200165,
          0.3312046527862549,
          0.33128273487091064,
          0.33134880661964417,
          0.33130165934562683,
          0.33129966259002686,
          0.3312837779521942,
          0.33120280504226685,
          0.3312234580516815,
          0.33129391074180603,
          0.33124086260795593,
          0.33122092485427856,
          0.3313416838645935,
          0.33141353726387024,
          0.3313784897327423,
          0.3313848376274109,
          0.3315635323524475,
          0.33184942603111267,
          0.3321038484573364,
          0.3322022557258606,
          0.3320890963077545,
          0.3318510055541992,
          0.33167538046836853,
          0.33175551891326904,
          0.3319607377052307,
          0.33198028802871704,
          0.3319410979747772,
          0.33206185698509216,
          0.33212384581565857,
          0.3320539891719818,
          0.3320499360561371,
          0.33209529519081116,
          0.3321477770805359,
          0.3321268856525421,
          0.3319682776927948,
          0.3318902254104614,
          0.3320470154285431,
          0.33225446939468384,
          0.33233463764190674,
          0.3323611617088318,
          0.3324168026447296,
          0.33247414231300354,
          0.33260706067085266,
          0.33274567127227783,
          0.33259132504463196,
          0.33225786685943604,
          0.3321695923805237,
          0.3323594927787781,
          0.3324683904647827,
          0.3323148787021637,
          0.3322201073169708,
          0.3324790596961975,
          0.33284464478492737,
          0.33291149139404297,
          0.33258581161499023,
          0.3321523368358612,
          0.3320342004299164,
          0.3324158489704132,
          0.3329657316207886,
          0.3330298364162445,
          0.33256959915161133,
          0.3322230577468872,
          0.3321641683578491,
          0.3321085274219513,
          0.3320215344429016,
          0.3319365382194519,
          0.3318638503551483,
          0.331910103559494,
          0.33205780386924744,
          0.33218806982040405,
          0.3322998583316803,
          0.3324336111545563,
          0.33248984813690186,
          0.3323867619037628,
          0.3323187828063965,
          0.3324378728866577,
          0.3325051963329315,
          0.3323935568332672,
          0.33233705163002014,
          0.3323671519756317,
          0.33232638239860535,
          0.33224526047706604,
          0.3321162462234497,
          0.33189913630485535,
          0.33181658387184143,
          0.3320487439632416,
          0.3323943316936493,
          0.33254578709602356,
          0.3324536383152008,
          0.3323145806789398,
          0.3322409391403198,
          0.33214566111564636,
          0.33204886317253113,
          0.3320482075214386,
          0.3319850265979767,
          0.33174440264701843,
          0.33158108592033386,
          0.33162400126457214,
          0.33162468671798706,
          0.3314899802207947,
          0.3314223885536194,
          0.3314673602581024,
          0.33149638772010803,
          0.3315003216266632,
          0.331520676612854,
          0.33153459429740906,
          0.3315417766571045,
          0.3315820097923279,
          0.33166396617889404,
          0.33174940943717957,
          0.33178165555000305,
          0.33172646164894104,
          0.3316454589366913,
          0.33164283633232117,
          0.3316781520843506,
          0.3316005766391754,
          0.331412136554718,
          0.33128583431243896,
          0.33131644129753113,
          0.33141112327575684,
          0.3314363360404968,
          0.3313678801059723,
          0.33128321170806885,
          0.33127281069755554,
          0.33134815096855164,
          0.33142900466918945,
          0.3314322829246521,
          0.3313943147659302,
          0.3314305245876312,
          0.33154934644699097,
          0.3316308259963989,
          0.33160752058029175,
          0.33152496814727783,
          0.33143341541290283,
          0.33135393261909485,
          0.33131837844848633,
          0.3313324451446533,
          0.3313314616680145,
          0.3312736451625824,
          0.3312303125858307,
          0.331264853477478,
          0.33132508397102356,
          0.33133465051651,
          0.3313390016555786,
          0.3314400315284729,
          0.33160966634750366,
          0.33170047402381897,
          0.33163705468177795,
          0.3314879834651947,
          0.3313479423522949,
          0.33124756813049316,
          0.33120107650756836,
          0.3312382698059082,
          0.3313443958759308,
          0.3314319849014282,
          0.33144423365592957,
          0.3314097821712494,
          0.3313840627670288,
          0.3313823640346527,
          0.3313760459423065,
          0.3313363790512085,
          0.3312727212905884,
          0.33122608065605164,
          0.33121824264526367,
          0.3312182128429413,
          0.3311941623687744,
          0.3311617970466614,
          0.33115389943122864,
          0.3311612010002136,
          0.33114901185035706,
          0.3311121463775635,
          0.33107614517211914,
          0.33106088638305664,
          0.33105936646461487,
          0.3310542404651642,
          0.3310452997684479,
          0.33104413747787476,
          0.33105242252349854,
          0.3310581147670746,
          0.33105483651161194,
          0.33104681968688965,
          0.33104100823402405,
          0.3310437500476837,
          0.33105745911598206,
          0.3310708999633789,
          0.33106639981269836,
          0.331043541431427,
          0.33102163672447205,
          0.3310129940509796,
          0.33101046085357666,
          0.3310057520866394,
          0.3310029208660126,
          0.3310059607028961,
          0.3310098648071289,
          0.3310104012489319,
          0.331009179353714,
          0.3310072422027588,
          0.3310023546218872,
          0.3309969902038574,
          0.33099818229675293,
          0.3310076892375946,
          0.3310215175151825,
          0.3310363292694092,
          0.3310477137565613,
          0.331046998500824,
          0.3310316205024719,
          0.33101361989974976,
          0.33100542426109314,
          0.3310055434703827,
          0.33100682497024536,
          0.3310088813304901,
          0.33101215958595276,
          0.33101147413253784,
          0.33100655674934387,
          0.33100640773773193,
          0.33101484179496765,
          0.33102211356163025,
          0.33101826906204224,
          0.3310072124004364,
          0.3309994339942932,
          0.33099737763404846,
          0.3309958577156067,
          0.3309919536113739,
          0.33098745346069336,
          0.3309842348098755,
          0.330982506275177,
          0.3309811055660248,
          0.33097898960113525,
          0.3309803605079651,
          0.33099207282066345,
          0.33101221919059753,
          0.3310263156890869,
          0.3310219645500183,
          0.33100426197052
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_4",
         "type": "scatter",
         "y": [
          0.338446706533432,
          0.34058183431625366,
          0.34047192335128784,
          0.34042537212371826,
          0.34041357040405273,
          0.3400631248950958,
          0.3402625322341919,
          0.34039366245269775,
          0.34027791023254395,
          0.34003543853759766,
          0.34003934264183044,
          0.3399106562137604,
          0.33983170986175537,
          0.3397769331932068,
          0.3391359746456146,
          0.33885711431503296,
          0.33878347277641296,
          0.3386332094669342,
          0.3385641574859619,
          0.3386420011520386,
          0.3385751247406006,
          0.3385985195636749,
          0.3383798599243164,
          0.3383658528327942,
          0.3384244441986084,
          0.33841437101364136,
          0.3384319245815277,
          0.33855390548706055,
          0.3387646973133087,
          0.3392924964427948,
          0.33993107080459595,
          0.33996617794036865,
          0.33985280990600586,
          0.3396381437778473,
          0.3396371603012085,
          0.3399297595024109,
          0.33995652198791504,
          0.3399416208267212,
          0.3401353061199188,
          0.3402785360813141,
          0.3404197692871094,
          0.34043648838996887,
          0.34041833877563477,
          0.3402860760688782,
          0.34043917059898376,
          0.3405258059501648,
          0.34073901176452637,
          0.3407588601112366,
          0.3407071530818939,
          0.34062501788139343,
          0.34070250391960144,
          0.34072402119636536,
          0.3407171666622162,
          0.34050971269607544,
          0.3403453826904297,
          0.3402775526046753,
          0.3401954174041748,
          0.3402392268180847,
          0.3404351770877838,
          0.34042367339134216,
          0.3403761386871338,
          0.3403257429599762,
          0.3402149975299835,
          0.3400793969631195,
          0.3400503098964691,
          0.3402753174304962,
          0.34038859605789185,
          0.3406399190425873,
          0.340707004070282,
          0.34053316712379456,
          0.3403594493865967,
          0.34019705653190613,
          0.3402283191680908,
          0.3402877151966095,
          0.34036171436309814,
          0.3404251039028168,
          0.34057897329330444,
          0.3405888080596924,
          0.34046217799186707,
          0.3404146730899811,
          0.3405231535434723,
          0.3405332565307617,
          0.3404475450515747,
          0.34016963839530945,
          0.3403688371181488,
          0.3406771719455719,
          0.341016948223114,
          0.34158656001091003,
          0.34162309765815735,
          0.34162044525146484,
          0.3417406380176544,
          0.34193772077560425,
          0.342116117477417,
          0.3420340120792389,
          0.3416954576969147,
          0.34167617559432983,
          0.34181827306747437,
          0.34185999631881714,
          0.3417397141456604,
          0.3417014181613922,
          0.34199967980384827,
          0.34228384494781494,
          0.3423195779323578,
          0.34201547503471375,
          0.34163662791252136,
          0.34152793884277344,
          0.34182003140449524,
          0.34223008155822754,
          0.34232938289642334,
          0.3419293761253357,
          0.34159618616104126,
          0.34159764647483826,
          0.34159231185913086,
          0.3414870798587799,
          0.341439425945282,
          0.34142404794692993,
          0.3414534330368042,
          0.34150320291519165,
          0.3415689766407013,
          0.3416702449321747,
          0.3417986035346985,
          0.34172606468200684,
          0.34144195914268494,
          0.34123310446739197,
          0.34131360054016113,
          0.3415839970111847,
          0.3414754867553711,
          0.34124675393104553,
          0.34117987751960754,
          0.34104612469673157,
          0.34105736017227173,
          0.3410578668117523,
          0.34100279211997986,
          0.3410079777240753,
          0.34115228056907654,
          0.3413846790790558,
          0.3414745330810547,
          0.3416392505168915,
          0.3417584002017975,
          0.34176942706108093,
          0.3417830467224121,
          0.34166184067726135,
          0.34179461002349854,
          0.34226998686790466,
          0.3420312702655792,
          0.34184375405311584,
          0.34187549352645874,
          0.34193968772888184,
          0.3417903482913971,
          0.3417953550815582,
          0.34179967641830444,
          0.3418557047843933,
          0.3421626091003418,
          0.3427538573741913,
          0.3429528772830963,
          0.34306854009628296,
          0.3431110084056854,
          0.34328073263168335,
          0.3432976305484772,
          0.3432427942752838,
          0.3429490029811859,
          0.3428231179714203,
          0.34290000796318054,
          0.34304070472717285,
          0.34301629662513733,
          0.3427610397338867,
          0.342568576335907,
          0.342538058757782,
          0.3424533009529114,
          0.3422224521636963,
          0.3418712913990021,
          0.34183216094970703,
          0.34177544713020325,
          0.3417261242866516,
          0.34171217679977417,
          0.3416130244731903,
          0.3415764272212982,
          0.3416098654270172,
          0.34213709831237793,
          0.3430768549442291,
          0.3439261317253113,
          0.34444373846054077,
          0.34462878108024597,
          0.344706654548645,
          0.3442966043949127,
          0.3435881733894348,
          0.3429381549358368,
          0.34244078397750854,
          0.34207722544670105,
          0.3419130742549896,
          0.3418310880661011,
          0.3417647182941437,
          0.34172162413597107,
          0.3417588472366333,
          0.34222450852394104,
          0.34233129024505615,
          0.3424636125564575,
          0.3422606289386749,
          0.34195399284362793,
          0.3417958915233612,
          0.34177207946777344,
          0.3417883515357971,
          0.34186017513275146,
          0.3419843316078186,
          0.34198635816574097,
          0.3418741226196289,
          0.3418717682361603,
          0.3419472277164459,
          0.34190094470977783,
          0.3417721092700958,
          0.3416917026042938,
          0.3416386842727661,
          0.34160760045051575,
          0.34160730242729187,
          0.3416118323802948,
          0.3415980637073517,
          0.34162190556526184,
          0.3416164517402649,
          0.3416081368923187,
          0.3415692150592804,
          0.3414947986602783,
          0.34144291281700134,
          0.3414279520511627,
          0.3414260745048523,
          0.34140533208847046,
          0.34136098623275757,
          0.34133628010749817,
          0.3413078486919403,
          0.3412356674671173,
          0.34117570519447327,
          0.3411315977573395,
          0.3411005139350891,
          0.3410787582397461,
          0.34109121561050415,
          0.3410724699497223,
          0.34102898836135864,
          0.34094998240470886,
          0.3409321904182434,
          0.3408767580986023,
          0.34075555205345154,
          0.3407121002674103,
          0.3406845033168793,
          0.34068092703819275,
          0.34068575501441956,
          0.34066712856292725,
          0.3406406342983246,
          0.34057721495628357,
          0.3404861092567444,
          0.3403934836387634,
          0.34028103947639465,
          0.34018898010253906,
          0.34014639258384705,
          0.34002336859703064,
          0.3397514224052429,
          0.33982059359550476,
          0.3396393060684204,
          0.33964166045188904,
          0.3397665023803711,
          0.33960965275764465,
          0.3396589159965515,
          0.3396039605140686,
          0.339455246925354,
          0.33933356404304504,
          0.33926093578338623,
          0.3391400873661041,
          0.33906376361846924,
          0.33900007605552673,
          0.3389416038990021,
          0.33885806798934937,
          0.33875036239624023,
          0.33873170614242554,
          0.3386962115764618,
          0.33872556686401367,
          0.33874818682670593,
          0.3387184143066406,
          0.33864927291870117,
          0.33863142132759094,
          0.33865389227867126,
          0.33866772055625916,
          0.338691383600235,
          0.3386869430541992,
          0.33864206075668335,
          0.3385615050792694
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_5",
         "type": "scatter",
         "y": [
          0.7793068289756775,
          0.7870516180992126,
          0.7854800820350647,
          0.7840408086776733,
          0.7857464551925659,
          0.7847184538841248,
          0.7831268906593323,
          0.783094048500061,
          0.781794011592865,
          0.7807654738426208,
          0.7803858518600464,
          0.7798801064491272,
          0.7796006798744202,
          0.7788047194480896,
          0.778106689453125,
          0.7774156332015991,
          0.77677983045578,
          0.7761818170547485,
          0.775343120098114,
          0.7749175429344177,
          0.7744836211204529,
          0.7751001119613647,
          0.775661051273346,
          0.7757643461227417,
          0.7759875059127808,
          0.7753451466560364,
          0.7745172381401062,
          0.77446049451828,
          0.7747895121574402,
          0.7759923934936523,
          0.7776433825492859,
          0.7781173586845398,
          0.7779639959335327,
          0.7779214978218079,
          0.778127908706665,
          0.7785139083862305,
          0.7784801721572876,
          0.7782368659973145,
          0.7787137627601624,
          0.779417097568512,
          0.7793737053871155,
          0.7793359160423279,
          0.7799783945083618,
          0.7805543541908264,
          0.7806336283683777,
          0.7807704210281372,
          0.7811805009841919,
          0.7812525033950806,
          0.7808637022972107,
          0.7808188199996948,
          0.7813446521759033,
          0.7822263240814209,
          0.7824264764785767,
          0.7816792726516724,
          0.7813077569007874,
          0.7813092470169067,
          0.7811047434806824,
          0.7813394069671631,
          0.7820871472358704,
          0.782171905040741,
          0.7822608947753906,
          0.7831131219863892,
          0.7837414145469666,
          0.7837914228439331,
          0.7840543985366821,
          0.7852573990821838,
          0.7868231534957886,
          0.788053572177887,
          0.7887100577354431,
          0.7884563207626343,
          0.7874143123626709,
          0.7866105437278748,
          0.7870977520942688,
          0.7881636619567871,
          0.7883199453353882,
          0.7881360054016113,
          0.7886304259300232,
          0.7888835072517395,
          0.7885253429412842,
          0.7883529663085938,
          0.7882863879203796,
          0.7882509827613831,
          0.7880050539970398,
          0.787304162979126,
          0.7871232628822327,
          0.7879033088684082,
          0.7886736392974854,
          0.7890263795852661,
          0.789461076259613,
          0.7899532318115234,
          0.7901161909103394,
          0.7904061675071716,
          0.7908578515052795,
          0.7903165221214294,
          0.7890085577964783,
          0.788571298122406,
          0.7893509864807129,
          0.7899506688117981,
          0.7893713116645813,
          0.7888797521591187,
          0.7899119257926941,
          0.7915050983428955,
          0.7919191718101501,
          0.7906380891799927,
          0.7887399792671204,
          0.7881700396537781,
          0.7898281812667847,
          0.7921119928359985,
          0.7921914458274841,
          0.7901256680488586,
          0.7887263298034668,
          0.7886562347412109,
          0.7885903120040894,
          0.7882918119430542,
          0.7878982424736023,
          0.7875095009803772,
          0.7876338958740234,
          0.7882635593414307,
          0.7889042496681213,
          0.7894348502159119,
          0.7899903059005737,
          0.790271520614624,
          0.7898796200752258,
          0.7894706130027771,
          0.7898357510566711,
          0.7901571989059448,
          0.7897401452064514,
          0.7894459366798401,
          0.7894930243492126,
          0.7892336845397949,
          0.7887468338012695,
          0.787979245185852,
          0.7868310809135437,
          0.7864557504653931,
          0.7876946330070496,
          0.7894260287284851,
          0.7900286912918091,
          0.7893369197845459,
          0.7885345220565796,
          0.7882418036460876,
          0.7878641486167908,
          0.7873887419700623,
          0.7873666286468506,
          0.7870599031448364,
          0.7858769297599792,
          0.7850871682167053,
          0.785374104976654,
          0.7855240702629089,
          0.7850199937820435,
          0.784748911857605,
          0.784935474395752,
          0.7851972579956055,
          0.7857035398483276,
          0.7864552140235901,
          0.7869362831115723,
          0.7870968580245972,
          0.7873552441596985,
          0.7878022193908691,
          0.788139283657074,
          0.788092851638794,
          0.7875809669494629,
          0.7870222926139832,
          0.7870727777481079,
          0.7874870300292969,
          0.787295937538147,
          0.7864806056022644,
          0.7859668135643005,
          0.7860498428344727,
          0.786139726638794,
          0.7858301401138306,
          0.7852883338928223,
          0.7848555445671082,
          0.7847655415534973,
          0.7849228382110596,
          0.7849027514457703,
          0.78448086977005,
          0.7840837836265564,
          0.7844062447547913,
          0.7856832146644592,
          0.7875216603279114,
          0.789291501045227,
          0.7904545664787292,
          0.7907096147537231,
          0.7901500463485718,
          0.789190411567688,
          0.7881661057472229,
          0.7870635986328125,
          0.7858790159225464,
          0.7850052118301392,
          0.7847191691398621,
          0.784726083278656,
          0.7846279740333557,
          0.7845848798751831,
          0.7850671410560608,
          0.7860199809074402,
          0.7867477536201477,
          0.786686897277832,
          0.7860135436058044,
          0.7852782607078552,
          0.784735918045044,
          0.7844328880310059,
          0.7845525145530701,
          0.7850908041000366,
          0.7855478525161743,
          0.7855571508407593,
          0.7853602170944214,
          0.7853348255157471,
          0.7854474782943726,
          0.7853708267211914,
          0.7849545478820801,
          0.7843766212463379,
          0.7839241027832031,
          0.7837383151054382,
          0.7836800813674927,
          0.7835853695869446,
          0.783477246761322,
          0.7834317684173584,
          0.7833808660507202,
          0.7831999659538269,
          0.7828799486160278,
          0.7825350761413574,
          0.7822752594947815,
          0.7820863723754883,
          0.7818542122840881,
          0.7815587520599365,
          0.7813112735748291,
          0.7811856269836426,
          0.7810685634613037,
          0.7808283567428589,
          0.7804882526397705,
          0.780168890953064,
          0.7799875140190125,
          0.7799834609031677,
          0.7800242900848389,
          0.7798940539360046,
          0.7795767188072205,
          0.7792875170707703,
          0.7791245579719543,
          0.7789300680160522,
          0.7786257863044739,
          0.7784144878387451,
          0.7784165740013123,
          0.778438150882721,
          0.7783063650131226,
          0.7780954837799072,
          0.7778685092926025,
          0.7775190472602844,
          0.7770734429359436,
          0.7767919898033142,
          0.7768062949180603,
          0.7769824266433716,
          0.7771689891815186,
          0.7772855162620544,
          0.7772036790847778,
          0.7768806219100952,
          0.7765519618988037,
          0.7764516472816467,
          0.7764958739280701,
          0.7764729857444763,
          0.7763862013816833,
          0.7763342261314392,
          0.7762672901153564,
          0.7761445045471191,
          0.7760925889015198,
          0.7761990427970886,
          0.7763369679450989,
          0.7763506174087524,
          0.7762523293495178,
          0.7761179208755493,
          0.7759294509887695,
          0.7756791114807129,
          0.7754430770874023,
          0.7752715349197388,
          0.7751267552375793,
          0.7749940752983093,
          0.774893045425415,
          0.7748143672943115,
          0.7748062610626221,
          0.7749956250190735,
          0.7753696441650391,
          0.7756447196006775,
          0.7755411863327026,
          0.7751446962356567
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_5",
         "type": "scatter",
         "y": [
          0.7860087752342224,
          0.7881438732147217,
          0.7880339622497559,
          0.787987470626831,
          0.7879756689071655,
          0.7876251935958862,
          0.7878245711326599,
          0.7879557013511658,
          0.787839949131012,
          0.7875974774360657,
          0.7876014113426208,
          0.7874727249145508,
          0.7873937487602234,
          0.7873390316963196,
          0.786698043346405,
          0.786419153213501,
          0.7863455414772034,
          0.7861952781677246,
          0.7861262559890747,
          0.7862040400505066,
          0.7861371636390686,
          0.7861605882644653,
          0.7859418988227844,
          0.7859278917312622,
          0.7859864830970764,
          0.7859764099121094,
          0.7859939932823181,
          0.7861159443855286,
          0.7863267660140991,
          0.7868545651435852,
          0.787493109703064,
          0.7875282168388367,
          0.7874148488044739,
          0.7872002124786377,
          0.7871992588043213,
          0.7874917984008789,
          0.7875185608863831,
          0.7875036597251892,
          0.7876973748207092,
          0.7878406047821045,
          0.7879818677902222,
          0.7879985570907593,
          0.7879803776741028,
          0.7878481149673462,
          0.7880012392997742,
          0.7880878448486328,
          0.7883010506629944,
          0.7883208990097046,
          0.7882692217826843,
          0.7881870865821838,
          0.7882645726203918,
          0.7882860898971558,
          0.7882792353630066,
          0.7880717515945435,
          0.7879074215888977,
          0.7878396511077881,
          0.7877574563026428,
          0.7878013253211975,
          0.7879972457885742,
          0.7879857420921326,
          0.7879382371902466,
          0.7878878116607666,
          0.7877770662307739,
          0.7876414656639099,
          0.7876123785972595,
          0.7878373861312866,
          0.7879506945610046,
          0.7882019877433777,
          0.78826904296875,
          0.788095235824585,
          0.7879215478897095,
          0.7877591252326965,
          0.7877903580665588,
          0.7878497838973999,
          0.7879237532615662,
          0.7879871726036072,
          0.7881410717964172,
          0.7881508469581604,
          0.7880242466926575,
          0.7879767417907715,
          0.7880852222442627,
          0.7880953550338745,
          0.7880096435546875,
          0.7877317070960999,
          0.7879309058189392,
          0.7882392406463623,
          0.7885790467262268,
          0.7891486287117004,
          0.7891851663589478,
          0.7891825437545776,
          0.7893027067184448,
          0.789499819278717,
          0.789678156375885,
          0.7895960807800293,
          0.7892575263977051,
          0.7892382144927979,
          0.7893803119659424,
          0.7894220948219299,
          0.7893018126487732,
          0.7892634868621826,
          0.7895617485046387,
          0.789845883846283,
          0.7898816466331482,
          0.7895775437355042,
          0.7891986966133118,
          0.7890899777412415,
          0.7893821001052856,
          0.7897921800613403,
          0.7898914217948914,
          0.7894914746284485,
          0.789158284664154,
          0.7891597151756287,
          0.7891544103622437,
          0.7890491485595703,
          0.7890015244483948,
          0.788986086845398,
          0.789015531539917,
          0.7890652418136597,
          0.7891310453414917,
          0.7892323136329651,
          0.7893606424331665,
          0.7892881631851196,
          0.7890040278434753,
          0.7887951731681824,
          0.7888756394386292,
          0.7891460657119751,
          0.7890375852584839,
          0.7888088226318359,
          0.788741946220398,
          0.788608193397522,
          0.7886193990707397,
          0.7886199355125427,
          0.7885648608207703,
          0.7885700464248657,
          0.7887143492698669,
          0.7889467477798462,
          0.7890365719795227,
          0.7892013192176819,
          0.7893204689025879,
          0.7893314957618713,
          0.7893450856208801,
          0.7892239093780518,
          0.7893566489219666,
          0.7898320555686951,
          0.7895933389663696,
          0.7894058227539062,
          0.7894375920295715,
          0.7895017862319946,
          0.7893524169921875,
          0.7893574237823486,
          0.7893617749214172,
          0.7894178032875061,
          0.7897246479988098,
          0.7903159260749817,
          0.7905149459838867,
          0.790630578994751,
          0.7906730771064758,
          0.7908427715301514,
          0.7908596992492676,
          0.7908048629760742,
          0.7905110716819763,
          0.7903851866722107,
          0.790462076663971,
          0.7906028032302856,
          0.7905783653259277,
          0.7903231382369995,
          0.7901306748390198,
          0.79010009765625,
          0.7900153398513794,
          0.7897844910621643,
          0.7894333600997925,
          0.789394199848175,
          0.7893375158309937,
          0.7892882227897644,
          0.789274275302887,
          0.7891750931739807,
          0.7891384959220886,
          0.7891719341278076,
          0.789699137210846,
          0.7906389236450195,
          0.7914882302284241,
          0.7920057773590088,
          0.7921908497810364,
          0.792268693447113,
          0.7918586730957031,
          0.7911502122879028,
          0.7905002236366272,
          0.7900028228759766,
          0.7896392941474915,
          0.78947514295578,
          0.7893931269645691,
          0.7893267869949341,
          0.7892836928367615,
          0.7893208861351013,
          0.7897865772247314,
          0.7898933291435242,
          0.7900256514549255,
          0.7898226976394653,
          0.789516031742096,
          0.7893579602241516,
          0.7893341779708862,
          0.7893504500389099,
          0.7894222736358643,
          0.7895464301109314,
          0.789548397064209,
          0.7894362211227417,
          0.7894338369369507,
          0.7895092964172363,
          0.7894630432128906,
          0.7893341779708862,
          0.7892537713050842,
          0.7892007231712341,
          0.7891696691513062,
          0.7891693711280823,
          0.7891739010810852,
          0.7891601324081421,
          0.7891839742660522,
          0.7891785502433777,
          0.7891702055931091,
          0.7891312837600708,
          0.7890568375587463,
          0.7890049815177917,
          0.7889900207519531,
          0.7889881730079651,
          0.7889674305915833,
          0.7889230847358704,
          0.7888983488082886,
          0.7888699173927307,
          0.7887977361679077,
          0.7887377738952637,
          0.7886936664581299,
          0.7886625528335571,
          0.7886407971382141,
          0.7886532545089722,
          0.7886345386505127,
          0.7885910868644714,
          0.7885120511054993,
          0.7884942889213562,
          0.7884388566017151,
          0.7883176207542419,
          0.7882741689682007,
          0.7882465720176697,
          0.7882429957389832,
          0.78824782371521,
          0.7882291674613953,
          0.788202702999115,
          0.788139283657074,
          0.7880481481552124,
          0.7879555225372314,
          0.7878431081771851,
          0.7877510786056519,
          0.7877084612846375,
          0.787585437297821,
          0.7873134613037109,
          0.7873826622962952,
          0.7872013449668884,
          0.7872037291526794,
          0.7873286008834839,
          0.7871717214584351,
          0.7872210144996643,
          0.7871659994125366,
          0.7870173454284668,
          0.7868956327438354,
          0.786823034286499,
          0.7867021560668945,
          0.786625862121582,
          0.7865621447563171,
          0.7865036725997925,
          0.7864201068878174,
          0.786312460899353,
          0.7862937450408936,
          0.7862582802772522,
          0.7862876057624817,
          0.7863102555274963,
          0.7862804532051086,
          0.786211371421814,
          0.7861934900283813,
          0.7862159609794617,
          0.7862297892570496,
          0.7862534523010254,
          0.7862489819526672,
          0.7862040996551514,
          0.7861235737800598
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_6",
         "type": "scatter",
         "y": [
          0.10709914565086365,
          0.11154361069202423,
          0.11164455860853195,
          0.11181938648223877,
          0.11153017729520798,
          0.11112778633832932,
          0.11135472357273102,
          0.11202778667211533,
          0.11217498034238815,
          0.11215179413557053,
          0.11230598390102386,
          0.11255750060081482,
          0.11297478526830673,
          0.11306581646203995,
          0.11283273249864578,
          0.11261839419603348,
          0.11247841268777847,
          0.11224275082349777,
          0.11204592138528824,
          0.11197495460510254,
          0.11175329238176346,
          0.1115017980337143,
          0.11131591349840164,
          0.11111891269683838,
          0.11083012074232101,
          0.11059530079364777,
          0.11063873767852783,
          0.11071903258562088,
          0.11055304110050201,
          0.11022166162729263,
          0.10990520566701889,
          0.10952520370483398,
          0.10917750000953674,
          0.10884121805429459,
          0.10845956951379776,
          0.10814167559146881,
          0.10774116963148117,
          0.1071867048740387,
          0.10677418857812881,
          0.10662771761417389,
          0.10660108923912048,
          0.10660000890493393,
          0.10659028589725494,
          0.10655845701694489,
          0.10657624155282974,
          0.10667813569307327,
          0.10677652806043625,
          0.10678490996360779,
          0.10670143365859985,
          0.10661914944648743,
          0.10662625730037689,
          0.10670548677444458,
          0.10672685503959656,
          0.10665330290794373,
          0.10658509284257889,
          0.10653827339410782,
          0.10651234537363052,
          0.10661541670560837,
          0.10675559192895889,
          0.1067204624414444,
          0.10668463259935379,
          0.10680133104324341,
          0.10692533850669861,
          0.10699031502008438,
          0.10706420242786407,
          0.10721439123153687,
          0.10751008987426758,
          0.10802870243787766,
          0.10852011293172836,
          0.1086815819144249,
          0.10867132246494293,
          0.10869558155536652,
          0.10883066803216934,
          0.10915825515985489,
          0.10946614295244217,
          0.10954239219427109,
          0.10968741029500961,
          0.10998104512691498,
          0.11009672284126282,
          0.11023680865764618,
          0.11058152467012405,
          0.11067266762256622,
          0.11044380068778992,
          0.11043160408735275,
          0.11108408123254776,
          0.11257148534059525,
          0.11443562805652618,
          0.11584281921386719,
          0.11656295508146286,
          0.11697673052549362,
          0.11744752526283264,
          0.11795850098133087,
          0.11820892989635468,
          0.11816786974668503,
          0.1181173101067543,
          0.11815234273672104,
          0.1181483194231987,
          0.11804160475730896,
          0.11795641481876373,
          0.11806168407201767,
          0.11820214241743088,
          0.11806223541498184,
          0.11772239953279495,
          0.11748373508453369,
          0.11741726100444794,
          0.11740570515394211,
          0.11737450957298279,
          0.11734148859977722,
          0.11727818846702576,
          0.11719692498445511,
          0.11721841245889664,
          0.11728707700967789,
          0.11723592132329941,
          0.1170911192893982,
          0.11695157736539841,
          0.11686104536056519,
          0.1168525293469429,
          0.11689172685146332,
          0.11690682172775269,
          0.11688734591007233,
          0.11682408303022385,
          0.11667249351739883,
          0.11644718050956726,
          0.11625059694051743,
          0.11613544821739197,
          0.11603682488203049,
          0.1159030944108963,
          0.11575872451066971,
          0.11558783054351807,
          0.11533717811107635,
          0.11505307257175446,
          0.11481604725122452,
          0.11461097002029419,
          0.1144130751490593,
          0.11424084007740021,
          0.11406651884317398,
          0.11385440081357956,
          0.11363516002893448,
          0.11342433094978333,
          0.11317887157201767,
          0.11288460344076157,
          0.1125822365283966,
          0.11226574331521988,
          0.11186956614255905,
          0.11141110211610794,
          0.11099902540445328,
          0.11064555495977402,
          0.11026611924171448,
          0.10987730324268341,
          0.10958593338727951,
          0.10942408442497253,
          0.10935986042022705,
          0.10938769578933716,
          0.10947716981172562,
          0.10954246670007706,
          0.10954376310110092,
          0.10951747745275497,
          0.10948774218559265,
          0.10942691564559937,
          0.10932692885398865,
          0.1092279851436615,
          0.10917481780052185,
          0.10917229950428009,
          0.10917195677757263,
          0.10912789404392242,
          0.10906392335891724,
          0.10902517288923264,
          0.10899455100297928,
          0.10892999917268753,
          0.10884405672550201,
          0.10877472162246704,
          0.10872777551412582,
          0.10868918895721436,
          0.10866202414035797,
          0.10865110158920288,
          0.10864697396755219,
          0.10864956676959991,
          0.10869469493627548,
          0.10885199904441833,
          0.10916762053966522,
          0.10959412902593613,
          0.10999838262796402,
          0.11026451736688614,
          0.11036870628595352,
          0.11034106463193893,
          0.11020901799201965,
          0.10998998582363129,
          0.10971236228942871,
          0.10941953957080841,
          0.10915334522724152,
          0.10893744230270386,
          0.10877125710248947,
          0.10864607244729996,
          0.1085728257894516,
          0.10856705904006958,
          0.10860570520162582,
          0.10863321274518967,
          0.10862381756305695,
          0.10859869420528412,
          0.10857756435871124,
          0.10856114327907562,
          0.10855665057897568,
          0.10856056213378906,
          0.1085415929555893,
          0.10848720371723175,
          0.10843615233898163,
          0.1084226667881012,
          0.10842687636613846,
          0.10841301083564758,
          0.10838582366704941,
          0.10837670415639877,
          0.1083926409482956,
          0.10841426998376846,
          0.10841921716928482,
          0.10840275883674622,
          0.10837936401367188,
          0.10836616158485413,
          0.10836771130561829,
          0.108372762799263,
          0.1083676889538765,
          0.10835225880146027,
          0.10833405703306198,
          0.10831252485513687,
          0.1082739308476448,
          0.10821206122636795,
          0.10814135521650314,
          0.10807831585407257,
          0.10802040249109268,
          0.10795731842517853,
          0.10789065808057785,
          0.10783100873231888,
          0.10779199004173279,
          0.10777890682220459,
          0.10777372121810913,
          0.10774531960487366,
          0.10768834501504898,
          0.10762596130371094,
          0.1075693741440773,
          0.1075059175491333,
          0.107431560754776,
          0.10736740380525589,
          0.10732962936162949,
          0.10730892419815063,
          0.1072862446308136,
          0.10724995285272598,
          0.10719697922468185,
          0.10713059455156326,
          0.10706248134374619,
          0.10700630396604538,
          0.10696560889482498,
          0.1069348081946373,
          0.10690627992153168,
          0.10687271505594254,
          0.10682691633701324,
          0.10676975548267365,
          0.10671684145927429,
          0.10668480396270752,
          0.1066756546497345,
          0.10667698830366135,
          0.10667411237955093,
          0.10665968805551529,
          0.10663412511348724,
          0.10660088062286377,
          0.10656339675188065,
          0.10652461647987366,
          0.10648898035287857,
          0.10646281391382217,
          0.10644864290952682,
          0.10644012689590454,
          0.10642609000205994,
          0.10640272498130798,
          0.10637567937374115,
          0.10635165870189667,
          0.10633273422718048,
          0.10631684958934784,
          0.1062999963760376,
          0.10627917945384979,
          0.106258325278759,
          0.10624729096889496,
          0.106249138712883,
          0.1062563806772232,
          0.10626394301652908,
          0.10627774894237518
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_6",
         "type": "scatter",
         "y": [
          0.10602539032697678,
          0.10816049575805664,
          0.10805058479309082,
          0.10800404101610184,
          0.10799223929643631,
          0.10764180123806,
          0.10784118622541428,
          0.10797233134508133,
          0.10785657912492752,
          0.10761409997940063,
          0.10761801153421402,
          0.10748932510614395,
          0.10741037130355835,
          0.10735560208559036,
          0.1067146435379982,
          0.10643578320741653,
          0.10636212676763535,
          0.10621187090873718,
          0.10614282637834549,
          0.10622066259384155,
          0.10615378618240356,
          0.1061771959066391,
          0.10595852881669998,
          0.10594451427459717,
          0.10600310564041138,
          0.10599303245544434,
          0.10601060837507248,
          0.10613257437944412,
          0.10634336620569229,
          0.10687116533517838,
          0.10750973969697952,
          0.10754484683275223,
          0.10743146389722824,
          0.10721680521965027,
          0.10721584409475327,
          0.10750841349363327,
          0.10753519088029861,
          0.10752028226852417,
          0.1077139750123024,
          0.10785721242427826,
          0.10799844563007355,
          0.10801515728235245,
          0.10799700021743774,
          0.10786474496126175,
          0.10801783204078674,
          0.10810447484254837,
          0.10831767320632935,
          0.10833752155303955,
          0.1082858145236969,
          0.10820368677377701,
          0.10828117281198502,
          0.10830270498991013,
          0.10829582065343857,
          0.10808836668729782,
          0.10792403668165207,
          0.10785623639822006,
          0.10777407139539719,
          0.10781791061162949,
          0.10801385343074799,
          0.10800234973430634,
          0.10795481503009796,
          0.10790441185235977,
          0.1077936664223671,
          0.10765806585550308,
          0.10762897878885269,
          0.1078539788722992,
          0.10796727240085602,
          0.10821859538555145,
          0.10828566551208496,
          0.10811184346675873,
          0.10793812572956085,
          0.1077757254242897,
          0.1078069806098938,
          0.10786638408899307,
          0.10794037580490112,
          0.10800378024578094,
          0.10815764218568802,
          0.10816747695207596,
          0.10804086178541183,
          0.10799333453178406,
          0.10810183733701706,
          0.1081119254231453,
          0.10802622884511948,
          0.10774830728769302,
          0.10794750601053238,
          0.10825583338737488,
          0.10859563201665878,
          0.10916522145271301,
          0.10920177400112152,
          0.10919911414384842,
          0.10931931436061859,
          0.10951639711856842,
          0.10969477146863937,
          0.10961269587278366,
          0.10927413403987885,
          0.10925483703613281,
          0.10939694195985794,
          0.10943868011236191,
          0.10931839048862457,
          0.10928009450435638,
          0.10957834124565125,
          0.10986250638961792,
          0.10989824682474136,
          0.10959414392709732,
          0.10921529680490494,
          0.10910660773515701,
          0.10939870774745941,
          0.10980875790119171,
          0.10990803688764572,
          0.10950805246829987,
          0.10917486995458603,
          0.10917631536722183,
          0.10917098075151443,
          0.10906574130058289,
          0.10901809483766556,
          0.10900270938873291,
          0.10903210192918777,
          0.10908187180757523,
          0.10914763808250427,
          0.10924890637397766,
          0.10937725752592087,
          0.10930474102497101,
          0.10902063548564911,
          0.10881176590919495,
          0.10889226198196411,
          0.10916266590356827,
          0.10905415564775467,
          0.1088254302740097,
          0.10875853896141052,
          0.10862478613853455,
          0.1086360290646553,
          0.1086365357041359,
          0.10858147591352463,
          0.10858664661645889,
          0.10873095691204071,
          0.10896333307027817,
          0.10905320197343826,
          0.10921791940927505,
          0.10933707654476166,
          0.10934809595346451,
          0.10936170816421509,
          0.10924051702022552,
          0.10937327146530151,
          0.10984864085912704,
          0.10960995405912399,
          0.10942243039608002,
          0.10945416986942291,
          0.1095183715224266,
          0.10936901718378067,
          0.1093740314245224,
          0.10937835276126862,
          0.10943438112735748,
          0.10974127799272537,
          0.11033251881599426,
          0.11053156107664108,
          0.11064720898866653,
          0.1106896847486496,
          0.11085940152406693,
          0.11087629944086075,
          0.11082146316766739,
          0.11052766442298889,
          0.11040178686380386,
          0.11047867685556412,
          0.11061938107013702,
          0.1105949729681015,
          0.11033972352743149,
          0.11014725267887115,
          0.11011672765016556,
          0.11003195494413376,
          0.10980110615491867,
          0.10944996774196625,
          0.10941081494092941,
          0.10935411602258682,
          0.10930480808019638,
          0.10929084569215775,
          0.10919167846441269,
          0.10915510356426239,
          0.10918854922056198,
          0.10971575975418091,
          0.1106555163860321,
          0.11150480061769485,
          0.11202239245176315,
          0.11220744252204895,
          0.112285315990448,
          0.1118752658367157,
          0.11116683483123779,
          0.11051680892705917,
          0.11001944541931152,
          0.10965590924024582,
          0.1094917505979538,
          0.10940974950790405,
          0.10934339463710785,
          0.10930029302835464,
          0.10933750867843628,
          0.10980318486690521,
          0.10990995913743973,
          0.1100422665476799,
          0.1098393127322197,
          0.10953265428543091,
          0.10937456041574478,
          0.10935075581073761,
          0.10936702787876129,
          0.10943885147571564,
          0.10956300795078278,
          0.10956501960754395,
          0.10945279896259308,
          0.10945042967796326,
          0.10952591150999069,
          0.109479621052742,
          0.1093507707118988,
          0.1092703640460968,
          0.1092173382639885,
          0.10918626934289932,
          0.10918596386909485,
          0.10919050872325897,
          0.10917674005031586,
          0.10920056700706482,
          0.10919512808322906,
          0.1091868206858635,
          0.10914789885282516,
          0.1090734675526619,
          0.10902158170938492,
          0.1090066209435463,
          0.10900475829839706,
          0.10898400843143463,
          0.10893966257572174,
          0.10891494154930115,
          0.10888652503490448,
          0.10881433635950089,
          0.10875437408685684,
          0.10871025174856186,
          0.10867917537689209,
          0.10865741968154907,
          0.10866986960172653,
          0.10865113884210587,
          0.10860766470432281,
          0.10852864384651184,
          0.10851085931062698,
          0.10845544189214706,
          0.10833421349525452,
          0.10829075425863266,
          0.10826316475868225,
          0.10825961083173752,
          0.10826442390680313,
          0.10824579000473022,
          0.10821931809186935,
          0.10815586894750595,
          0.10806477069854736,
          0.10797213762998581,
          0.10785970836877823,
          0.10776766389608383,
          0.10772506147623062,
          0.10760203003883362,
          0.1073300838470459,
          0.10739924758672714,
          0.10721797496080399,
          0.10722031444311142,
          0.10734517872333527,
          0.10718833655118942,
          0.10723759979009628,
          0.10718262940645218,
          0.10703393071889877,
          0.10691223293542862,
          0.1068396121263504,
          0.1067187637090683,
          0.10664243251085281,
          0.10657874494791031,
          0.10652027279138565,
          0.10643672943115234,
          0.10632903128862381,
          0.10631036013364792,
          0.10627489537000656,
          0.10630422830581665,
          0.10632684081792831,
          0.1062970757484436,
          0.10622794926166534,
          0.10621009767055511,
          0.10623256117105484,
          0.10624638944864273,
          0.10627004504203796,
          0.1062656119465828,
          0.10622071474790573,
          0.10614017397165298
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_7",
         "type": "scatter",
         "y": [
          0.35738807916641235,
          0.35661396384239197,
          0.356496125459671,
          0.35629141330718994,
          0.35625743865966797,
          0.3562147915363312,
          0.3561878800392151,
          0.3562222123146057,
          0.3562415540218353,
          0.35622161626815796,
          0.35623008012771606,
          0.35628271102905273,
          0.3563947081565857,
          0.3566044867038727,
          0.35674116015434265,
          0.35671091079711914,
          0.35686245560646057,
          0.35708674788475037,
          0.3572928309440613,
          0.3576933443546295,
          0.3582410514354706,
          0.3588116765022278,
          0.35932302474975586,
          0.3598874509334564,
          0.3607984185218811,
          0.3615048825740814,
          0.36250579357147217,
          0.3638666272163391,
          0.365198016166687,
          0.36838337779045105,
          0.3718383014202118,
          0.37225979566574097,
          0.3709743916988373,
          0.3699762523174286,
          0.37048694491386414,
          0.37220290303230286,
          0.3730064332485199,
          0.37301674485206604,
          0.37372827529907227,
          0.37479859590530396,
          0.3754426836967468,
          0.37539276480674744,
          0.37433043122291565,
          0.373282790184021,
          0.3736857771873474,
          0.3751920163631439,
          0.3769424855709076,
          0.37803778052330017,
          0.37735864520072937,
          0.376514732837677,
          0.3774116039276123,
          0.3782125413417816,
          0.37697935104370117,
          0.374290406703949,
          0.37208765745162964,
          0.37173041701316833,
          0.3719099462032318,
          0.37143048644065857,
          0.3714970052242279,
          0.3718000650405884,
          0.37111416459083557,
          0.37000662088394165,
          0.36884912848472595,
          0.36781764030456543,
          0.36719179153442383,
          0.36681655049324036,
          0.3663599491119385,
          0.3654879331588745,
          0.36443889141082764,
          0.3636525571346283,
          0.3631926476955414,
          0.36303725838661194,
          0.3628763258457184,
          0.3624701201915741,
          0.36233559250831604,
          0.3628169298171997,
          0.3632887005805969,
          0.3630836606025696,
          0.3623584806919098,
          0.3619239032268524,
          0.362145334482193,
          0.3623323142528534,
          0.36180710792541504,
          0.36096006631851196,
          0.36055856943130493,
          0.36061471700668335,
          0.3605118989944458,
          0.3598043620586395,
          0.35891634225845337,
          0.35845881700515747,
          0.35823962092399597,
          0.358113557100296,
          0.35811203718185425,
          0.35787495970726013,
          0.3574581444263458,
          0.3572043776512146,
          0.3570539355278015,
          0.3569321632385254,
          0.3568333387374878,
          0.3569539487361908,
          0.35759061574935913,
          0.3581812083721161,
          0.35790887475013733,
          0.3571617305278778,
          0.3567657172679901,
          0.3567618429660797,
          0.35698720812797546,
          0.3573891222476959,
          0.35752207040786743,
          0.35717886686325073,
          0.35686835646629333,
          0.3568393290042877,
          0.35684290528297424,
          0.35682404041290283,
          0.35683533549308777,
          0.3568558990955353,
          0.35693359375,
          0.3570634424686432,
          0.3572158217430115,
          0.3574303984642029,
          0.3576107621192932,
          0.357605516910553,
          0.35756397247314453,
          0.3577136993408203,
          0.3579358756542206,
          0.35805830359458923,
          0.3582238256931305,
          0.3585495948791504,
          0.35884299874305725,
          0.3590143620967865,
          0.3592682182788849,
          0.3596617579460144,
          0.36015304923057556,
          0.3609080910682678,
          0.36185160279273987,
          0.36244916915893555,
          0.3626994788646698,
          0.36353906989097595,
          0.3653232455253601,
          0.36700743436813354,
          0.3679359555244446,
          0.3688758313655853,
          0.3704550862312317,
          0.3721761405467987,
          0.37336722016334534,
          0.3739543855190277,
          0.3744851052761078,
          0.37561124563217163,
          0.3772026300430298,
          0.37857869267463684,
          0.37966984510421753,
          0.3811637759208679,
          0.3835388123989105,
          0.3862248659133911,
          0.38791099190711975,
          0.38822516798973083,
          0.3881770968437195,
          0.3884366750717163,
          0.38845962285995483,
          0.387747198343277,
          0.38673317432403564,
          0.3863277733325958,
          0.3869769275188446,
          0.3879721760749817,
          0.3881973922252655,
          0.38770028948783875,
          0.3872305154800415,
          0.3867189884185791,
          0.38576430082321167,
          0.38491976261138916,
          0.38479873538017273,
          0.3848706781864166,
          0.3843805491924286,
          0.38350650668144226,
          0.382661372423172,
          0.3818000257015228,
          0.38097885251045227,
          0.3810124695301056,
          0.3832280933856964,
          0.3881719410419464,
          0.3943968713283539,
          0.39922410249710083,
          0.40102553367614746,
          0.40019339323043823,
          0.3978792428970337,
          0.39488622546195984,
          0.3917733430862427,
          0.3890692889690399,
          0.38706162571907043,
          0.38568776845932007,
          0.3846728801727295,
          0.38376882672309875,
          0.3829491138458252,
          0.3825860619544983,
          0.3830781877040863,
          0.38419461250305176,
          0.3851390779018402,
          0.3854661285877228,
          0.385393887758255,
          0.38518980145454407,
          0.38494202494621277,
          0.3847735822200775,
          0.38463732600212097,
          0.3843282461166382,
          0.3839319944381714,
          0.3838883936405182,
          0.384347528219223,
          0.384894996881485,
          0.3850017786026001,
          0.3845594823360443,
          0.3838843107223511,
          0.38333195447921753,
          0.3830874264240265,
          0.3830829858779907,
          0.3832486569881439,
          0.3835819959640503,
          0.3838723301887512,
          0.3838072717189789,
          0.3833297789096832,
          0.3826505243778229,
          0.38197407126426697,
          0.38136357069015503,
          0.3808783292770386,
          0.38053634762763977,
          0.3802167475223541,
          0.3797798752784729,
          0.3792373538017273,
          0.37866395711898804,
          0.3780961036682129,
          0.37756863236427307,
          0.3771076500415802,
          0.37679198384284973,
          0.37666940689086914,
          0.37657710909843445,
          0.37623506784439087,
          0.3756422698497772,
          0.3751336634159088,
          0.3748795986175537,
          0.3746036887168884,
          0.3740375339984894,
          0.37338021397590637,
          0.3729901909828186,
          0.37289515137672424,
          0.37283310294151306,
          0.37258827686309814,
          0.3721221089363098,
          0.3715335726737976,
          0.3709905445575714,
          0.3705809712409973,
          0.37019938230514526,
          0.3697544038295746,
          0.369352251291275,
          0.3690938949584961,
          0.3688576817512512,
          0.36854130029678345,
          0.3683359920978546,
          0.3684849739074707,
          0.3688713610172272,
          0.3690277934074402,
          0.36865943670272827,
          0.3680060803890228,
          0.36753547191619873,
          0.36737245321273804,
          0.36726874113082886,
          0.3670395314693451,
          0.36678287386894226,
          0.3666311800479889,
          0.3664763271808624,
          0.3661109507083893,
          0.36557483673095703,
          0.3652130663394928,
          0.3652593493461609,
          0.36550819873809814,
          0.3655443489551544,
          0.3652673363685608,
          0.36500051617622375,
          0.36508363485336304,
          0.3654785752296448,
          0.36583787202835083,
          0.36586177349090576,
          0.3655480444431305,
          0.36511489748954773,
          0.3647337257862091
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_7",
         "type": "scatter",
         "y": [
          0.37264809012413025,
          0.3747832179069519,
          0.3746733069419861,
          0.3746267557144165,
          0.374614953994751,
          0.37426450848579407,
          0.37446391582489014,
          0.374595046043396,
          0.3744792938232422,
          0.3742368221282959,
          0.3742407262325287,
          0.3741120398044586,
          0.3740330934524536,
          0.37397831678390503,
          0.37333735823631287,
          0.3730584979057312,
          0.3729848563671112,
          0.37283459305763245,
          0.37276554107666016,
          0.3728433847427368,
          0.37277650833129883,
          0.37279990315437317,
          0.37258124351501465,
          0.37256723642349243,
          0.37262582778930664,
          0.3726157546043396,
          0.37263330817222595,
          0.3727552890777588,
          0.37296608090400696,
          0.37349388003349304,
          0.3741324543952942,
          0.3741675615310669,
          0.3740541934967041,
          0.37383952736854553,
          0.37383854389190674,
          0.37413114309310913,
          0.3741579055786133,
          0.37414300441741943,
          0.37433668971061707,
          0.37447991967201233,
          0.3746211528778076,
          0.3746378719806671,
          0.374619722366333,
          0.3744874596595764,
          0.374640554189682,
          0.37472718954086304,
          0.3749403953552246,
          0.3749602437019348,
          0.37490853667259216,
          0.3748264014720917,
          0.3749038875102997,
          0.3749254047870636,
          0.37491855025291443,
          0.3747110962867737,
          0.37454676628112793,
          0.37447893619537354,
          0.37439680099487305,
          0.37444061040878296,
          0.37463656067848206,
          0.3746250569820404,
          0.37457752227783203,
          0.37452712655067444,
          0.37441638112068176,
          0.37428078055381775,
          0.37425169348716736,
          0.37447670102119446,
          0.3745899796485901,
          0.3748413026332855,
          0.3749083876609802,
          0.3747345507144928,
          0.3745608329772949,
          0.37439844012260437,
          0.37442970275878906,
          0.37448909878730774,
          0.3745630979537964,
          0.374626487493515,
          0.3747803568840027,
          0.3747901916503906,
          0.3746635615825653,
          0.3746160566806793,
          0.37472453713417053,
          0.37473464012145996,
          0.37464892864227295,
          0.3743710219860077,
          0.37457022070884705,
          0.37487855553627014,
          0.37521833181381226,
          0.3757879436016083,
          0.3758244812488556,
          0.3758218288421631,
          0.37594202160835266,
          0.3761391043663025,
          0.37631750106811523,
          0.37623539566993713,
          0.3758968412876129,
          0.3758775591850281,
          0.3760196566581726,
          0.3760613799095154,
          0.37594109773635864,
          0.37590280175209045,
          0.3762010633945465,
          0.3764852285385132,
          0.37652096152305603,
          0.376216858625412,
          0.3758380115032196,
          0.3757293224334717,
          0.3760214149951935,
          0.3764314651489258,
          0.3765307664871216,
          0.37613075971603394,
          0.3757975697517395,
          0.3757990300655365,
          0.3757936954498291,
          0.37568846344947815,
          0.3756408095359802,
          0.3756254315376282,
          0.37565481662750244,
          0.3757045865058899,
          0.37577036023139954,
          0.3758716285228729,
          0.37599998712539673,
          0.3759274482727051,
          0.3756433427333832,
          0.3754344880580902,
          0.3755149841308594,
          0.37578538060188293,
          0.37567687034606934,
          0.3754481375217438,
          0.3753812611103058,
          0.3752475082874298,
          0.37525874376296997,
          0.37525925040245056,
          0.3752041757106781,
          0.37520936131477356,
          0.3753536641597748,
          0.37558606266975403,
          0.37567591667175293,
          0.3758406341075897,
          0.3759597837924957,
          0.3759708106517792,
          0.37598443031311035,
          0.3758632242679596,
          0.3759959936141968,
          0.3764713704586029,
          0.37623265385627747,
          0.3760451376438141,
          0.376076877117157,
          0.3761410713195801,
          0.37599173188209534,
          0.37599673867225647,
          0.3760010600090027,
          0.37605708837509155,
          0.37636399269104004,
          0.3769552409648895,
          0.37715426087379456,
          0.3772699236869812,
          0.37731239199638367,
          0.3774821162223816,
          0.3774990141391754,
          0.37744417786598206,
          0.37715038657188416,
          0.37702450156211853,
          0.3771013915538788,
          0.3772420883178711,
          0.37721768021583557,
          0.37696242332458496,
          0.3767699599266052,
          0.3767394423484802,
          0.3766546845436096,
          0.37642383575439453,
          0.3760726749897003,
          0.3760335445404053,
          0.3759768307209015,
          0.37592750787734985,
          0.3759135603904724,
          0.37581440806388855,
          0.37577781081199646,
          0.37581124901771545,
          0.37633848190307617,
          0.37727823853492737,
          0.3781275153160095,
          0.378645122051239,
          0.3788301646709442,
          0.37890803813934326,
          0.37849798798561096,
          0.37778955698013306,
          0.37713953852653503,
          0.3766421675682068,
          0.3762786090373993,
          0.37611445784568787,
          0.3760324716567993,
          0.3759661018848419,
          0.3759230077266693,
          0.37596023082733154,
          0.3764258921146393,
          0.3765326738357544,
          0.37666499614715576,
          0.37646201252937317,
          0.37615537643432617,
          0.37599727511405945,
          0.3759734630584717,
          0.37598973512649536,
          0.3760615587234497,
          0.37618571519851685,
          0.3761877417564392,
          0.37607550621032715,
          0.3760731518268585,
          0.37614861130714417,
          0.3761023283004761,
          0.37597349286079407,
          0.37589308619499207,
          0.37584006786346436,
          0.375808984041214,
          0.3758086860179901,
          0.37581321597099304,
          0.3757994472980499,
          0.3758232891559601,
          0.37581783533096313,
          0.37580952048301697,
          0.37577059864997864,
          0.37569618225097656,
          0.3756442964076996,
          0.37562933564186096,
          0.37562745809555054,
          0.3756067156791687,
          0.3755623698234558,
          0.3755376636981964,
          0.37550923228263855,
          0.37543705105781555,
          0.3753770887851715,
          0.3753329813480377,
          0.37530189752578735,
          0.37528014183044434,
          0.3752925992012024,
          0.37527385354042053,
          0.3752303719520569,
          0.3751513659954071,
          0.37513357400894165,
          0.37507814168930054,
          0.3749569356441498,
          0.3749134838581085,
          0.3748858869075775,
          0.374882310628891,
          0.3748871386051178,
          0.3748685121536255,
          0.3748420178890228,
          0.3747785985469818,
          0.3746874928474426,
          0.37459486722946167,
          0.3744824230670929,
          0.3743903636932373,
          0.3743477761745453,
          0.3742247521877289,
          0.37395280599594116,
          0.374021977186203,
          0.37384068965911865,
          0.3738430440425873,
          0.37396788597106934,
          0.3738110363483429,
          0.37386029958724976,
          0.37380534410476685,
          0.37365663051605225,
          0.3735349476337433,
          0.3734623193740845,
          0.37334147095680237,
          0.3732651472091675,
          0.373201459646225,
          0.3731429874897003,
          0.3730594515800476,
          0.3729517459869385,
          0.3729330897331238,
          0.37289759516716003,
          0.3729269504547119,
          0.3729495704174042,
          0.37291979789733887,
          0.3728506565093994,
          0.3728328049182892,
          0.3728552758693695,
          0.3728691041469574,
          0.3728927671909332,
          0.37288832664489746,
          0.3728434443473816,
          0.37276288866996765
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_8",
         "type": "scatter",
         "y": [
          0.3851528763771057,
          0.3842458128929138,
          0.38363751769065857,
          0.3834458589553833,
          0.3829495310783386,
          0.3824211657047272,
          0.38315439224243164,
          0.3846145570278168,
          0.38433343172073364,
          0.38380753993988037,
          0.38376346230506897,
          0.3833923935890198,
          0.38325193524360657,
          0.3829314410686493,
          0.38241735100746155,
          0.3823292851448059,
          0.3826458752155304,
          0.3826183080673218,
          0.3824895918369293,
          0.3825013041496277,
          0.3823697865009308,
          0.38224759697914124,
          0.38213586807250977,
          0.3820212185382843,
          0.381974995136261,
          0.381945937871933,
          0.381953626871109,
          0.38197776675224304,
          0.3820059895515442,
          0.3821558952331543,
          0.38233768939971924,
          0.3823467791080475,
          0.3822856843471527,
          0.38224419951438904,
          0.38227495551109314,
          0.3824051022529602,
          0.38244956731796265,
          0.38241055607795715,
          0.38251498341560364,
          0.3826359212398529,
          0.38264280557632446,
          0.38263022899627686,
          0.38257473707199097,
          0.38253894448280334,
          0.3825885057449341,
          0.3826828598976135,
          0.3829200565814972,
          0.3831494152545929,
          0.38300007581710815,
          0.38283270597457886,
          0.3830015957355499,
          0.3831053376197815,
          0.382960706949234,
          0.38268718123435974,
          0.38245782256126404,
          0.38243699073791504,
          0.3824334442615509,
          0.3823510706424713,
          0.3823662996292114,
          0.38238826394081116,
          0.38232600688934326,
          0.3822794258594513,
          0.3822549283504486,
          0.3822643756866455,
          0.382338285446167,
          0.38244786858558655,
          0.3827172517776489,
          0.38322702050209045,
          0.3835090398788452,
          0.3832740783691406,
          0.3829212784767151,
          0.38278451561927795,
          0.38288673758506775,
          0.3830866515636444,
          0.3831203579902649,
          0.3831274211406708,
          0.38345882296562195,
          0.3837345838546753,
          0.3836592733860016,
          0.3837358355522156,
          0.3839632570743561,
          0.3838879466056824,
          0.3835391700267792,
          0.3830805718898773,
          0.3830128014087677,
          0.3843022882938385,
          0.3869684934616089,
          0.3897469639778137,
          0.39132630825042725,
          0.39189377427101135,
          0.39330339431762695,
          0.395828515291214,
          0.39688578248023987,
          0.395641952753067,
          0.39432185888290405,
          0.3938942551612854,
          0.3934815227985382,
          0.39257344603538513,
          0.3916594386100769,
          0.3920852839946747,
          0.3936157822608948,
          0.39385998249053955,
          0.39235085248947144,
          0.3909980356693268,
          0.3907127380371094,
          0.3911193609237671,
          0.3921273648738861,
          0.3933233916759491,
          0.39325836300849915,
          0.39214423298835754,
          0.3919815719127655,
          0.3924739956855774,
          0.3920237720012665,
          0.391065388917923,
          0.39041468501091003,
          0.3899611830711365,
          0.3896646201610565,
          0.389320433139801,
          0.3887246251106262,
          0.3881852328777313,
          0.3876820206642151,
          0.38674095273017883,
          0.3854708671569824,
          0.38456961512565613,
          0.38423290848731995,
          0.383948415517807,
          0.38353079557418823,
          0.3832801580429077,
          0.3831426203250885,
          0.3829030692577362,
          0.3827066421508789,
          0.3826178014278412,
          0.3824922442436218,
          0.3824083209037781,
          0.3825378715991974,
          0.38279756903648376,
          0.38291698694229126,
          0.38281598687171936,
          0.38274696469306946,
          0.3828454315662384,
          0.382890909910202,
          0.3828069567680359,
          0.382829874753952,
          0.38299036026000977,
          0.3830914795398712,
          0.3831433951854706,
          0.3832862973213196,
          0.3834801912307739,
          0.38362425565719604,
          0.38376885652542114,
          0.3839988708496094,
          0.3843727707862854,
          0.38501426577568054,
          0.38588953018188477,
          0.3866158723831177,
          0.3869316875934601,
          0.3870234191417694,
          0.38710281252861023,
          0.3870845139026642,
          0.38687777519226074,
          0.3865826725959778,
          0.38641780614852905,
          0.38656365871429443,
          0.38687559962272644,
          0.387010782957077,
          0.3869427740573883,
          0.3868241012096405,
          0.3865480422973633,
          0.3860291540622711,
          0.3855591118335724,
          0.38536497950553894,
          0.38527196645736694,
          0.38506102561950684,
          0.38477152585983276,
          0.38451868295669556,
          0.3843388855457306,
          0.3841798007488251,
          0.3842066824436188,
          0.3851834535598755,
          0.3877401351928711,
          0.3910927176475525,
          0.3934352397918701,
          0.3938046097755432,
          0.3927105963230133,
          0.39099738001823425,
          0.3891471028327942,
          0.38749921321868896,
          0.38633453845977783,
          0.385684609413147,
          0.38540157675743103,
          0.38528743386268616,
          0.3851810097694397,
          0.385020911693573,
          0.3849744200706482,
          0.38526681065559387,
          0.38577890396118164,
          0.3860461115837097,
          0.38586366176605225,
          0.38552284240722656,
          0.3853529691696167,
          0.38534054160118103,
          0.38539251685142517,
          0.38550621271133423,
          0.38564586639404297,
          0.3856903314590454,
          0.3856411576271057,
          0.38560616970062256,
          0.3856123387813568,
          0.38555118441581726,
          0.38536182045936584,
          0.3851209282875061,
          0.3849334716796875,
          0.3848462998867035,
          0.3848206102848053,
          0.3847964107990265,
          0.3847739100456238,
          0.3847644627094269,
          0.3847306966781616,
          0.38461488485336304,
          0.38440588116645813,
          0.3841722011566162,
          0.38398653268814087,
          0.3838668167591095,
          0.3837627172470093,
          0.3836240768432617,
          0.38345685601234436,
          0.3833126425743103,
          0.3832016885280609,
          0.38308653235435486,
          0.38295769691467285,
          0.38283711671829224,
          0.3827541768550873,
          0.3827192783355713,
          0.3827112317085266,
          0.38268333673477173,
          0.382615327835083,
          0.3825405240058899,
          0.3824934661388397,
          0.3824586570262909,
          0.3824042081832886,
          0.3823506236076355,
          0.3823421597480774,
          0.3823646306991577,
          0.3823617398738861,
          0.3823196291923523,
          0.3822712004184723,
          0.38222864270210266,
          0.3821774423122406,
          0.38212379813194275,
          0.38208699226379395,
          0.38206595182418823,
          0.3820488750934601,
          0.3820358216762543,
          0.3820261061191559,
          0.3820122182369232,
          0.3819983899593353,
          0.3819996118545532,
          0.3820156753063202,
          0.38202613592147827,
          0.3820164203643799,
          0.38199329376220703,
          0.3819720149040222,
          0.38195863366127014,
          0.38194969296455383,
          0.3819398283958435,
          0.3819273114204407,
          0.3819134533405304,
          0.3818981349468231,
          0.38188090920448303,
          0.381865918636322,
          0.381862074136734,
          0.3818713426589966,
          0.38188377022743225,
          0.3818872272968292,
          0.38188016414642334,
          0.381870836019516,
          0.38186800479888916,
          0.3818737268447876,
          0.3818826675415039,
          0.38188543915748596,
          0.3818769156932831,
          0.3818609118461609,
          0.38184550404548645
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_8",
         "type": "scatter",
         "y": [
          0.3939974009990692,
          0.39613252878189087,
          0.39602261781692505,
          0.39597606658935547,
          0.39596426486968994,
          0.39561381936073303,
          0.3958132266998291,
          0.39594435691833496,
          0.39582860469818115,
          0.39558613300323486,
          0.39559003710746765,
          0.3954613506793976,
          0.3953824043273926,
          0.395327627658844,
          0.39468666911125183,
          0.39440780878067017,
          0.39433416724205017,
          0.3941839039325714,
          0.3941148519515991,
          0.3941926956176758,
          0.3941258192062378,
          0.39414921402931213,
          0.3939305543899536,
          0.3939165472984314,
          0.3939751386642456,
          0.39396506547927856,
          0.3939826190471649,
          0.39410459995269775,
          0.3943153917789459,
          0.394843190908432,
          0.39548176527023315,
          0.39551687240600586,
          0.39540350437164307,
          0.3951888382434845,
          0.3951878547668457,
          0.3954804539680481,
          0.39550721645355225,
          0.3954923152923584,
          0.39568600058555603,
          0.3958292305469513,
          0.3959704637527466,
          0.3959871828556061,
          0.395969033241272,
          0.3958367705345154,
          0.39598986506462097,
          0.396076500415802,
          0.3962897062301636,
          0.3963095545768738,
          0.39625784754753113,
          0.39617571234703064,
          0.39625319838523865,
          0.39627471566200256,
          0.3962678611278534,
          0.39606040716171265,
          0.3958960771560669,
          0.3958282470703125,
          0.395746111869812,
          0.3957899212837219,
          0.395985871553421,
          0.39597436785697937,
          0.395926833152771,
          0.3958764374256134,
          0.3957656919956207,
          0.3956300914287567,
          0.3956010043621063,
          0.3958260118961334,
          0.39593929052352905,
          0.3961906135082245,
          0.3962576985359192,
          0.39608386158943176,
          0.3959101438522339,
          0.39574775099754333,
          0.395779013633728,
          0.3958384096622467,
          0.39591240882873535,
          0.395975798368454,
          0.39612966775894165,
          0.3961395025253296,
          0.3960128724575043,
          0.3959653675556183,
          0.3960738480091095,
          0.3960839509963989,
          0.3959982395172119,
          0.39572033286094666,
          0.395919531583786,
          0.3962278664112091,
          0.3965676426887512,
          0.39713725447654724,
          0.39717379212379456,
          0.39717113971710205,
          0.3972913324832916,
          0.39748841524124146,
          0.3976668119430542,
          0.3975847065448761,
          0.3972461521625519,
          0.39722687005996704,
          0.3973689675331116,
          0.39741069078445435,
          0.3972904086112976,
          0.3972521126270294,
          0.3975503742694855,
          0.39783453941345215,
          0.397870272397995,
          0.39756616950035095,
          0.39718732237815857,
          0.39707863330841064,
          0.39737072587013245,
          0.39778077602386475,
          0.39788007736206055,
          0.3974800705909729,
          0.39714688062667847,
          0.39714834094047546,
          0.39714300632476807,
          0.3970377743244171,
          0.3969901204109192,
          0.39697474241256714,
          0.3970041275024414,
          0.39705389738082886,
          0.3971196711063385,
          0.3972209393978119,
          0.3973492980003357,
          0.39727675914764404,
          0.39699265360832214,
          0.3967837989330292,
          0.39686429500579834,
          0.3971346914768219,
          0.3970261812210083,
          0.39679744839668274,
          0.39673057198524475,
          0.3965968191623688,
          0.39660805463790894,
          0.3966085612773895,
          0.39655348658561707,
          0.3965586721897125,
          0.39670297503471375,
          0.396935373544693,
          0.3970252275466919,
          0.3971899449825287,
          0.3973090946674347,
          0.39732012152671814,
          0.3973337411880493,
          0.39721253514289856,
          0.39734530448913574,
          0.39782068133354187,
          0.39758196473121643,
          0.39739444851875305,
          0.39742618799209595,
          0.39749038219451904,
          0.3973410427570343,
          0.39734604954719543,
          0.39735037088394165,
          0.3974063992500305,
          0.397713303565979,
          0.3983045518398285,
          0.3985035717487335,
          0.39861923456192017,
          0.39866170287132263,
          0.39883142709732056,
          0.3988483250141144,
          0.398793488740921,
          0.3984996974468231,
          0.3983738124370575,
          0.39845070242881775,
          0.39859139919281006,
          0.39856699109077454,
          0.3983117341995239,
          0.3981192708015442,
          0.3980887532234192,
          0.3980039954185486,
          0.3977731466293335,
          0.3974219858646393,
          0.39738285541534424,
          0.39732614159584045,
          0.3972768187522888,
          0.3972628712654114,
          0.3971637189388275,
          0.3971271216869354,
          0.3971605598926544,
          0.39768779277801514,
          0.39862754940986633,
          0.3994768261909485,
          0.399994432926178,
          0.4001794755458832,
          0.4002573490142822,
          0.3998472988605499,
          0.399138867855072,
          0.398488849401474,
          0.39799147844314575,
          0.39762791991233826,
          0.39746376872062683,
          0.3973817825317383,
          0.3973154127597809,
          0.3972723186016083,
          0.3973095417022705,
          0.39777520298957825,
          0.39788198471069336,
          0.3980143070220947,
          0.39781132340431213,
          0.39750468730926514,
          0.3973465859889984,
          0.39732277393341064,
          0.3973390460014343,
          0.39741086959838867,
          0.3975350260734558,
          0.3975370526313782,
          0.3974248170852661,
          0.3974224627017975,
          0.39749792218208313,
          0.39745163917541504,
          0.39732280373573303,
          0.39724239706993103,
          0.3971893787384033,
          0.39715829491615295,
          0.3971579968929291,
          0.397162526845932,
          0.3971487581729889,
          0.39717260003089905,
          0.3971671462059021,
          0.39715883135795593,
          0.3971199095249176,
          0.3970454931259155,
          0.39699360728263855,
          0.3969786465167999,
          0.3969767689704895,
          0.39695602655410767,
          0.3969116806983948,
          0.3968869745731354,
          0.3968585431575775,
          0.3967863619327545,
          0.3967263996601105,
          0.3966822922229767,
          0.3966512084007263,
          0.3966294527053833,
          0.39664191007614136,
          0.3966231644153595,
          0.39657968282699585,
          0.39650067687034607,
          0.3964828848838806,
          0.3964274525642395,
          0.39630624651908875,
          0.3962627947330475,
          0.3962351977825165,
          0.39623162150382996,
          0.39623644948005676,
          0.39621782302856445,
          0.3961913287639618,
          0.3961279094219208,
          0.3960368037223816,
          0.39594417810440063,
          0.39583173394203186,
          0.39573967456817627,
          0.39569708704948425,
          0.39557406306266785,
          0.3953021168708801,
          0.39537128806114197,
          0.3951900005340576,
          0.39519235491752625,
          0.3953171968460083,
          0.39516034722328186,
          0.3952096104621887,
          0.3951546549797058,
          0.3950059413909912,
          0.39488425850868225,
          0.39481163024902344,
          0.39469078183174133,
          0.39461445808410645,
          0.39455077052116394,
          0.3944922983646393,
          0.3944087624549866,
          0.39430105686187744,
          0.39428240060806274,
          0.394246906042099,
          0.3942762613296509,
          0.39429888129234314,
          0.39426910877227783,
          0.3941999673843384,
          0.39418211579322815,
          0.39420458674430847,
          0.39421841502189636,
          0.3942420780658722,
          0.3942376375198364,
          0.39419275522232056,
          0.3941121995449066
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_9",
         "type": "scatter",
         "y": [
          0.46809113025665283,
          0.469072550535202,
          0.468841552734375,
          0.4684377610683441,
          0.4686603546142578,
          0.4684560298919678,
          0.4682142734527588,
          0.4682237505912781,
          0.46811360120773315,
          0.4680692255496979,
          0.4680609703063965,
          0.46799877285957336,
          0.46799275279045105,
          0.46797680854797363,
          0.46794402599334717,
          0.4679313004016876,
          0.46792498230934143,
          0.46792474389076233,
          0.4679303765296936,
          0.46792662143707275,
          0.4679158627986908,
          0.46792465448379517,
          0.46792706847190857,
          0.46792712807655334,
          0.46793508529663086,
          0.46792900562286377,
          0.46791982650756836,
          0.4679192900657654,
          0.4679228663444519,
          0.46794378757476807,
          0.46797460317611694,
          0.4679831862449646,
          0.46797940135002136,
          0.46797409653663635,
          0.46796971559524536,
          0.4679796099662781,
          0.4679926335811615,
          0.4679899513721466,
          0.4680030345916748,
          0.4680338501930237,
          0.46802809834480286,
          0.4680515229701996,
          0.46818187832832336,
          0.4682452976703644,
          0.4681728482246399,
          0.46814092993736267,
          0.46819525957107544,
          0.46822255849838257,
          0.46816927194595337,
          0.4681547284126282,
          0.46819302439689636,
          0.4682782292366028,
          0.4683270752429962,
          0.46824702620506287,
          0.4682146906852722,
          0.4681949317455292,
          0.468107670545578,
          0.468104749917984,
          0.46815839409828186,
          0.4681236743927002,
          0.4681120812892914,
          0.46819832921028137,
          0.46825510263442993,
          0.46824607253074646,
          0.4682554602622986,
          0.4683900773525238,
          0.4686955213546753,
          0.4690733850002289,
          0.4692026376724243,
          0.46897462010383606,
          0.4686555862426758,
          0.4684574604034424,
          0.4685361087322235,
          0.46879008412361145,
          0.46883895993232727,
          0.46878787875175476,
          0.46894288063049316,
          0.46905314922332764,
          0.4689903259277344,
          0.46901464462280273,
          0.469107985496521,
          0.4691566526889801,
          0.4690815806388855,
          0.4688478410243988,
          0.4687040448188782,
          0.46890440583229065,
          0.4692510664463043,
          0.4693637192249298,
          0.4692997336387634,
          0.46930810809135437,
          0.46939411759376526,
          0.46961259841918945,
          0.4698171317577362,
          0.46958446502685547,
          0.46911126375198364,
          0.4689593017101288,
          0.46913978457450867,
          0.46924901008605957,
          0.4690905511379242,
          0.4690244495868683,
          0.4694317877292633,
          0.4699782133102417,
          0.4700399339199066,
          0.4695270359516144,
          0.4689396619796753,
          0.4688090682029724,
          0.469311386346817,
          0.4700596034526825,
          0.4701972007751465,
          0.46958300471305847,
          0.46906137466430664,
          0.46895483136177063,
          0.46890705823898315,
          0.46881672739982605,
          0.4687027037143707,
          0.4686022102832794,
          0.4686613380908966,
          0.46883177757263184,
          0.46897825598716736,
          0.4691312313079834,
          0.46930059790611267,
          0.4693143665790558,
          0.4691278040409088,
          0.46906107664108276,
          0.4692612886428833,
          0.46934518218040466,
          0.46917659044265747,
          0.4691038727760315,
          0.46913981437683105,
          0.46907034516334534,
          0.4689846932888031,
          0.468894362449646,
          0.46869608759880066,
          0.46860313415527344,
          0.4688361883163452,
          0.46921807527542114,
          0.46941784024238586,
          0.46934351325035095,
          0.46920502185821533,
          0.46915382146835327,
          0.46907269954681396,
          0.46894535422325134,
          0.46892765164375305,
          0.468914270401001,
          0.46873563528060913,
          0.46858224272727966,
          0.46860843896865845,
          0.4686107039451599,
          0.4684866666793823,
          0.46841946244239807,
          0.46846696734428406,
          0.4685247242450714,
          0.46861764788627625,
          0.46879756450653076,
          0.46896809339523315,
          0.46903538703918457,
          0.46904733777046204,
          0.46908771991729736,
          0.4691368341445923,
          0.46911367774009705,
          0.46898502111434937,
          0.4688488841056824,
          0.46886470913887024,
          0.4689989686012268,
          0.46902260184288025,
          0.4688684940338135,
          0.46871358156204224,
          0.46867209672927856,
          0.46867579221725464,
          0.46865302324295044,
          0.46860384941101074,
          0.4685405194759369,
          0.46849796175956726,
          0.4685128331184387,
          0.4685515761375427,
          0.4685494303703308,
          0.4685171842575073,
          0.46854522824287415,
          0.4687464237213135,
          0.46917611360549927,
          0.4697358310222626,
          0.4701707363128662,
          0.4702637195587158,
          0.47002506256103516,
          0.4696265459060669,
          0.46923115849494934,
          0.46891656517982483,
          0.4686989486217499,
          0.4685845375061035,
          0.46854865550994873,
          0.46852749586105347,
          0.46847453713417053,
          0.46844199299812317,
          0.46854326128959656,
          0.4687789976596832,
          0.4689744710922241,
          0.46896615624427795,
          0.4687941074371338,
          0.4686116874217987,
          0.46849364042282104,
          0.4684401750564575,
          0.46846067905426025,
          0.46853676438331604,
          0.46858981251716614,
          0.4685797095298767,
          0.4685531258583069,
          0.46856215596199036,
          0.4685990512371063,
          0.4686090052127838,
          0.46855437755584717,
          0.4684600234031677,
          0.4683894217014313,
          0.4683762788772583,
          0.46838364005088806,
          0.46837106347084045,
          0.4683506190776825,
          0.46834737062454224,
          0.4683469235897064,
          0.4683202803134918,
          0.4682694971561432,
          0.46822184324264526,
          0.46819180250167847,
          0.46817266941070557,
          0.4681524932384491,
          0.46813100576400757,
          0.4681142568588257,
          0.46810320019721985,
          0.4680904746055603,
          0.4680725038051605,
          0.4680553674697876,
          0.4680439233779907,
          0.4680406451225281,
          0.46804705262184143,
          0.4680558145046234,
          0.4680524170398712,
          0.4680335521697998,
          0.46801361441612244,
          0.46800392866134644,
          0.467998743057251,
          0.4679902493953705,
          0.46798476576805115,
          0.46799078583717346,
          0.46800172328948975,
          0.46800360083580017,
          0.46799278259277344,
          0.46797749400138855,
          0.46796509623527527,
          0.46795767545700073,
          0.4679553508758545,
          0.46795663237571716,
          0.4679594337940216,
          0.4679633677005768,
          0.4679676294326782,
          0.46796727180480957,
          0.4679591655731201,
          0.467949241399765,
          0.4679453670978546,
          0.4679470658302307,
          0.46794846653938293,
          0.46794748306274414,
          0.4679456055164337,
          0.4679431915283203,
          0.46794065833091736,
          0.4679407477378845,
          0.4679439961910248,
          0.46794620156288147,
          0.4679434895515442,
          0.4679374396800995,
          0.4679323136806488,
          0.46792957186698914,
          0.4679281711578369,
          0.46792739629745483,
          0.46792688965797424,
          0.4679260551929474,
          0.4679248034954071,
          0.4679236114025116,
          0.46792271733283997,
          0.46792399883270264,
          0.46793031692504883,
          0.46794021129608154,
          0.4679463505744934,
          0.4679427444934845,
          0.4679323732852936
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_9",
         "type": "scatter",
         "y": [
          0.48130401968955994,
          0.4834391474723816,
          0.48332923650741577,
          0.4832826852798462,
          0.48327088356018066,
          0.48292043805122375,
          0.4831198453903198,
          0.4832509756088257,
          0.4831352233886719,
          0.4828927516937256,
          0.4828966557979584,
          0.4827679693698883,
          0.4826890230178833,
          0.4826342463493347,
          0.48199328780174255,
          0.4817144274711609,
          0.4816407859325409,
          0.48149052262306213,
          0.48142147064208984,
          0.4814993143081665,
          0.4814324378967285,
          0.48145583271980286,
          0.48123717308044434,
          0.4812231659889221,
          0.48128175735473633,
          0.4812716841697693,
          0.48128923773765564,
          0.4814112186431885,
          0.48162201046943665,
          0.48214980959892273,
          0.4827883839607239,
          0.4828234910964966,
          0.4827101230621338,
          0.4824954569339752,
          0.4824944734573364,
          0.4827870726585388,
          0.48281383514404297,
          0.4827989339828491,
          0.48299261927604675,
          0.483135849237442,
          0.4832770824432373,
          0.4832938015460968,
          0.4832756519317627,
          0.4831433892250061,
          0.4832964837551117,
          0.4833831191062927,
          0.4835963249206543,
          0.4836161732673645,
          0.48356446623802185,
          0.48348233103752136,
          0.48355981707572937,
          0.4835813343524933,
          0.4835744798183441,
          0.48336702585220337,
          0.4832026958465576,
          0.4831348657608032,
          0.48305273056030273,
          0.48309653997421265,
          0.48329249024391174,
          0.4832809865474701,
          0.4832334518432617,
          0.4831830561161041,
          0.48307231068611145,
          0.48293671011924744,
          0.48290762305259705,
          0.48313263058662415,
          0.4832459092140198,
          0.4834972321987152,
          0.4835643172264099,
          0.4833904802799225,
          0.4832167625427246,
          0.48305436968803406,
          0.48308563232421875,
          0.4831450283527374,
          0.4832190275192261,
          0.4832824170589447,
          0.4834362864494324,
          0.4834461212158203,
          0.483319491147995,
          0.483271986246109,
          0.4833804666996002,
          0.48339056968688965,
          0.48330485820770264,
          0.4830269515514374,
          0.48322615027427673,
          0.48353448510169983,
          0.48387426137924194,
          0.48444387316703796,
          0.4844804108142853,
          0.4844777584075928,
          0.48459795117378235,
          0.4847950339317322,
          0.4849734306335449,
          0.4848913252353668,
          0.4845527708530426,
          0.48453348875045776,
          0.4846755862236023,
          0.48471730947494507,
          0.48459702730178833,
          0.48455873131752014,
          0.4848569929599762,
          0.48514115810394287,
          0.4851768910884857,
          0.4848727881908417,
          0.4844939410686493,
          0.48438525199890137,
          0.48467734456062317,
          0.48508739471435547,
          0.48518669605255127,
          0.4847866892814636,
          0.4844534993171692,
          0.4844549596309662,
          0.4844496250152588,
          0.48434439301490784,
          0.4842967391014099,
          0.48428136110305786,
          0.48431074619293213,
          0.4843605160713196,
          0.4844262897968292,
          0.4845275580883026,
          0.4846559166908264,
          0.48458337783813477,
          0.48429927229881287,
          0.4840904176235199,
          0.48417091369628906,
          0.4844413101673126,
          0.484332799911499,
          0.48410406708717346,
          0.4840371906757355,
          0.4839034378528595,
          0.48391467332839966,
          0.48391517996788025,
          0.4838601052761078,
          0.48386529088020325,
          0.48400959372520447,
          0.4842419922351837,
          0.4843318462371826,
          0.4844965636730194,
          0.4846157133579254,
          0.48462674021720886,
          0.48464035987854004,
          0.4845191538333893,
          0.48465192317962646,
          0.4851273000240326,
          0.48488858342170715,
          0.4847010672092438,
          0.48473280668258667,
          0.48479700088500977,
          0.484647661447525,
          0.48465266823768616,
          0.4846569895744324,
          0.48471301794052124,
          0.4850199222564697,
          0.4856111705303192,
          0.48581019043922424,
          0.4859258532524109,
          0.48596832156181335,
          0.4861380457878113,
          0.4861549437046051,
          0.48610010743141174,
          0.48580631613731384,
          0.4856804311275482,
          0.48575732111930847,
          0.4858980178833008,
          0.48587360978126526,
          0.48561835289001465,
          0.4854258894920349,
          0.4853953719139099,
          0.4853106141090393,
          0.4850797653198242,
          0.48472860455513,
          0.48468947410583496,
          0.4846327602863312,
          0.48458343744277954,
          0.4845694899559021,
          0.48447033762931824,
          0.48443374037742615,
          0.48446717858314514,
          0.48499441146850586,
          0.48593416810035706,
          0.4867834448814392,
          0.4873010516166687,
          0.4874860942363739,
          0.48756396770477295,
          0.48715391755104065,
          0.48644548654556274,
          0.4857954680919647,
          0.4852980971336365,
          0.484934538602829,
          0.48477038741111755,
          0.484688401222229,
          0.4846220314502716,
          0.484578937292099,
          0.48461616039276123,
          0.48508182168006897,
          0.4851886034011841,
          0.48532092571258545,
          0.48511794209480286,
          0.48481130599975586,
          0.48465320467948914,
          0.48462939262390137,
          0.48464566469192505,
          0.4847174882888794,
          0.48484164476394653,
          0.4848436713218689,
          0.48473143577575684,
          0.4847290813922882,
          0.48480454087257385,
          0.48475825786590576,
          0.48462942242622375,
          0.48454901576042175,
          0.48449599742889404,
          0.4844649136066437,
          0.4844646155834198,
          0.48446914553642273,
          0.4844553768634796,
          0.48447921872138977,
          0.4844737648963928,
          0.48446545004844666,
          0.4844265282154083,
          0.48435211181640625,
          0.4843002259731293,
          0.48428526520729065,
          0.4842833876609802,
          0.4842626452445984,
          0.4842182993888855,
          0.4841935932636261,
          0.48416516184806824,
          0.48409298062324524,
          0.4840330183506012,
          0.4839889109134674,
          0.48395782709121704,
          0.483936071395874,
          0.4839485287666321,
          0.4839297831058502,
          0.4838863015174866,
          0.4838072955608368,
          0.48378950357437134,
          0.4837340712547302,
          0.48361286520957947,
          0.4835694134235382,
          0.4835418164730072,
          0.4835382401943207,
          0.4835430681705475,
          0.4835244417190552,
          0.4834979474544525,
          0.4834345281124115,
          0.4833434224128723,
          0.48325079679489136,
          0.4831383526325226,
          0.483046293258667,
          0.483003705739975,
          0.48288068175315857,
          0.48260873556137085,
          0.4826779067516327,
          0.48249661922454834,
          0.48249897360801697,
          0.482623815536499,
          0.4824669659137726,
          0.48251622915267944,
          0.48246127367019653,
          0.48231256008148193,
          0.482190877199173,
          0.48211824893951416,
          0.48199740052223206,
          0.48192107677459717,
          0.48185738921165466,
          0.48179891705513,
          0.4817153811454773,
          0.48160767555236816,
          0.48158901929855347,
          0.4815535247325897,
          0.4815828800201416,
          0.48160549998283386,
          0.48157572746276855,
          0.4815065860748291,
          0.48148873448371887,
          0.4815112054347992,
          0.4815250337123871,
          0.4815486967563629,
          0.48154425621032715,
          0.4814993739128113,
          0.48141881823539734
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calcStats(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
      "mae 7.452522223492814e-05 mse 1.1778906054688656e-08 mean sigma 0.007988296\n",
      "scaled: mae 0.009325035493734157 mse 0.0001844163344383012 mean sigma 0.9995427814800002\n",
      "0.999999980038264 27094.65 -9612305000000.0 191878.29130813776\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_0",
         "type": "scatter",
         "y": [
          0.15595178306102753,
          0.15536810457706451,
          0.15463262796401978,
          0.15383365750312805,
          0.153988778591156,
          0.1534208208322525,
          0.15296940505504608,
          0.15306049585342407,
          0.15280182659626007,
          0.15244485437870026,
          0.15223942697048187,
          0.15205790102481842,
          0.15205588936805725,
          0.1520853489637375,
          0.15209902822971344,
          0.15216811001300812,
          0.15233607590198517,
          0.1525908261537552,
          0.1528645157814026,
          0.15320681035518646,
          0.15370775759220123,
          0.15434284508228302,
          0.15478010475635529,
          0.15504491329193115,
          0.1554863005876541,
          0.15588173270225525,
          0.15639682114124298,
          0.15706060826778412,
          0.15773624181747437,
          0.15909023582935333,
          0.16041508316993713,
          0.16051802039146423,
          0.16009528934955597,
          0.15989595651626587,
          0.16011565923690796,
          0.16066022217273712,
          0.1609080582857132,
          0.16089393198490143,
          0.1612028330564499,
          0.16161347925662994,
          0.16181036829948425,
          0.1618039757013321,
          0.16143754124641418,
          0.16109971702098846,
          0.16130277514457703,
          0.16182999312877655,
          0.16244745254516602,
          0.1628476083278656,
          0.16254669427871704,
          0.16221003234386444,
          0.16257376968860626,
          0.16288278996944427,
          0.16243037581443787,
          0.16146430373191833,
          0.16067980229854584,
          0.16054347157478333,
          0.1605697125196457,
          0.16039930284023285,
          0.16046462953090668,
          0.16055284440517426,
          0.16023880243301392,
          0.15979859232902527,
          0.15941716730594635,
          0.15915250778198242,
          0.15904168784618378,
          0.15898796916007996,
          0.15902800858020782,
          0.15915286540985107,
          0.15907074511051178,
          0.15870808064937592,
          0.15830767154693604,
          0.15806281566619873,
          0.1580759584903717,
          0.15823130309581757,
          0.1582803875207901,
          0.15836496651172638,
          0.15870068967342377,
          0.15889692306518555,
          0.15868481993675232,
          0.15847520530223846,
          0.15847010910511017,
          0.1584394872188568,
          0.15812765061855316,
          0.15752874314785004,
          0.15712107717990875,
          0.15733402967453003,
          0.15784479677677155,
          0.1581542044878006,
          0.15830448269844055,
          0.15844720602035522,
          0.15861469507217407,
          0.1589244306087494,
          0.15903809666633606,
          0.15852090716362,
          0.15781380236148834,
          0.15746912360191345,
          0.15748630464076996,
          0.15753139555454254,
          0.15726712346076965,
          0.15720604360103607,
          0.15795962512493134,
          0.15882007777690887,
          0.15881416201591492,
          0.15796315670013428,
          0.15707267820835114,
          0.15700455009937286,
          0.15792828798294067,
          0.15898281335830688,
          0.15894952416419983,
          0.15795089304447174,
          0.15726889669895172,
          0.15719272196292877,
          0.15713785588741302,
          0.15700359642505646,
          0.15684819221496582,
          0.15667439997196198,
          0.1566309630870819,
          0.15670543909072876,
          0.15680421888828278,
          0.15695086121559143,
          0.15710920095443726,
          0.1570681780576706,
          0.15675953030586243,
          0.15655723214149475,
          0.15674300491809845,
          0.1569211483001709,
          0.15679040551185608,
          0.15669198334217072,
          0.1567191183567047,
          0.1566489338874817,
          0.1566103994846344,
          0.15665210783481598,
          0.15665407478809357,
          0.1568603366613388,
          0.15743444859981537,
          0.15802805125713348,
          0.15834058821201324,
          0.1585676521062851,
          0.15903058648109436,
          0.15962208807468414,
          0.16002801060676575,
          0.16033849120140076,
          0.16083450615406036,
          0.16141164302825928,
          0.16180704534053802,
          0.16202014684677124,
          0.1622452437877655,
          0.16261887550354004,
          0.16308049857616425,
          0.1634989082813263,
          0.16389299929141998,
          0.16444815695285797,
          0.16530683636665344,
          0.16627918183803558,
          0.16689763963222504,
          0.16702237725257874,
          0.16701528429985046,
          0.1671152114868164,
          0.16712629795074463,
          0.16687552630901337,
          0.16651450097560883,
          0.16635404527187347,
          0.1665576994419098,
          0.16689221560955048,
          0.1669701188802719,
          0.16680580377578735,
          0.16665007174015045,
          0.16645649075508118,
          0.16608069837093353,
          0.16573737561702728,
          0.16566026210784912,
          0.16566795110702515,
          0.16548192501068115,
          0.16514438390731812,
          0.164809450507164,
          0.16449379920959473,
          0.16421619057655334,
          0.16424724459648132,
          0.16509771347045898,
          0.16700933873653412,
          0.16941477358341217,
          0.1712513118982315,
          0.1719057410955429,
          0.17156395316123962,
          0.17067009210586548,
          0.16951808333396912,
          0.1683170646429062,
          0.1672697216272354,
          0.1664925515651703,
          0.16598106920719147,
          0.16563405096530914,
          0.16533270478248596,
          0.16505053639411926,
          0.16494104266166687,
          0.16515861451625824,
          0.1655859798192978,
          0.16589553654193878,
          0.16594372689723969,
          0.16586849093437195,
          0.16579918563365936,
          0.16573788225650787,
          0.16569826006889343,
          0.16567611694335938,
          0.1656116545200348,
          0.16550306975841522,
          0.16547921299934387,
          0.16561461985111237,
          0.16578954458236694,
          0.16581115126609802,
          0.1656349152326584,
          0.16538432240486145,
          0.16518747806549072,
          0.16509629786014557,
          0.165078803896904,
          0.16511118412017822,
          0.16520178318023682,
          0.16529184579849243,
          0.16527564823627472,
          0.16511544585227966,
          0.16486845910549164,
          0.16461600363254547,
          0.16439884901046753,
          0.1642332226037979,
          0.16409803926944733,
          0.16393810510635376,
          0.16372783482074738,
          0.16350531578063965,
          0.1632992923259735,
          0.1630956530570984,
          0.16288802027702332,
          0.16269131004810333,
          0.16254951059818268,
          0.16249097883701324,
          0.16246189177036285,
          0.16235096752643585,
          0.1621357500553131,
          0.16192759573459625,
          0.16180305182933807,
          0.16168098151683807,
          0.16146571934223175,
          0.16122692823410034,
          0.16109541058540344,
          0.1610671430826187,
          0.16103146970272064,
          0.1609237790107727,
          0.16074921190738678,
          0.16052290797233582,
          0.160281702876091,
          0.16009056568145752,
          0.1599581390619278,
          0.1598338931798935,
          0.15970294177532196,
          0.15959416329860687,
          0.15949547290802002,
          0.1593761295080185,
          0.15928372740745544,
          0.15930092334747314,
          0.159409761428833,
          0.1594734936952591,
          0.1593906283378601,
          0.15920615196228027,
          0.15903551876544952,
          0.15892408788204193,
          0.158832848072052,
          0.1587231457233429,
          0.1586071103811264,
          0.1585056483745575,
          0.15839044749736786,
          0.15822210907936096,
          0.15803897380828857,
          0.1579563170671463,
          0.15802441537380219,
          0.1581462174654007,
          0.1581750214099884,
          0.15807394683361053,
          0.15794113278388977,
          0.1579003930091858,
          0.15797960758209229,
          0.15808914601802826,
          0.1581067591905594,
          0.15799492597579956,
          0.1578218936920166,
          0.15766845643520355
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_0",
         "type": "scatter",
         "y": [
          0.1553405374288559,
          0.15747563540935516,
          0.15736573934555054,
          0.15731918811798096,
          0.15730738639831543,
          0.15695695579051971,
          0.1571563333272934,
          0.15728747844696045,
          0.15717172622680664,
          0.15692925453186035,
          0.15693315863609314,
          0.15680447220802307,
          0.15672552585601807,
          0.15667074918746948,
          0.15602979063987732,
          0.15575093030929565,
          0.15567727386951447,
          0.1555270254611969,
          0.1554579734802246,
          0.15553581714630127,
          0.15546894073486328,
          0.15549233555793762,
          0.1552736759185791,
          0.1552596539258957,
          0.1553182601928711,
          0.15530818700790405,
          0.1553257554769516,
          0.15544772148132324,
          0.1556585133075714,
          0.1561863124370575,
          0.15682488679885864,
          0.15685999393463135,
          0.15674661099910736,
          0.15653195977210999,
          0.15653099119663239,
          0.1568235605955124,
          0.15685033798217773,
          0.1568354368209839,
          0.15702912211418152,
          0.15717235207557678,
          0.15731358528137207,
          0.15733030438423157,
          0.15731215476989746,
          0.15717989206314087,
          0.15733297169208527,
          0.1574196219444275,
          0.15763282775878906,
          0.15765267610549927,
          0.15760096907615662,
          0.15751883387565613,
          0.15759631991386414,
          0.15761785209178925,
          0.1576109677553177,
          0.15740351378917694,
          0.1572391837835312,
          0.15717138350009918,
          0.1570892184972763,
          0.1571330577135086,
          0.1573290079832077,
          0.15731748938560486,
          0.15726996958255768,
          0.1572195589542389,
          0.15710881352424622,
          0.1569732129573822,
          0.1569441258907318,
          0.1571691334247589,
          0.15728241205215454,
          0.15753374993801117,
          0.15760082006454468,
          0.15742698311805725,
          0.15725326538085938,
          0.15709087252616882,
          0.15712212026119232,
          0.1571815311908722,
          0.15725553035736084,
          0.15731893479824066,
          0.15747278928756714,
          0.15748262405395508,
          0.15735600888729095,
          0.15730847418308258,
          0.15741698443889618,
          0.15742707252502441,
          0.1573413759469986,
          0.15706345438957214,
          0.1572626531124115,
          0.1575709730386734,
          0.1579107791185379,
          0.15848036110401154,
          0.15851691365242004,
          0.15851426124572754,
          0.1586344689130783,
          0.15883155167102814,
          0.1590099185705185,
          0.15892784297466278,
          0.15858928859233856,
          0.15856999158859253,
          0.15871208906173706,
          0.15875382721424103,
          0.1586335301399231,
          0.1585952341556549,
          0.15889348089694977,
          0.15917764604091644,
          0.15921339392662048,
          0.15890929102897644,
          0.15853044390678406,
          0.15842175483703613,
          0.15871386229991913,
          0.15912389755249023,
          0.15922318398952484,
          0.1588231921195984,
          0.15849001705646515,
          0.15849146246910095,
          0.15848612785339355,
          0.1583808809518814,
          0.15833324193954468,
          0.15831786394119263,
          0.1583472490310669,
          0.15839701890945435,
          0.1584627777338028,
          0.15856406092643738,
          0.1586924046278,
          0.15861989557743073,
          0.15833579003810883,
          0.15812690556049347,
          0.15820740163326263,
          0.1584778130054474,
          0.1583693027496338,
          0.15814058482646942,
          0.15807367861270905,
          0.15793994069099426,
          0.15795117616653442,
          0.15795168280601501,
          0.15789662301540375,
          0.157901793718338,
          0.15804609656333923,
          0.1582784801721573,
          0.15836834907531738,
          0.15853306651115417,
          0.15865223109722137,
          0.15866324305534363,
          0.1586768478155136,
          0.15855565667152405,
          0.15868841111660004,
          0.15916378796100616,
          0.1589251011610031,
          0.15873758494853973,
          0.15876930952072144,
          0.15883351862430573,
          0.1586841642856598,
          0.15868917107582092,
          0.15869350731372833,
          0.158749520778656,
          0.1590564250946045,
          0.15964767336845398,
          0.1598467081785202,
          0.15996235609054565,
          0.16000482439994812,
          0.16017454862594604,
          0.16019144654273987,
          0.1601366102695465,
          0.1598428189754486,
          0.15971693396568298,
          0.15979382395744324,
          0.15993453562259674,
          0.15991011261940002,
          0.1596548706293106,
          0.15946240723133087,
          0.15943187475204468,
          0.15934710204601288,
          0.1591162532567978,
          0.15876510739326477,
          0.15872596204280853,
          0.15866926312446594,
          0.1586199551820755,
          0.15860599279403687,
          0.1585068255662918,
          0.1584702581167221,
          0.1585036963224411,
          0.15903089940547943,
          0.15997067093849182,
          0.16081994771957397,
          0.16133753955364227,
          0.16152258217334747,
          0.16160045564174652,
          0.16119042038917542,
          0.16048197448253632,
          0.1598319560289383,
          0.15933458507061005,
          0.15897105634212494,
          0.15880689024925232,
          0.15872488915920258,
          0.15865854918956757,
          0.15861544013023376,
          0.1586526483297348,
          0.15911833941936493,
          0.15922510623931885,
          0.15935741364955902,
          0.15915445983409882,
          0.15884780883789062,
          0.1586897075176239,
          0.15866589546203613,
          0.158682182431221,
          0.15875400602817535,
          0.1588781625032425,
          0.15888015925884247,
          0.1587679535150528,
          0.15876558423042297,
          0.1588410586118698,
          0.15879476070404053,
          0.15866592526435852,
          0.15858550369739532,
          0.15853248536586761,
          0.15850141644477844,
          0.15850111842155457,
          0.1585056632757187,
          0.15849189460277557,
          0.15851572155952454,
          0.1585102677345276,
          0.15850196778774261,
          0.15846304595470428,
          0.15838861465454102,
          0.15833672881126404,
          0.15832176804542542,
          0.15831990540027618,
          0.15829914808273315,
          0.15825480222702026,
          0.15823008120059967,
          0.158201664686203,
          0.15812948346138,
          0.15806952118873596,
          0.15802539885044098,
          0.1579943299293518,
          0.1579725742340088,
          0.15798501670360565,
          0.15796628594398499,
          0.15792281925678253,
          0.15784378349781036,
          0.1578260064125061,
          0.15777058899402618,
          0.15764935314655304,
          0.15760590136051178,
          0.15757831931114197,
          0.15757475793361664,
          0.15757957100868225,
          0.15756094455718994,
          0.15753446519374847,
          0.15747101604938507,
          0.15737992525100708,
          0.15728728473186493,
          0.15717485547065735,
          0.15708281099796295,
          0.15704020857810974,
          0.15691716969013214,
          0.15664522349834442,
          0.15671439468860626,
          0.1565331220626831,
          0.15653546154499054,
          0.1566603183746338,
          0.15650348365306854,
          0.1565527468919754,
          0.1564977765083313,
          0.1563490778207779,
          0.15622738003730774,
          0.15615475177764893,
          0.15603390336036682,
          0.15595757961273193,
          0.15589389204978943,
          0.15583541989326477,
          0.15575188398361206,
          0.15564417839050293,
          0.15562550723552704,
          0.15559004247188568,
          0.15561938285827637,
          0.15564198791980743,
          0.15561223030090332,
          0.15554308891296387,
          0.15552523732185364,
          0.15554770827293396,
          0.15556153655052185,
          0.15558519959449768,
          0.15558075904846191,
          0.15553586184978485,
          0.1554553210735321
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_1",
         "type": "scatter",
         "y": [
          0.05368368700146675,
          0.0546589270234108,
          0.05452866479754448,
          0.05433350428938866,
          0.05464249476790428,
          0.05445045977830887,
          0.054283373057842255,
          0.054371703416109085,
          0.05420922115445137,
          0.05413564294576645,
          0.05410357564687729,
          0.054129935801029205,
          0.05421315133571625,
          0.054215531796216965,
          0.054059505462646484,
          0.05387132614850998,
          0.053905412554740906,
          0.053873155266046524,
          0.05373208224773407,
          0.05366751179099083,
          0.05363850295543671,
          0.053698815405368805,
          0.0536997951567173,
          0.05362796038389206,
          0.05361316725611687,
          0.05359222739934921,
          0.05355587229132652,
          0.05354149639606476,
          0.05355432629585266,
          0.053611189126968384,
          0.05368990823626518,
          0.05369363725185394,
          0.05367411673069,
          0.05369972810149193,
          0.053722452372312546,
          0.05373044312000275,
          0.05373551324009895,
          0.05371242016553879,
          0.05372924730181694,
          0.05379451811313629,
          0.05380561947822571,
          0.053834930062294006,
          0.05392954871058464,
          0.05396657809615135,
          0.053949739784002304,
          0.05399250611662865,
          0.054057247936725616,
          0.05404231324791908,
          0.053980614989995956,
          0.053962286561727524,
          0.0540182925760746,
          0.05413173511624336,
          0.05418166518211365,
          0.05415833368897438,
          0.05415479093790054,
          0.054121002554893494,
          0.05408990755677223,
          0.05422070994973183,
          0.05437813326716423,
          0.054317865520715714,
          0.05426555871963501,
          0.05439688265323639,
          0.0545087568461895,
          0.05448417738080025,
          0.05448250472545624,
          0.05468066781759262,
          0.055034492164850235,
          0.055398669093847275,
          0.05554785951972008,
          0.055391766130924225,
          0.05510326474905014,
          0.054885756224393845,
          0.05488269031047821,
          0.0550403818488121,
          0.0551595576107502,
          0.05527390539646149,
          0.055419523268938065,
          0.055348120629787445,
          0.05510696768760681,
          0.0549912229180336,
          0.05499221384525299,
          0.05502753704786301,
          0.054990414530038834,
          0.05485852435231209,
          0.05483512952923775,
          0.05495119467377663,
          0.055037740617990494,
          0.05504269897937775,
          0.05505155399441719,
          0.055081047117710114,
          0.05504269152879715,
          0.05501508712768555,
          0.055085282772779465,
          0.055096786469221115,
          0.05501978099346161,
          0.05505168437957764,
          0.05525493621826172,
          0.05540717765688896,
          0.05534401535987854,
          0.055317264050245285,
          0.05554267764091492,
          0.05581299215555191,
          0.0558621920645237,
          0.05562359839677811,
          0.05528651177883148,
          0.05518509820103645,
          0.05544441193342209,
          0.05581297352910042,
          0.05583979934453964,
          0.05547459051012993,
          0.05521722882986069,
          0.05527562275528908,
          0.05531634762883186,
          0.05520745366811752,
          0.05513456463813782,
          0.05517042055726051,
          0.055254049599170685,
          0.05534989759325981,
          0.0554439015686512,
          0.0555562898516655,
          0.055710677057504654,
          0.055773526430130005,
          0.05563236027956009,
          0.055513329803943634,
          0.05564353987574577,
          0.05579950287938118,
          0.05574106052517891,
          0.05560097098350525,
          0.05551760271191597,
          0.05552519112825394,
          0.055655933916568756,
          0.055712420493364334,
          0.055522482842206955,
          0.05535305291414261,
          0.05550876259803772,
          0.055857326835393906,
          0.05603235960006714,
          0.05590995028614998,
          0.055741943418979645,
          0.055710114538669586,
          0.05567217245697975,
          0.055634330958127975,
          0.05577046051621437,
          0.055899351835250854,
          0.055791884660720825,
          0.05565476417541504,
          0.05566906929016113,
          0.05566876381635666,
          0.055566947907209396,
          0.05549703910946846,
          0.055508144199848175,
          0.05554566904902458,
          0.0556110143661499,
          0.055704157799482346,
          0.05577829107642174,
          0.0558183379471302,
          0.05584273859858513,
          0.05585704743862152,
          0.05583536997437477,
          0.05575979873538017,
          0.05566681548953056,
          0.055628903210163116,
          0.05565706267952919,
          0.05566704645752907,
          0.05559752136468887,
          0.055507078766822815,
          0.055474720895290375,
          0.05548194423317909,
          0.0554608590900898,
          0.0553918331861496,
          0.055305201560258865,
          0.055238865315914154,
          0.05521426349878311,
          0.055225223302841187,
          0.055228475481271744,
          0.05518452450633049,
          0.05512763559818268,
          0.05514928326010704,
          0.05529536306858063,
          0.05552339926362038,
          0.05575939267873764,
          0.05595505237579346,
          0.056092843413352966,
          0.05616772919893265,
          0.05617164075374603,
          0.05609775707125664,
          0.05594725161790848,
          0.05574379116296768,
          0.05553978681564331,
          0.05538027733564377,
          0.05527518689632416,
          0.055205557495355606,
          0.055157728493213654,
          0.05514553189277649,
          0.055179011076688766,
          0.05522426962852478,
          0.0552261583507061,
          0.05517694354057312,
          0.05512019619345665,
          0.055086251348257065,
          0.055079661309719086,
          0.05510702729225159,
          0.05515030771493912,
          0.05515080317854881,
          0.05509044602513313,
          0.055028315633535385,
          0.055015433579683304,
          0.055032599717378616,
          0.055033471435308456,
          0.05500829219818115,
          0.05498326197266579,
          0.05498028174042702,
          0.05499471724033356,
          0.05499975383281708,
          0.05498231574892998,
          0.05495799705386162,
          0.05494535714387894,
          0.05494192987680435,
          0.05493545904755592,
          0.05492451414465904,
          0.054915040731430054,
          0.054904814809560776,
          0.054883912205696106,
          0.05484788119792938,
          0.05480852350592613,
          0.05477998033165932,
          0.05475916340947151,
          0.05473052337765694,
          0.05469067767262459,
          0.05464992672204971,
          0.05461849644780159,
          0.05460655689239502,
          0.0546179935336113,
          0.05463007837533951,
          0.05461034178733826,
          0.05456189811229706,
          0.054516348987817764,
          0.05448261275887489,
          0.05444324389100075,
          0.05439550057053566,
          0.054359033703804016,
          0.05433856323361397,
          0.05431948974728584,
          0.05429750308394432,
          0.05427638068795204,
          0.05424800142645836,
          0.05419987440109253,
          0.05414142459630966,
          0.05409722775220871,
          0.054077617824077606,
          0.05407565459609032,
          0.054080065339803696,
          0.0540756992995739,
          0.05404222011566162,
          0.053978510200977325,
          0.05392076447606087,
          0.05390431731939316,
          0.05392031371593475,
          0.05392972752451897,
          0.0539124496281147,
          0.053883783519268036,
          0.05386460945010185,
          0.05385680869221687,
          0.05384848266839981,
          0.05383322015404701,
          0.053819239139556885,
          0.05381835252046585,
          0.053828831762075424,
          0.05383260175585747,
          0.05381203442811966,
          0.0537705235183239,
          0.05372807756066322,
          0.0536995530128479,
          0.053686242550611496,
          0.05368362367153168,
          0.05368538200855255,
          0.05368124693632126,
          0.05366666615009308,
          0.053652022033929825,
          0.05364961177110672,
          0.05365394428372383,
          0.05364555865526199,
          0.053619351238012314
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_1",
         "type": "scatter",
         "y": [
          0.045748256146907806,
          0.04788336157798767,
          0.04777345433831215,
          0.047726914286613464,
          0.04771510884165764,
          0.047364670783281326,
          0.047564055770635605,
          0.04769519716501236,
          0.04757944867014885,
          0.04733697324991226,
          0.04734088107943535,
          0.04721219465136528,
          0.04713324084877968,
          0.04707847163081169,
          0.04643751308321953,
          0.046158649027347565,
          0.04608500003814697,
          0.04593474417924881,
          0.04586569964885712,
          0.04594353586435318,
          0.04587665945291519,
          0.04590006545186043,
          0.04568139836192131,
          0.0456673838198185,
          0.045725978910923004,
          0.04571590572595596,
          0.04573347792029381,
          0.04585544392466545,
          0.04606623947620392,
          0.046594034880399704,
          0.04723260924220085,
          0.047267716377973557,
          0.04715433344244957,
          0.046939678490161896,
          0.046938709914684296,
          0.0472312830388546,
          0.047258056700229645,
          0.0472431518137455,
          0.04743684455752373,
          0.04758008196949959,
          0.047721315175294876,
          0.047738030552864075,
          0.04771986976265907,
          0.04758761078119278,
          0.04774070158600807,
          0.0478273406624794,
          0.04804054647684097,
          0.04806039482355118,
          0.04800868779420853,
          0.04792655259370804,
          0.048004038631916046,
          0.048025574535131454,
          0.0480186901986599,
          0.04781123995780945,
          0.047646909952163696,
          0.04757910594344139,
          0.047496940940618515,
          0.047540776431560516,
          0.047736722975969315,
          0.047725219279527664,
          0.04767768457531929,
          0.0476272813975811,
          0.047516535967588425,
          0.04738093912601471,
          0.04735184460878372,
          0.04757685214281082,
          0.04769014194607735,
          0.04794146865606308,
          0.04800853878259659,
          0.04783471301198006,
          0.04766099527478218,
          0.047498591244220734,
          0.04752985015511513,
          0.0475892499089241,
          0.04766324907541275,
          0.04772665351629257,
          0.047880515456199646,
          0.04789034649729729,
          0.04776373133063316,
          0.04771620035171509,
          0.04782470688223839,
          0.04783479869365692,
          0.047749098390340805,
          0.04747117683291435,
          0.04767037555575371,
          0.04797869920730591,
          0.04831849783658981,
          0.04888808727264404,
          0.04892463982105255,
          0.048921987414360046,
          0.04904218763113022,
          0.04923926666378975,
          0.049417644739151,
          0.04933556541800499,
          0.048997003585100174,
          0.04897771030664444,
          0.04911981150507927,
          0.04916154965758324,
          0.0490412563085556,
          0.04900296404957771,
          0.049301210790872574,
          0.04958537220954895,
          0.04962112009525299,
          0.04931701719760895,
          0.048938170075416565,
          0.04882947728037834,
          0.04912157729268074,
          0.04953162372112274,
          0.04963090643286705,
          0.049230918288230896,
          0.04889773949980736,
          0.04889918118715286,
          0.04889385402202606,
          0.048788607120513916,
          0.048740968108177185,
          0.04872557893395424,
          0.0487549751996994,
          0.048804741352796555,
          0.0488705039024353,
          0.04897177964448929,
          0.049100130796432495,
          0.04902761057019234,
          0.04874350503087044,
          0.04853463172912598,
          0.04861513152718544,
          0.048885539174079895,
          0.048777028918266296,
          0.04854830354452133,
          0.04848140478134155,
          0.048347655683755875,
          0.04835889860987663,
          0.048359401524066925,
          0.04830434173345566,
          0.04830951616168022,
          0.04845382645726204,
          0.048686202615499496,
          0.04877607151865959,
          0.048940785229206085,
          0.049059949815273285,
          0.04907096549868584,
          0.04908457770943642,
          0.048963386565446854,
          0.049096137285232544,
          0.04957151040434837,
          0.04933282360434532,
          0.049145303666591644,
          0.04917703941464424,
          0.049241237342357635,
          0.0490918830037117,
          0.04909689724445343,
          0.049101222306489944,
          0.04915724694728851,
          0.0494641438126564,
          0.05005538836121559,
          0.05025443062186241,
          0.050370074808597565,
          0.050412554293870926,
          0.050582271069288254,
          0.05059916526079178,
          0.050544336438179016,
          0.05025053396821022,
          0.05012466013431549,
          0.050201546400785446,
          0.05034225061535835,
          0.05031784251332283,
          0.05006258934736252,
          0.049870122224092484,
          0.04983959719538689,
          0.049754828214645386,
          0.04952397570014,
          0.049172837287187576,
          0.04913368448615074,
          0.04907698556780815,
          0.04902767390012741,
          0.049013715237379074,
          0.04891454800963402,
          0.04887797683477402,
          0.04891141504049301,
          0.04943862557411194,
          0.05037838965654373,
          0.051227670162916183,
          0.05174526199698448,
          0.05193030834197998,
          0.05200818181037903,
          0.051598139107227325,
          0.05088970437645912,
          0.0502396821975708,
          0.049742311239242554,
          0.04937877506017685,
          0.04921461641788483,
          0.04913261905312538,
          0.049066267907619476,
          0.04902316629886627,
          0.04906037449836731,
          0.04952605441212654,
          0.049632828682661057,
          0.04976513609290123,
          0.049562178552150726,
          0.04925552383065224,
          0.04909742996096611,
          0.04907362163066864,
          0.04908990114927292,
          0.049161721020936966,
          0.0492858812212944,
          0.049287889152765274,
          0.049175672233104706,
          0.049173302948474884,
          0.04924877732992172,
          0.04920249059796333,
          0.04907364398241043,
          0.04899323359131813,
          0.04894021153450012,
          0.04890913888812065,
          0.04890883341431618,
          0.0489133819937706,
          0.048899609595537186,
          0.04892343655228615,
          0.048917993903160095,
          0.048909686505794525,
          0.048870764672756195,
          0.048796337097883224,
          0.048744454979896545,
          0.048729490488767624,
          0.04872762784361839,
          0.04870687425136566,
          0.04866253212094307,
          0.048637811094522476,
          0.04860939458012581,
          0.048537202179431915,
          0.04847723990678787,
          0.048433125019073486,
          0.04840204864740372,
          0.0483802892267704,
          0.04839274287223816,
          0.048374008387327194,
          0.048330534249544144,
          0.04825150966644287,
          0.04823372885584831,
          0.04817831143736839,
          0.048057083040475845,
          0.04801362380385399,
          0.04798603802919388,
          0.04798248037695885,
          0.04798728972673416,
          0.04796865954995155,
          0.047942183911800385,
          0.04787873849272728,
          0.04778764396905899,
          0.04769500717520714,
          0.047582581639289856,
          0.04749053344130516,
          0.04744793102145195,
          0.04732489585876465,
          0.04705295339226723,
          0.04712212085723877,
          0.046940840780735016,
          0.04694318771362305,
          0.047068044543266296,
          0.04691120609641075,
          0.04696046933531761,
          0.04690549895167351,
          0.0467567965388298,
          0.04663509875535965,
          0.04656247794628143,
          0.04644163325428963,
          0.04636530205607414,
          0.04630161449313164,
          0.04624314233660698,
          0.04615960270166397,
          0.04605190083384514,
          0.046033233404159546,
          0.04599776118993759,
          0.04602709785103798,
          0.04604971036314964,
          0.04601994901895523,
          0.045950815081596375,
          0.045932963490486145,
          0.04595543071627617,
          0.04596925899386406,
          0.04599291831254959,
          0.045988477766513824,
          0.04594358429312706,
          0.045863039791584015
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_2",
         "type": "scatter",
         "y": [
          0.1981814205646515,
          0.20235976576805115,
          0.2020731121301651,
          0.20127850770950317,
          0.20162588357925415,
          0.19942401349544525,
          0.1999703347682953,
          0.20313715934753418,
          0.20274995267391205,
          0.20162679255008698,
          0.20141424238681793,
          0.20232725143432617,
          0.20417429506778717,
          0.20439720153808594,
          0.2029413878917694,
          0.2016831338405609,
          0.20119591057300568,
          0.2001296877861023,
          0.198940247297287,
          0.19839385151863098,
          0.19743765890598297,
          0.19674403965473175,
          0.19610051810741425,
          0.19514745473861694,
          0.19424381852149963,
          0.19353576004505157,
          0.19358618557453156,
          0.19393806159496307,
          0.19388306140899658,
          0.19372445344924927,
          0.19387111067771912,
          0.19357748329639435,
          0.19300620257854462,
          0.19269685447216034,
          0.19257666170597076,
          0.19255881011486053,
          0.19248580932617188,
          0.19231818616390228,
          0.19256998598575592,
          0.19315865635871887,
          0.19344405829906464,
          0.1936141848564148,
          0.19388379156589508,
          0.19396768510341644,
          0.1942058652639389,
          0.19506020843982697,
          0.19590765237808228,
          0.19594725966453552,
          0.1952839195728302,
          0.19463585317134857,
          0.1947466880083084,
          0.19561390578746796,
          0.19602236151695251,
          0.19569270312786102,
          0.19540835916996002,
          0.19509010016918182,
          0.1949676275253296,
          0.1961602121591568,
          0.19759872555732727,
          0.19729112088680267,
          0.1969653069972992,
          0.19804497063159943,
          0.19889327883720398,
          0.19873054325580597,
          0.19880017638206482,
          0.20004521310329437,
          0.20236249268054962,
          0.20518240332603455,
          0.2065466046333313,
          0.20552612841129303,
          0.20353522896766663,
          0.2019972801208496,
          0.20202191174030304,
          0.2034057378768921,
          0.20428459346294403,
          0.2047846019268036,
          0.20590215921401978,
          0.20573359727859497,
          0.20411083102226257,
          0.20362535119056702,
          0.20403948426246643,
          0.2039080411195755,
          0.20303046703338623,
          0.20209276676177979,
          0.20342132449150085,
          0.2089511603116989,
          0.21663589775562286,
          0.22247372567653656,
          0.22540447115898132,
          0.2273419350385666,
          0.2301085889339447,
          0.23349332809448242,
          0.2354891300201416,
          0.23528170585632324,
          0.23461027443408966,
          0.2346900850534439,
          0.23472999036312103,
          0.23401695489883423,
          0.23336659371852875,
          0.23412230610847473,
          0.23550677299499512,
          0.23522630333900452,
          0.2332172393798828,
          0.23128767311573029,
          0.23035378754138947,
          0.23012249171733856,
          0.23035159707069397,
          0.23094473779201508,
          0.2307899296283722,
          0.22952835261821747,
          0.2288365215063095,
          0.2290475070476532,
          0.22874490916728973,
          0.22783459722995758,
          0.22704030573368073,
          0.22660966217517853,
          0.2266307771205902,
          0.2269216924905777,
          0.22728492319583893,
          0.22774380445480347,
          0.22789503633975983,
          0.2270175963640213,
          0.22533515095710754,
          0.22421392798423767,
          0.22416412830352783,
          0.2239188849925995,
          0.2228303849697113,
          0.22174321115016937,
          0.2207808941602707,
          0.21958792209625244,
          0.21867068111896515,
          0.21788164973258972,
          0.21642138063907623,
          0.21496789157390594,
          0.2148599773645401,
          0.21575166285037994,
          0.21592436730861664,
          0.21477793157100677,
          0.21361394226551056,
          0.213137686252594,
          0.21245761215686798,
          0.21183016896247864,
          0.21236127614974976,
          0.21293127536773682,
          0.21214620769023895,
          0.2112010270357132,
          0.21114251017570496,
          0.21098899841308594,
          0.21030499041080475,
          0.20994892716407776,
          0.21017690002918243,
          0.21062904596328735,
          0.21133099496364594,
          0.21220959722995758,
          0.21282513439655304,
          0.21305182576179504,
          0.21311892569065094,
          0.21313241124153137,
          0.2128734141588211,
          0.21221768856048584,
          0.2114885151386261,
          0.2111741453409195,
          0.21130409836769104,
          0.21136194467544556,
          0.21097958087921143,
          0.21044166386127472,
          0.21012111008167267,
          0.20987507700920105,
          0.20939812064170837,
          0.20875154435634613,
          0.20819298923015594,
          0.20782582461833954,
          0.20759639143943787,
          0.2074708342552185,
          0.20738066732883453,
          0.20721973478794098,
          0.20708714425563812,
          0.2073947787284851,
          0.2086000293493271,
          0.2107950747013092,
          0.2135261595249176,
          0.21602404117584229,
          0.21771438419818878,
          0.2184835821390152,
          0.21844586730003357,
          0.21769511699676514,
          0.21630921959877014,
          0.21448081731796265,
          0.21254262328147888,
          0.21082346141338348,
          0.20950929820537567,
          0.20858386158943176,
          0.20793189108371735,
          0.20755712687969208,
          0.2075381875038147,
          0.20775212347507477,
          0.20786289870738983,
          0.20771944522857666,
          0.20749808847904205,
          0.20736411213874817,
          0.207326740026474,
          0.20741160213947296,
          0.20754843950271606,
          0.20746207237243652,
          0.20705388486385345,
          0.20664143562316895,
          0.20651821792125702,
          0.20657069981098175,
          0.20652548968791962,
          0.20635917782783508,
          0.20626454055309296,
          0.20633335411548615,
          0.20646777749061584,
          0.20648981630802155,
          0.20635156333446503,
          0.20618322491645813,
          0.20611713826656342,
          0.20614409446716309,
          0.20616067945957184,
          0.20610462129116058,
          0.20600436627864838,
          0.2059018462896347,
          0.20577660202980042,
          0.2055477797985077,
          0.20519407093524933,
          0.2048002928495407,
          0.2044413536787033,
          0.20408889651298523,
          0.20370200276374817,
          0.20330922305583954,
          0.20296861231327057,
          0.20275385677814484,
          0.20269790291786194,
          0.2026774138212204,
          0.20248910784721375,
          0.20212115347385406,
          0.20174220204353333,
          0.20140685141086578,
          0.2010067254304886,
          0.20051774382591248,
          0.20008814334869385,
          0.19981196522712708,
          0.19962099194526672,
          0.19942006468772888,
          0.19915995001792908,
          0.19880355894565582,
          0.19833442568778992,
          0.19782570004463196,
          0.19739854335784912,
          0.19710081815719604,
          0.19689200818538666,
          0.19669708609580994,
          0.19643621146678925,
          0.19603893160820007,
          0.19552797079086304,
          0.1950690895318985,
          0.19480891525745392,
          0.19470611214637756,
          0.19458939135074615,
          0.19435523450374603,
          0.19404229521751404,
          0.19372808933258057,
          0.1934421956539154,
          0.1931813806295395,
          0.19294512271881104,
          0.1927482783794403,
          0.19261232018470764,
          0.19253084063529968,
          0.19242873787879944,
          0.192196324467659,
          0.19181875884532928,
          0.19140790402889252,
          0.1910809427499771,
          0.19085924327373505,
          0.19070109724998474,
          0.190566748380661,
          0.1904173344373703,
          0.1902310997247696,
          0.19004666805267334,
          0.18992333114147186,
          0.18983954191207886,
          0.18969081342220306,
          0.1894405633211136
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_2",
         "type": "scatter",
         "y": [
          0.20461513102054596,
          0.20675022900104523,
          0.2066403329372406,
          0.20659378170967102,
          0.2065819799900055,
          0.20623154938220978,
          0.20643092691898346,
          0.2065620720386505,
          0.2064463198184967,
          0.20620384812355042,
          0.2062077522277832,
          0.20607906579971313,
          0.20600011944770813,
          0.20594534277915955,
          0.20530438423156738,
          0.20502552390098572,
          0.20495186746120453,
          0.20480161905288696,
          0.20473256707191467,
          0.20481041073799133,
          0.20474353432655334,
          0.20476692914962769,
          0.20454826951026917,
          0.20453424751758575,
          0.20459285378456116,
          0.20458278059959412,
          0.20460034906864166,
          0.2047223150730133,
          0.20493310689926147,
          0.20546090602874756,
          0.2060994803905487,
          0.2061345875263214,
          0.20602120459079742,
          0.20580655336380005,
          0.20580558478832245,
          0.20609815418720245,
          0.2061249315738678,
          0.20611003041267395,
          0.20630371570587158,
          0.20644694566726685,
          0.20658817887306213,
          0.20660489797592163,
          0.20658674836158752,
          0.20645448565483093,
          0.20660756528377533,
          0.20669421553611755,
          0.20690742135047913,
          0.20692726969718933,
          0.20687556266784668,
          0.2067934274673462,
          0.2068709135055542,
          0.2068924456834793,
          0.20688556134700775,
          0.206678107380867,
          0.20651377737522125,
          0.20644597709178925,
          0.20636381208896637,
          0.20640765130519867,
          0.20660360157489777,
          0.20659208297729492,
          0.20654456317424774,
          0.20649415254592896,
          0.20638340711593628,
          0.20624780654907227,
          0.20621871948242188,
          0.20644372701644897,
          0.2065570056438446,
          0.20680834352970123,
          0.20687541365623474,
          0.20670157670974731,
          0.20652785897254944,
          0.2063654661178589,
          0.20639671385288239,
          0.20645612478256226,
          0.2065301239490509,
          0.20659352838993073,
          0.2067473828792572,
          0.20675721764564514,
          0.20663060247898102,
          0.20658306777477264,
          0.20669157803058624,
          0.20670166611671448,
          0.20661596953868866,
          0.2063380479812622,
          0.20653724670410156,
          0.20684556663036346,
          0.20718537271022797,
          0.2077549546957016,
          0.2077915072441101,
          0.2077888548374176,
          0.20790906250476837,
          0.2081061452627182,
          0.20828451216220856,
          0.20820243656635284,
          0.20786388218402863,
          0.2078445851802826,
          0.20798668265342712,
          0.2080284208059311,
          0.20790812373161316,
          0.20786982774734497,
          0.20816807448863983,
          0.2084522396326065,
          0.20848798751831055,
          0.2081838846206665,
          0.20780503749847412,
          0.2076963484287262,
          0.2079884558916092,
          0.2083984911441803,
          0.2084977775812149,
          0.20809778571128845,
          0.2077646106481552,
          0.20776605606079102,
          0.20776072144508362,
          0.20765547454357147,
          0.20760783553123474,
          0.2075924575328827,
          0.20762184262275696,
          0.2076716125011444,
          0.20773737132549286,
          0.20783865451812744,
          0.20796699821949005,
          0.2078944891691208,
          0.2076103836297989,
          0.20740149915218353,
          0.2074819952249527,
          0.20775240659713745,
          0.20764389634132385,
          0.20741517841815948,
          0.2073482722043991,
          0.20721453428268433,
          0.2072257697582245,
          0.20722627639770508,
          0.2071712166070938,
          0.20717638731002808,
          0.2073206901550293,
          0.20755307376384735,
          0.20764294266700745,
          0.20780766010284424,
          0.20792682468891144,
          0.2079378366470337,
          0.20795144140720367,
          0.2078302502632141,
          0.2079630047082901,
          0.20843838155269623,
          0.20819969475269318,
          0.2080121785402298,
          0.2080439031124115,
          0.2081081122159958,
          0.20795875787734985,
          0.207963764667511,
          0.2079681009054184,
          0.20802411437034607,
          0.20833101868629456,
          0.20892226696014404,
          0.20912130177021027,
          0.20923694968223572,
          0.20927941799163818,
          0.2094491422176361,
          0.20946604013442993,
          0.20941120386123657,
          0.20911741256713867,
          0.20899152755737305,
          0.2090684175491333,
          0.2092091292142868,
          0.2091847062110901,
          0.20892946422100067,
          0.20873700082302094,
          0.20870646834373474,
          0.20862169563770294,
          0.20839084684848785,
          0.20803970098495483,
          0.2080005556344986,
          0.207943856716156,
          0.20789454877376556,
          0.20788058638572693,
          0.20778141915798187,
          0.20774485170841217,
          0.20777828991413116,
          0.2083054929971695,
          0.20924526453018188,
          0.21009454131126404,
          0.21061213314533234,
          0.21079717576503754,
          0.21087504923343658,
          0.21046501398086548,
          0.20975656807422638,
          0.20910654962062836,
          0.2086091786623001,
          0.208245649933815,
          0.20808148384094238,
          0.20799948275089264,
          0.20793314278125763,
          0.20789003372192383,
          0.20792724192142487,
          0.208392933011055,
          0.2084996998310089,
          0.20863200724124908,
          0.20842905342578888,
          0.2081224024295807,
          0.20796430110931396,
          0.2079404890537262,
          0.20795677602291107,
          0.20802859961986542,
          0.20815275609493256,
          0.20815475285053253,
          0.20804254710674286,
          0.20804017782211304,
          0.20811565220355988,
          0.2080693542957306,
          0.20794051885604858,
          0.2078600972890854,
          0.20780707895755768,
          0.2077760100364685,
          0.20777571201324463,
          0.20778025686740875,
          0.20776648819446564,
          0.2077903151512146,
          0.20778486132621765,
          0.20777656137943268,
          0.20773763954639435,
          0.20766320824623108,
          0.2076113224029541,
          0.20759636163711548,
          0.20759449899196625,
          0.20757374167442322,
          0.20752939581871033,
          0.20750467479228973,
          0.20747625827789307,
          0.20740407705307007,
          0.20734411478042603,
          0.20729999244213104,
          0.20726892352104187,
          0.20724716782569885,
          0.20725961029529572,
          0.20724087953567505,
          0.2071974128484726,
          0.20711837708950043,
          0.20710060000419617,
          0.20704518258571625,
          0.2069239467382431,
          0.20688049495220184,
          0.20685291290283203,
          0.2068493515253067,
          0.20685416460037231,
          0.20683553814888,
          0.20680905878543854,
          0.20674560964107513,
          0.20665451884269714,
          0.206561878323555,
          0.2064494490623474,
          0.20635740458965302,
          0.2063148021697998,
          0.2061917632818222,
          0.20591981709003448,
          0.20598898828029633,
          0.20580771565437317,
          0.2058100551366806,
          0.20593491196632385,
          0.2057780772447586,
          0.20582734048366547,
          0.20577237010002136,
          0.20562367141246796,
          0.2055019736289978,
          0.205429345369339,
          0.20530849695205688,
          0.205232173204422,
          0.2051684856414795,
          0.20511001348495483,
          0.20502647757530212,
          0.204918771982193,
          0.2049001008272171,
          0.20486463606357574,
          0.20489397644996643,
          0.2049165815114975,
          0.20488682389259338,
          0.20481768250465393,
          0.2047998309135437,
          0.20482230186462402,
          0.20483613014221191,
          0.20485979318618774,
          0.20485535264015198,
          0.20481045544147491,
          0.20472991466522217
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_3",
         "type": "scatter",
         "y": [
          0.13201463222503662,
          0.13319174945354462,
          0.1330970674753189,
          0.13299979269504547,
          0.13325095176696777,
          0.13311856985092163,
          0.13299517333507538,
          0.1330726444721222,
          0.13296377658843994,
          0.13291272521018982,
          0.13291428983211517,
          0.13297061622142792,
          0.13303808867931366,
          0.13306109607219696,
          0.1329725980758667,
          0.1328357458114624,
          0.13287203013896942,
          0.13286127150058746,
          0.13276594877243042,
          0.13274408876895905,
          0.13274641335010529,
          0.13281182944774628,
          0.13284844160079956,
          0.1328386813402176,
          0.13286654651165009,
          0.13289393484592438,
          0.13292039930820465,
          0.13296329975128174,
          0.1330237239599228,
          0.1331128031015396,
          0.13321514427661896,
          0.13325533270835876,
          0.1332443654537201,
          0.13324062526226044,
          0.13325157761573792,
          0.1332736611366272,
          0.13328979909420013,
          0.13329383730888367,
          0.1333264857530594,
          0.13337922096252441,
          0.13340045511722565,
          0.13341441750526428,
          0.1334381401538849,
          0.13343819975852966,
          0.13344833254814148,
          0.13350851833820343,
          0.13356445729732513,
          0.13356390595436096,
          0.1335190087556839,
          0.1334807574748993,
          0.13349197804927826,
          0.13354341685771942,
          0.13355347514152527,
          0.1335006058216095,
          0.13345736265182495,
          0.1334293633699417,
          0.13340361416339874,
          0.1334444135427475,
          0.13350939750671387,
          0.13347367942333221,
          0.13342493772506714,
          0.13345332443714142,
          0.1334787756204605,
          0.13345031440258026,
          0.13343431055545807,
          0.13351944088935852,
          0.1336957961320877,
          0.13388055562973022,
          0.13394711911678314,
          0.13384893536567688,
          0.133680060505867,
          0.1335553675889969,
          0.13354429602622986,
          0.13361413776874542,
          0.1336742341518402,
          0.13374656438827515,
          0.13383173942565918,
          0.13378319144248962,
          0.13363628089427948,
          0.13356246054172516,
          0.13355614244937897,
          0.13357414305210114,
          0.13355682790279388,
          0.13348174095153809,
          0.1334758698940277,
          0.13358137011528015,
          0.13368669152259827,
          0.13373051583766937,
          0.13375087082386017,
          0.13377396762371063,
          0.13378942012786865,
          0.13384047150611877,
          0.13392117619514465,
          0.13392171263694763,
          0.13386096060276031,
          0.13386699557304382,
          0.13394998013973236,
          0.1339908242225647,
          0.1339409351348877,
          0.13394282758235931,
          0.1340760886669159,
          0.13420210778713226,
          0.1341901421546936,
          0.13404467701911926,
          0.13387952744960785,
          0.13383598625659943,
          0.13396134972572327,
          0.1341453492641449,
          0.13415668904781342,
          0.1339605748653412,
          0.1338285505771637,
          0.1338653266429901,
          0.13387998938560486,
          0.1338110864162445,
          0.13376754522323608,
          0.13378234207630157,
          0.1338215470314026,
          0.1338738203048706,
          0.13393382728099823,
          0.1340038925409317,
          0.13408513367176056,
          0.13410811126232147,
          0.13402608036994934,
          0.1339627057313919,
          0.13402865827083588,
          0.13410554826259613,
          0.13407741487026215,
          0.13401299715042114,
          0.13397595286369324,
          0.13398344814777374,
          0.13405638933181763,
          0.13409721851348877,
          0.13401804864406586,
          0.133949413895607,
          0.13404183089733124,
          0.13422919809818268,
          0.13433462381362915,
          0.13430503010749817,
          0.13426029682159424,
          0.13427846133708954,
          0.13428978621959686,
          0.13429489731788635,
          0.13437551259994507,
          0.13446350395679474,
          0.1344546675682068,
          0.13441574573516846,
          0.13442568480968475,
          0.13444073498249054,
          0.13442815840244293,
          0.134424090385437,
          0.13444364070892334,
          0.13447578251361847,
          0.1345280259847641,
          0.1345926970243454,
          0.13463547825813293,
          0.13464774191379547,
          0.13465029001235962,
          0.13465268909931183,
          0.1346389353275299,
          0.13459958136081696,
          0.13455328345298767,
          0.13453000783920288,
          0.13453619182109833,
          0.13454267382621765,
          0.13452191650867462,
          0.13448834419250488,
          0.13446874916553497,
          0.13445612788200378,
          0.13442833721637726,
          0.1343894600868225,
          0.1343589425086975,
          0.13434109091758728,
          0.1343277543783188,
          0.13431748747825623,
          0.13430969417095184,
          0.13429880142211914,
          0.13429024815559387,
          0.13430950045585632,
          0.13438498973846436,
          0.13452287018299103,
          0.1346958875656128,
          0.13485610485076904,
          0.13496524095535278,
          0.13501375913619995,
          0.13500939309597015,
          0.13496068120002747,
          0.13487470149993896,
          0.13476301729679108,
          0.13464418053627014,
          0.13453707098960876,
          0.13445346057415009,
          0.1343933343887329,
          0.13435086607933044,
          0.13432788848876953,
          0.13432875275611877,
          0.13434363901615143,
          0.13435204327106476,
          0.13434697687625885,
          0.13433803617954254,
          0.1343308538198471,
          0.13432562351226807,
          0.13432680070400238,
          0.13433022797107697,
          0.13431867957115173,
          0.13429026305675507,
          0.13426803052425385,
          0.13426680862903595,
          0.1342736929655075,
          0.13427041471004486,
          0.13425830006599426,
          0.13425129652023315,
          0.13425466418266296,
          0.13426198065280914,
          0.13426326215267181,
          0.1342570185661316,
          0.13425081968307495,
          0.13424992561340332,
          0.13425177335739136,
          0.13425108790397644,
          0.13424672186374664,
          0.13424159586429596,
          0.13423652946949005,
          0.1342279613018036,
          0.13421031832695007,
          0.1341833919286728,
          0.13415475189685822,
          0.13413076102733612,
          0.13410954177379608,
          0.1340864896774292,
          0.13406173884868622,
          0.1340392827987671,
          0.1340256929397583,
          0.13402444124221802,
          0.13402661681175232,
          0.13401645421981812,
          0.13399164378643036,
          0.13396435976028442,
          0.1339401751756668,
          0.13391225039958954,
          0.1338789463043213,
          0.13385143876075745,
          0.1338367760181427,
          0.13382910192012787,
          0.13381996750831604,
          0.1338050812482834,
          0.13378162682056427,
          0.1337483674287796,
          0.13371101021766663,
          0.13367976248264313,
          0.1336592733860016,
          0.13364627957344055,
          0.13363422453403473,
          0.13361623883247375,
          0.13358668982982635,
          0.13354775309562683,
          0.13351339101791382,
          0.13349668681621552,
          0.13349464535713196,
          0.13349077105522156,
          0.13347359001636505,
          0.13344740867614746,
          0.13342323899269104,
          0.133405402302742,
          0.13338974118232727,
          0.13337227702140808,
          0.13335543870925903,
          0.13334430754184723,
          0.13333800435066223,
          0.13332800567150116,
          0.13330668210983276,
          0.13327692449092865,
          0.13324856758117676,
          0.13322784006595612,
          0.1332142949104309,
          0.13320483267307281,
          0.13319610059261322,
          0.13318493962287903,
          0.13317206501960754,
          0.13316193222999573,
          0.13315516710281372,
          0.13314469158649445,
          0.13312312960624695,
          0.1330937296152115
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_3",
         "type": "scatter",
         "y": [
          0.1449214369058609,
          0.14705653488636017,
          0.14694663882255554,
          0.14690008759498596,
          0.14688828587532043,
          0.14653785526752472,
          0.1467372328042984,
          0.14686837792396545,
          0.14675262570381165,
          0.14651015400886536,
          0.14651405811309814,
          0.14638537168502808,
          0.14630642533302307,
          0.1462516486644745,
          0.14561069011688232,
          0.14533182978630066,
          0.14525817334651947,
          0.1451079249382019,
          0.14503887295722961,
          0.14511671662330627,
          0.1450498402118683,
          0.14507323503494263,
          0.1448545753955841,
          0.1448405534029007,
          0.1448991596698761,
          0.14488908648490906,
          0.1449066549539566,
          0.14502862095832825,
          0.14523941278457642,
          0.1457672119140625,
          0.14640578627586365,
          0.14644089341163635,
          0.14632751047611237,
          0.146112859249115,
          0.1461118906736374,
          0.1464044600725174,
          0.14643123745918274,
          0.1464163362979889,
          0.14661002159118652,
          0.1467532515525818,
          0.14689448475837708,
          0.14691120386123657,
          0.14689305424690247,
          0.14676079154014587,
          0.14691387116909027,
          0.1470005214214325,
          0.14721372723579407,
          0.14723357558250427,
          0.14718186855316162,
          0.14709973335266113,
          0.14717721939086914,
          0.14719875156879425,
          0.1471918672323227,
          0.14698441326618195,
          0.1468200832605362,
          0.1467522829771042,
          0.1466701179742813,
          0.1467139571905136,
          0.1469099074602127,
          0.14689838886260986,
          0.14685086905956268,
          0.1468004584312439,
          0.14668971300125122,
          0.1465541124343872,
          0.14652502536773682,
          0.14675003290176392,
          0.14686331152915955,
          0.14711464941501617,
          0.14718171954154968,
          0.14700788259506226,
          0.14683416485786438,
          0.14667177200317383,
          0.14670301973819733,
          0.1467624306678772,
          0.14683642983436584,
          0.14689983427524567,
          0.14705368876457214,
          0.14706352353096008,
          0.14693690836429596,
          0.14688937366008759,
          0.14699788391590118,
          0.14700797200202942,
          0.1469222754240036,
          0.14664435386657715,
          0.1468435525894165,
          0.1471518725156784,
          0.1474916785955429,
          0.14806126058101654,
          0.14809781312942505,
          0.14809516072273254,
          0.1482153683900833,
          0.14841245114803314,
          0.1485908180475235,
          0.14850874245166779,
          0.14817018806934357,
          0.14815089106559753,
          0.14829298853874207,
          0.14833472669124603,
          0.1482144296169281,
          0.1481761336326599,
          0.14847438037395477,
          0.14875854551792145,
          0.1487942934036255,
          0.14849019050598145,
          0.14811134338378906,
          0.14800265431404114,
          0.14829476177692413,
          0.14870479702949524,
          0.14880408346652985,
          0.1484040915966034,
          0.14807091653347015,
          0.14807236194610596,
          0.14806702733039856,
          0.1479617804288864,
          0.14791414141654968,
          0.14789876341819763,
          0.1479281485080719,
          0.14797791838645935,
          0.1480436772108078,
          0.14814496040344238,
          0.148273304104805,
          0.14820079505443573,
          0.14791668951511383,
          0.14770780503749847,
          0.14778830111026764,
          0.1480587124824524,
          0.1479502022266388,
          0.14772148430347443,
          0.14765457808971405,
          0.14752084016799927,
          0.14753207564353943,
          0.14753258228302002,
          0.14747752249240875,
          0.14748269319534302,
          0.14762699604034424,
          0.1478593796491623,
          0.1479492485523224,
          0.14811396598815918,
          0.14823313057422638,
          0.14824414253234863,
          0.14825774729251862,
          0.14813655614852905,
          0.14826931059360504,
          0.14874468743801117,
          0.14850600063800812,
          0.14831848442554474,
          0.14835020899772644,
          0.14841441810131073,
          0.1482650637626648,
          0.14827007055282593,
          0.14827440679073334,
          0.148330420255661,
          0.1486373245716095,
          0.14922857284545898,
          0.1494276076555252,
          0.14954325556755066,
          0.14958572387695312,
          0.14975544810295105,
          0.14977234601974487,
          0.1497175097465515,
          0.1494237184524536,
          0.149297833442688,
          0.14937472343444824,
          0.14951543509960175,
          0.14949101209640503,
          0.1492357701063156,
          0.14904330670833588,
          0.14901277422904968,
          0.14892800152301788,
          0.1486971527338028,
          0.14834600687026978,
          0.14830686151981354,
          0.14825016260147095,
          0.1482008546590805,
          0.14818689227104187,
          0.14808772504329681,
          0.1480511575937271,
          0.1480845957994461,
          0.14861179888248444,
          0.14955157041549683,
          0.15040084719657898,
          0.15091843903064728,
          0.15110348165035248,
          0.15118135511875153,
          0.15077131986618042,
          0.15006287395954132,
          0.1494128555059433,
          0.14891548454761505,
          0.14855195581912994,
          0.14838778972625732,
          0.14830578863620758,
          0.14823944866657257,
          0.14819633960723877,
          0.1482335478067398,
          0.14869923889636993,
          0.14880600571632385,
          0.14893831312656403,
          0.14873535931110382,
          0.14842870831489563,
          0.1482706069946289,
          0.14824679493904114,
          0.148263081908226,
          0.14833490550518036,
          0.1484590619802475,
          0.14846105873584747,
          0.1483488529920578,
          0.14834648370742798,
          0.14842195808887482,
          0.14837566018104553,
          0.14824682474136353,
          0.14816640317440033,
          0.14811338484287262,
          0.14808231592178345,
          0.14808201789855957,
          0.1480865627527237,
          0.14807279407978058,
          0.14809662103652954,
          0.1480911672115326,
          0.14808286726474762,
          0.1480439454317093,
          0.14796951413154602,
          0.14791762828826904,
          0.14790266752243042,
          0.1479008048772812,
          0.14788004755973816,
          0.14783570170402527,
          0.14781098067760468,
          0.147782564163208,
          0.147710382938385,
          0.14765042066574097,
          0.14760629832744598,
          0.1475752294063568,
          0.1475534737110138,
          0.14756591618061066,
          0.14754718542099,
          0.14750371873378754,
          0.14742468297481537,
          0.1474069058895111,
          0.1473514884710312,
          0.14723025262355804,
          0.14718680083751678,
          0.14715921878814697,
          0.14715565741062164,
          0.14716047048568726,
          0.14714184403419495,
          0.14711536467075348,
          0.14705191552639008,
          0.14696082472801208,
          0.14686818420886993,
          0.14675575494766235,
          0.14666371047496796,
          0.14662110805511475,
          0.14649806916713715,
          0.14622612297534943,
          0.14629529416561127,
          0.1461140215396881,
          0.14611636102199554,
          0.1462412178516388,
          0.14608438313007355,
          0.1461336463689804,
          0.1460786759853363,
          0.1459299772977829,
          0.14580827951431274,
          0.14573565125465393,
          0.14561480283737183,
          0.14553847908973694,
          0.14547479152679443,
          0.14541631937026978,
          0.14533278346061707,
          0.14522507786750793,
          0.14520640671253204,
          0.14517094194889069,
          0.14520028233528137,
          0.14522288739681244,
          0.14519312977790833,
          0.14512398838996887,
          0.14510613679885864,
          0.14512860774993896,
          0.14514243602752686,
          0.14516609907150269,
          0.14516165852546692,
          0.14511676132678986,
          0.1450362205505371
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_4",
         "type": "scatter",
         "y": [
          0.25575733184814453,
          0.2563537657260895,
          0.25612902641296387,
          0.2562767267227173,
          0.25607478618621826,
          0.2558785378932953,
          0.25626450777053833,
          0.25693634152412415,
          0.256777822971344,
          0.2565581202507019,
          0.256551057100296,
          0.2563623785972595,
          0.25629547238349915,
          0.25616225600242615,
          0.2559467554092407,
          0.25591856241226196,
          0.25607484579086304,
          0.25606122612953186,
          0.2559894323348999,
          0.2559843957424164,
          0.25592294335365295,
          0.25586771965026855,
          0.2558196485042572,
          0.2557729184627533,
          0.25575000047683716,
          0.25573551654815674,
          0.25573110580444336,
          0.255729079246521,
          0.25572386384010315,
          0.25571921467781067,
          0.25571757555007935,
          0.2557167410850525,
          0.25571590662002563,
          0.255715012550354,
          0.25571465492248535,
          0.2557152509689331,
          0.25571542978286743,
          0.2557150423526764,
          0.2557154595851898,
          0.2557161748409271,
          0.2557162344455719,
          0.2557164132595062,
          0.2557174265384674,
          0.25571781396865845,
          0.2557169795036316,
          0.2557169795036316,
          0.25571975111961365,
          0.2557223439216614,
          0.2557203769683838,
          0.2557183802127838,
          0.2557195723056793,
          0.25572115182876587,
          0.25572115182876587,
          0.25571903586387634,
          0.2557174861431122,
          0.25571706891059875,
          0.2557159960269928,
          0.2557152509689331,
          0.255715548992157,
          0.2557155191898346,
          0.25571537017822266,
          0.25571590662002563,
          0.2557169497013092,
          0.2557189464569092,
          0.25572267174720764,
          0.2557259798049927,
          0.2557375431060791,
          0.25576967000961304,
          0.2557925283908844,
          0.25577861070632935,
          0.2557559907436371,
          0.25575241446495056,
          0.25575974583625793,
          0.25576385855674744,
          0.2557618319988251,
          0.2557600140571594,
          0.2557705044746399,
          0.2557987570762634,
          0.25586241483688354,
          0.25596505403518677,
          0.2560371458530426,
          0.25600913166999817,
          0.255911648273468,
          0.2558252215385437,
          0.25590959191322327,
          0.25644001364707947,
          0.25756174325942993,
          0.2589772641658783,
          0.2599339783191681,
          0.26028257608413696,
          0.2610337436199188,
          0.2623533308506012,
          0.26283127069473267,
          0.26215168833732605,
          0.2615436613559723,
          0.2613999545574188,
          0.26120153069496155,
          0.26072561740875244,
          0.26027897000312805,
          0.26047977805137634,
          0.2610871493816376,
          0.2610219717025757,
          0.26028478145599365,
          0.2597804665565491,
          0.25974082946777344,
          0.2599303424358368,
          0.260338693857193,
          0.2607523798942566,
          0.2606193423271179,
          0.2602391242980957,
          0.2604171633720398,
          0.26072198152542114,
          0.2604209780693054,
          0.2598983943462372,
          0.2595657408237457,
          0.25933876633644104,
          0.25922176241874695,
          0.2590794861316681,
          0.2587195336818695,
          0.25832366943359375,
          0.2579870820045471,
          0.25752583146095276,
          0.25697454810142517,
          0.25657105445861816,
          0.25635629892349243,
          0.2562061846256256,
          0.25608327984809875,
          0.25601521134376526,
          0.25597116351127625,
          0.2559080719947815,
          0.2558451294898987,
          0.2558102607727051,
          0.255791038274765,
          0.25577419996261597,
          0.2557673156261444,
          0.25577083230018616,
          0.25577059388160706,
          0.25576257705688477,
          0.25576072931289673,
          0.25576889514923096,
          0.25576910376548767,
          0.2557538151741028,
          0.2557399272918701,
          0.2557368874549866,
          0.2557363510131836,
          0.2557348310947418,
          0.25573575496673584,
          0.2557365298271179,
          0.25573378801345825,
          0.255730539560318,
          0.2557297646999359,
          0.25573062896728516,
          0.25573691725730896,
          0.2557552754878998,
          0.255779504776001,
          0.2557925879955292,
          0.25578922033309937,
          0.2557791471481323,
          0.25576868653297424,
          0.25575771927833557,
          0.255747526884079,
          0.2557429373264313,
          0.25574955344200134,
          0.25576332211494446,
          0.25577059388160706,
          0.2557685375213623,
          0.25576722621917725,
          0.25576677918434143,
          0.2557588517665863,
          0.2557472884654999,
          0.2557416260242462,
          0.25574061274528503,
          0.255738765001297,
          0.25573647022247314,
          0.25573498010635376,
          0.2557345926761627,
          0.2557348608970642,
          0.25573527812957764,
          0.25575125217437744,
          0.2558055520057678,
          0.2558835446834564,
          0.2559302747249603,
          0.25591543316841125,
          0.2558649778366089,
          0.2558155655860901,
          0.2557791471481323,
          0.2557561695575714,
          0.25574609637260437,
          0.2557430863380432,
          0.2557402551174164,
          0.25573617219924927,
          0.2557319104671478,
          0.2557283341884613,
          0.2557316720485687,
          0.25575000047683716,
          0.255775511264801,
          0.25578510761260986,
          0.25577011704444885,
          0.2557477355003357,
          0.2557354271411896,
          0.2557326555252075,
          0.25573259592056274,
          0.25573283433914185,
          0.2557328939437866,
          0.2557322680950165,
          0.25573283433914185,
          0.2557365596294403,
          0.2557430565357208,
          0.2557476758956909,
          0.25574541091918945,
          0.2557373642921448,
          0.2557300925254822,
          0.25572776794433594,
          0.25572848320007324,
          0.25572916865348816,
          0.2557295858860016,
          0.2557300925254822,
          0.2557299733161926,
          0.25572851300239563,
          0.2557261288166046,
          0.2557241916656494,
          0.25572317838668823,
          0.25572243332862854,
          0.25572142004966736,
          0.2557203471660614,
          0.25571951270103455,
          0.25571897625923157,
          0.2557183504104614,
          0.25571754574775696,
          0.25571686029434204,
          0.2557164132595062,
          0.2557162344455719,
          0.2557162642478943,
          0.2557164430618286,
          0.25571635365486145,
          0.25571590662002563,
          0.25571539998054504,
          0.25571516156196594,
          0.2557149827480316,
          0.2557147145271301,
          0.2557145953178406,
          0.25571489334106445,
          0.25571539998054504,
          0.2557154595851898,
          0.25571492314338684,
          0.2557142376899719,
          0.2557137608528137,
          0.255713552236557,
          0.25571343302726746,
          0.2557133734226227,
          0.2557132840156555,
          0.25571322441101074,
          0.25571322441101074,
          0.25571325421333313,
          0.25571316480636597,
          0.2557130455970764,
          0.25571298599243164,
          0.2557130753993988,
          0.2557131052017212,
          0.2557130455970764,
          0.25571295619010925,
          0.2557128667831421,
          0.2557128667831421,
          0.2557128667831421,
          0.2557128369808197,
          0.25571274757385254,
          0.25571268796920776,
          0.2557125985622406,
          0.2557125389575958,
          0.25571244955062866,
          0.2557124197483063,
          0.25571244955062866,
          0.2557125389575958,
          0.2557125687599182,
          0.2557125687599182,
          0.25571250915527344,
          0.25571247935295105,
          0.25571250915527344,
          0.2557125687599182,
          0.255712628364563,
          0.2557126581668854,
          0.2557125687599182,
          0.25571244955062866
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_4",
         "type": "scatter",
         "y": [
          0.2520767152309418,
          0.2542118430137634,
          0.2541019320487976,
          0.254055380821228,
          0.2540435791015625,
          0.2536931335926056,
          0.25389254093170166,
          0.2540236711502075,
          0.2539079189300537,
          0.2536654472351074,
          0.2536693513393402,
          0.25354066491127014,
          0.25346171855926514,
          0.25340694189071655,
          0.2527659833431244,
          0.2524871230125427,
          0.25241348147392273,
          0.25226321816444397,
          0.2521941661834717,
          0.25227200984954834,
          0.25220513343811035,
          0.2522285282611847,
          0.25200986862182617,
          0.25199586153030396,
          0.25205445289611816,
          0.2520443797111511,
          0.2520619332790375,
          0.2521839141845703,
          0.2523947060108185,
          0.25292250514030457,
          0.2535610795021057,
          0.2535961866378784,
          0.2534828186035156,
          0.25326815247535706,
          0.25326716899871826,
          0.25355976819992065,
          0.2535865306854248,
          0.25357162952423096,
          0.2537653148174286,
          0.25390854477882385,
          0.25404977798461914,
          0.25406649708747864,
          0.25404834747314453,
          0.25391608476638794,
          0.25406917929649353,
          0.25415581464767456,
          0.25436902046203613,
          0.25438886880874634,
          0.2543371617794037,
          0.2542550265789032,
          0.2543325126171112,
          0.2543540298938751,
          0.25434717535972595,
          0.2541397213935852,
          0.25397539138793945,
          0.25390756130218506,
          0.25382542610168457,
          0.2538692355155945,
          0.2540651857852936,
          0.25405368208885193,
          0.25400614738464355,
          0.25395575165748596,
          0.2538450062274933,
          0.2537094056606293,
          0.2536803185939789,
          0.253905326128006,
          0.2540186047554016,
          0.25426992774009705,
          0.25433701276779175,
          0.2541631758213043,
          0.25398945808410645,
          0.2538270652294159,
          0.2538583278656006,
          0.25391772389411926,
          0.2539917230606079,
          0.25405511260032654,
          0.2542089819908142,
          0.25421881675720215,
          0.25409218668937683,
          0.25404468178749084,
          0.25415316224098206,
          0.2541632652282715,
          0.2540775537490845,
          0.2537996470928192,
          0.25399884581565857,
          0.25430718064308167,
          0.2546469569206238,
          0.2552165687084198,
          0.2552531063556671,
          0.2552504539489746,
          0.2553706467151642,
          0.255567729473114,
          0.25574612617492676,
          0.25566402077674866,
          0.25532546639442444,
          0.2553061842918396,
          0.25544828176498413,
          0.2554900050163269,
          0.25536972284317017,
          0.255331426858902,
          0.25562968850135803,
          0.2559138536453247,
          0.25594958662986755,
          0.2556454837322235,
          0.25526663661003113,
          0.2551579475402832,
          0.255450040102005,
          0.2558600902557373,
          0.2559593915939331,
          0.25555938482284546,
          0.255226194858551,
          0.255227655172348,
          0.2552223205566406,
          0.2551170885562897,
          0.25506943464279175,
          0.2550540566444397,
          0.25508344173431396,
          0.2551332116127014,
          0.25519898533821106,
          0.25530025362968445,
          0.25542861223220825,
          0.2553560733795166,
          0.2550719678401947,
          0.25486311316490173,
          0.2549436092376709,
          0.25521400570869446,
          0.25510549545288086,
          0.2548767626285553,
          0.2548098862171173,
          0.25467613339424133,
          0.2546873688697815,
          0.2546878755092621,
          0.2546328008174896,
          0.2546379864215851,
          0.2547822892665863,
          0.25501468777656555,
          0.25510454177856445,
          0.25526925921440125,
          0.25538840889930725,
          0.2553994357585907,
          0.2554130554199219,
          0.2552918493747711,
          0.2554246187210083,
          0.25589999556541443,
          0.255661278963089,
          0.2554737627506256,
          0.2555055022239685,
          0.2555696964263916,
          0.25542035698890686,
          0.255425363779068,
          0.2554296851158142,
          0.2554857134819031,
          0.25579261779785156,
          0.25638386607170105,
          0.2565828859806061,
          0.2566985487937927,
          0.2567410171031952,
          0.2569107413291931,
          0.25692763924598694,
          0.2568728029727936,
          0.2565790116786957,
          0.25645312666893005,
          0.2565300166606903,
          0.2566707134246826,
          0.2566463053226471,
          0.2563910484313965,
          0.25619858503341675,
          0.25616806745529175,
          0.25608330965042114,
          0.25585246086120605,
          0.25550130009651184,
          0.2554621696472168,
          0.255405455827713,
          0.2553561329841614,
          0.25534218549728394,
          0.2552430331707001,
          0.255206435918808,
          0.255239874124527,
          0.2557671070098877,
          0.2567068636417389,
          0.25755614042282104,
          0.25807374715805054,
          0.25825878977775574,
          0.2583366632461548,
          0.2579266130924225,
          0.2572181820869446,
          0.25656816363334656,
          0.2560707926750183,
          0.2557072341442108,
          0.2555430829524994,
          0.25546109676361084,
          0.25539472699165344,
          0.25535163283348083,
          0.25538885593414307,
          0.2558545172214508,
          0.2559612989425659,
          0.2560936212539673,
          0.2558906376361847,
          0.2555840015411377,
          0.25542590022087097,
          0.2554020881652832,
          0.2554183602333069,
          0.25549018383026123,
          0.25561434030532837,
          0.25561636686325073,
          0.25550413131713867,
          0.25550177693367004,
          0.2555772364139557,
          0.2555309534072876,
          0.2554021179676056,
          0.2553217113018036,
          0.2552686929702759,
          0.2552376091480255,
          0.25523731112480164,
          0.25524184107780457,
          0.25522807240486145,
          0.2552519142627716,
          0.25524646043777466,
          0.2552381455898285,
          0.25519922375679016,
          0.2551248073577881,
          0.2550729215145111,
          0.2550579607486725,
          0.25505608320236206,
          0.2550353407859802,
          0.25499099493026733,
          0.25496628880500793,
          0.2549378573894501,
          0.2548656761646271,
          0.25480571389198303,
          0.25476160645484924,
          0.2547305226325989,
          0.25470876693725586,
          0.2547212243080139,
          0.25470247864723206,
          0.2546589970588684,
          0.25457999110221863,
          0.2545621991157532,
          0.25450676679611206,
          0.2543855607509613,
          0.25434210896492004,
          0.25431451201438904,
          0.2543109357357025,
          0.2543157637119293,
          0.254297137260437,
          0.25427064299583435,
          0.25420722365379333,
          0.25411611795425415,
          0.2540234923362732,
          0.2539110481739044,
          0.25381898880004883,
          0.2537764012813568,
          0.2536533772945404,
          0.2533814311027527,
          0.2534506022930145,
          0.2532693147659302,
          0.2532716691493988,
          0.25339651107788086,
          0.2532396614551544,
          0.2532889246940613,
          0.25323396921157837,
          0.25308525562286377,
          0.2529635727405548,
          0.252890944480896,
          0.2527700960636139,
          0.252693772315979,
          0.2526300847530365,
          0.25257161259651184,
          0.25248807668685913,
          0.25238037109375,
          0.2523617148399353,
          0.25232622027397156,
          0.25235557556152344,
          0.2523781955242157,
          0.2523484230041504,
          0.25227928161621094,
          0.2522614300251007,
          0.25228390097618103,
          0.2522977292537689,
          0.25232139229774475,
          0.252316951751709,
          0.2522720694541931,
          0.2521915137767792
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_5",
         "type": "scatter",
         "y": [
          0.08225777745246887,
          0.08439759910106659,
          0.08419742435216904,
          0.08329182118177414,
          0.08386584371328354,
          0.08332140743732452,
          0.08290152251720428,
          0.08309129625558853,
          0.08273258060216904,
          0.0825924202799797,
          0.08252193033695221,
          0.08246856927871704,
          0.08262589573860168,
          0.08259867131710052,
          0.0823797658085823,
          0.08224380016326904,
          0.08232488483190536,
          0.08233964443206787,
          0.0822591558098793,
          0.08224908262491226,
          0.08228857070207596,
          0.08240789920091629,
          0.0824815109372139,
          0.08250518143177032,
          0.0825883075594902,
          0.08269266784191132,
          0.08282188326120377,
          0.08298323303461075,
          0.0831817165017128,
          0.08349338918924332,
          0.08388230949640274,
          0.08407092094421387,
          0.08403675258159637,
          0.08398295193910599,
          0.08401031792163849,
          0.08413194119930267,
          0.08422542363405228,
          0.08427876979112625,
          0.08446786552667618,
          0.08471944183111191,
          0.08482899516820908,
          0.08487599343061447,
          0.08491436392068863,
          0.08486238867044449,
          0.08496323972940445,
          0.08537278324365616,
          0.08575966954231262,
          0.08582159876823425,
          0.08553211390972137,
          0.0852244645357132,
          0.08524244278669357,
          0.08550293743610382,
          0.08551373332738876,
          0.08511069416999817,
          0.08478032797574997,
          0.08463968336582184,
          0.08450727164745331,
          0.08470983803272247,
          0.08505642414093018,
          0.08486340939998627,
          0.08459609746932983,
          0.08471904695034027,
          0.0847930759191513,
          0.08460084348917007,
          0.0844343975186348,
          0.08463292568922043,
          0.0855376124382019,
          0.0869772806763649,
          0.08752360939979553,
          0.08667442202568054,
          0.08556769788265228,
          0.0848442018032074,
          0.08477850258350372,
          0.0853855162858963,
          0.08581192046403885,
          0.08599751442670822,
          0.08636712282896042,
          0.08605336397886276,
          0.0851869061589241,
          0.08501092344522476,
          0.08518341928720474,
          0.08520485460758209,
          0.08506301790475845,
          0.08463381975889206,
          0.08429872244596481,
          0.08458764106035233,
          0.08527984470129013,
          0.0854782834649086,
          0.08515036851167679,
          0.08497956395149231,
          0.08483602851629257,
          0.08487648516893387,
          0.08530712127685547,
          0.08520646393299103,
          0.08456386625766754,
          0.08457829803228378,
          0.08535346388816833,
          0.08569561690092087,
          0.08514419198036194,
          0.0850343182682991,
          0.0862758681178093,
          0.0877082571387291,
          0.08787623047828674,
          0.08665372431278229,
          0.08510621637105942,
          0.08451701700687408,
          0.0855007991194725,
          0.08727090060710907,
          0.08768632262945175,
          0.08616411685943604,
          0.08482946455478668,
          0.08476158231496811,
          0.08482812345027924,
          0.08453880995512009,
          0.08434342592954636,
          0.08444178849458694,
          0.08478503674268723,
          0.08519292622804642,
          0.08565157651901245,
          0.08644350618124008,
          0.0873836874961853,
          0.08752569556236267,
          0.08674678206443787,
          0.08647201210260391,
          0.08734003454446793,
          0.08793125301599503,
          0.08748044073581696,
          0.08697380125522614,
          0.08682616800069809,
          0.08686897158622742,
          0.08740156143903732,
          0.08787024766206741,
          0.0874212309718132,
          0.08692009001970291,
          0.08771105855703354,
          0.0894140899181366,
          0.09048803150653839,
          0.0904543399810791,
          0.09040699899196625,
          0.09091517329216003,
          0.09123155474662781,
          0.0914296880364418,
          0.09237559884786606,
          0.09349828213453293,
          0.09373755007982254,
          0.09351859986782074,
          0.0936352089047432,
          0.09394870698451996,
          0.09416750818490982,
          0.09441385418176651,
          0.09477683901786804,
          0.0952804684638977,
          0.09609029442071915,
          0.09707524627447128,
          0.09771460294723511,
          0.09783651679754257,
          0.09777495265007019,
          0.09773436188697815,
          0.0975104421377182,
          0.09698133915662766,
          0.09638289362192154,
          0.09605451673269272,
          0.0960933119058609,
          0.09620563685894012,
          0.09603552520275116,
          0.09567471593618393,
          0.09539997577667236,
          0.09512756764888763,
          0.09465039521455765,
          0.09413774311542511,
          0.09386048465967178,
          0.09373997151851654,
          0.09354815632104874,
          0.09330180287361145,
          0.0931251123547554,
          0.09302718937397003,
          0.09302116930484772,
          0.09331560134887695,
          0.0943228006362915,
          0.09634514153003693,
          0.09911444783210754,
          0.10178142040967941,
          0.10356328636407852,
          0.10426845401525497,
          0.1040899008512497,
          0.10322864353656769,
          0.10184428840875626,
          0.10013847798109055,
          0.09836075454950333,
          0.09676160663366318,
          0.09551012516021729,
          0.09460850059986115,
          0.09396027028560638,
          0.0935775488615036,
          0.09354246407747269,
          0.09376072138547897,
          0.09395655989646912,
          0.09398704022169113,
          0.09393854439258575,
          0.09388510137796402,
          0.09382190555334091,
          0.09378234297037125,
          0.09373791515827179,
          0.0935564860701561,
          0.09324557334184647,
          0.0930408239364624,
          0.09307065606117249,
          0.09316714107990265,
          0.09312140196561813,
          0.09296395629644394,
          0.09287052601575851,
          0.09289684146642685,
          0.0929684042930603,
          0.09298694133758545,
          0.09294018149375916,
          0.0928976759314537,
          0.09290346503257751,
          0.09293219447135925,
          0.0929262712597847,
          0.09286509454250336,
          0.09278549998998642,
          0.09271173924207687,
          0.0926101878285408,
          0.09239856898784637,
          0.09204451739788055,
          0.09163664281368256,
          0.09129011631011963,
          0.09101621061563492,
          0.090749591588974,
          0.09046520292758942,
          0.09019757062196732,
          0.09002659469842911,
          0.09000182151794434,
          0.09003559499979019,
          0.0899496003985405,
          0.0896943211555481,
          0.0893917977809906,
          0.08911966532468796,
          0.08881857246160507,
          0.08846078813076019,
          0.08815760165452957,
          0.0880059152841568,
          0.08796391636133194,
          0.0879177376627922,
          0.0877872109413147,
          0.08755213767290115,
          0.0872378870844841,
          0.08690883219242096,
          0.08663732558488846,
          0.08644794672727585,
          0.0863119438290596,
          0.0861821249127388,
          0.08602453768253326,
          0.08582502603530884,
          0.08559959381818771,
          0.08540549129247665,
          0.08529443293809891,
          0.08525527268648148,
          0.08521706610918045,
          0.08511967957019806,
          0.0849708691239357,
          0.08482271432876587,
          0.08470743149518967,
          0.08461634069681168,
          0.08452919870615005,
          0.08444412052631378,
          0.08437293022871017,
          0.08431461453437805,
          0.08424768596887589,
          0.08415421843528748,
          0.08404583483934402,
          0.08394961804151535,
          0.08387824892997742,
          0.08382513374090195,
          0.08377959579229355,
          0.0837344154715538,
          0.08368875086307526,
          0.08365175127983093,
          0.0836290717124939,
          0.08360423147678375,
          0.08355008810758591,
          0.08346251398324966,
          0.08336855471134186
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_5",
         "type": "scatter",
         "y": [
          0.08661354333162308,
          0.08874864876270294,
          0.08863873779773712,
          0.08859219402074814,
          0.08858039230108261,
          0.0882299542427063,
          0.08842933923006058,
          0.08856048434972763,
          0.08844473212957382,
          0.08820225298404694,
          0.08820616453886032,
          0.08807747811079025,
          0.08799852430820465,
          0.08794375509023666,
          0.0873027965426445,
          0.08702393621206284,
          0.08695027977228165,
          0.08680002391338348,
          0.08673097938299179,
          0.08680881559848785,
          0.08674193918704987,
          0.0867653489112854,
          0.08654668182134628,
          0.08653266727924347,
          0.08659125864505768,
          0.08658118546009064,
          0.08659876137971878,
          0.08672072738409042,
          0.08693151921033859,
          0.08745931833982468,
          0.08809789270162582,
          0.08813299983739853,
          0.08801961690187454,
          0.08780495822429657,
          0.08780399709939957,
          0.08809656649827957,
          0.08812334388494492,
          0.08810843527317047,
          0.0883021280169487,
          0.08844536542892456,
          0.08858659863471985,
          0.08860331028699875,
          0.08858515322208405,
          0.08845289796590805,
          0.08860598504543304,
          0.08869262784719467,
          0.08890582621097565,
          0.08892567455768585,
          0.0888739675283432,
          0.08879183977842331,
          0.08886932581663132,
          0.08889085799455643,
          0.08888397365808487,
          0.08867651969194412,
          0.08851218968629837,
          0.08844438940286636,
          0.08836222440004349,
          0.08840606361627579,
          0.08860200643539429,
          0.08859050273895264,
          0.08854296803474426,
          0.08849256485700607,
          0.0883818194270134,
          0.08824621886014938,
          0.08821713179349899,
          0.0884421318769455,
          0.08855542540550232,
          0.08880674839019775,
          0.08887381851673126,
          0.08869999647140503,
          0.08852627873420715,
          0.088363878428936,
          0.0883951336145401,
          0.08845453709363937,
          0.08852852880954742,
          0.08859193325042725,
          0.08874579519033432,
          0.08875562995672226,
          0.08862901479005814,
          0.08858148753643036,
          0.08868999034166336,
          0.0887000784277916,
          0.08861438184976578,
          0.08833646029233932,
          0.08853565901517868,
          0.08884398639202118,
          0.08918378502130508,
          0.08975337445735931,
          0.08978992700576782,
          0.08978726714849472,
          0.08990746736526489,
          0.09010455012321472,
          0.09028292447328568,
          0.09020084887742996,
          0.08986228704452515,
          0.08984299004077911,
          0.08998509496450424,
          0.09002683311700821,
          0.08990654349327087,
          0.08986824750900269,
          0.09016649425029755,
          0.09045065939426422,
          0.09048639982938766,
          0.09018229693174362,
          0.08980344980955124,
          0.08969476073980331,
          0.08998686075210571,
          0.09039691090583801,
          0.09049618989229202,
          0.09009620547294617,
          0.08976302295923233,
          0.08976446837186813,
          0.08975913375616074,
          0.08965389430522919,
          0.08960624784231186,
          0.08959086239337921,
          0.08962025493383408,
          0.08967002481222153,
          0.08973579108715057,
          0.08983705937862396,
          0.08996541053056717,
          0.08989289402961731,
          0.08960878849029541,
          0.08939991891384125,
          0.08948041498661041,
          0.08975081890821457,
          0.08964230865240097,
          0.089413583278656,
          0.08934669196605682,
          0.08921293914318085,
          0.0892241820693016,
          0.0892246887087822,
          0.08916962891817093,
          0.0891747996211052,
          0.08931910991668701,
          0.08955148607492447,
          0.08964135497808456,
          0.08980607241392136,
          0.08992522954940796,
          0.08993624895811081,
          0.08994986116886139,
          0.08982867002487183,
          0.08996142446994781,
          0.09043679386377335,
          0.0901981070637703,
          0.09001058340072632,
          0.09004232287406921,
          0.0901065245270729,
          0.08995717018842697,
          0.0899621844291687,
          0.08996650576591492,
          0.09002253413200378,
          0.09032943099737167,
          0.09092067182064056,
          0.09111971408128738,
          0.09123536199331284,
          0.0912778377532959,
          0.09144755452871323,
          0.09146445244550705,
          0.09140961617231369,
          0.09111581742763519,
          0.09098993986845016,
          0.09106682986021042,
          0.09120753407478333,
          0.0911831259727478,
          0.09092787653207779,
          0.09073540568351746,
          0.09070488065481186,
          0.09062010794878006,
          0.09038925915956497,
          0.09003812074661255,
          0.08999896794557571,
          0.08994226902723312,
          0.08989296108484268,
          0.08987899869680405,
          0.08977983146905899,
          0.08974325656890869,
          0.08977670222520828,
          0.09030391275882721,
          0.0912436693906784,
          0.09209295362234116,
          0.09261054545640945,
          0.09279559552669525,
          0.0928734689950943,
          0.092463418841362,
          0.0917549878358841,
          0.09110496193170547,
          0.09060759842395782,
          0.09024406224489212,
          0.0900799036026001,
          0.08999790251255035,
          0.08993154764175415,
          0.08988844603300095,
          0.08992566168308258,
          0.09039133787155151,
          0.09049811214208603,
          0.0906304195523262,
          0.090427465736866,
          0.09012080729007721,
          0.08996271342039108,
          0.08993890881538391,
          0.08995518088340759,
          0.09002700448036194,
          0.09015116095542908,
          0.09015317261219025,
          0.09004095196723938,
          0.09003858268260956,
          0.090114064514637,
          0.0900677740573883,
          0.0899389237165451,
          0.0898585170507431,
          0.0898054912686348,
          0.08977442234754562,
          0.08977411687374115,
          0.08977866172790527,
          0.08976489305496216,
          0.08978872001171112,
          0.08978328108787537,
          0.0897749736905098,
          0.08973605185747147,
          0.0896616205573082,
          0.08960973471403122,
          0.0895947739481926,
          0.08959291130304337,
          0.08957216143608093,
          0.08952781558036804,
          0.08950309455394745,
          0.08947467803955078,
          0.08940248936414719,
          0.08934252709150314,
          0.08929840475320816,
          0.08926732838153839,
          0.08924557268619537,
          0.08925802260637283,
          0.08923929184675217,
          0.08919581770896912,
          0.08911679685115814,
          0.08909901231527328,
          0.08904359489679337,
          0.08892236649990082,
          0.08887890726327896,
          0.08885131776332855,
          0.08884776383638382,
          0.08885257691144943,
          0.08883394300937653,
          0.08880747109651566,
          0.08874402195215225,
          0.08865292370319366,
          0.08856029063463211,
          0.08844786137342453,
          0.08835581690073013,
          0.08831321448087692,
          0.08819018304347992,
          0.0879182368516922,
          0.08798740059137344,
          0.08780612796545029,
          0.08780846744775772,
          0.08793333172798157,
          0.08777648955583572,
          0.08782575279474258,
          0.08777078241109848,
          0.08762208372354507,
          0.08750038594007492,
          0.0874277651309967,
          0.0873069167137146,
          0.08723058551549911,
          0.08716689795255661,
          0.08710842579603195,
          0.08702488243579865,
          0.08691718429327011,
          0.08689851313829422,
          0.08686304837465286,
          0.08689238131046295,
          0.08691499382257462,
          0.0868852287530899,
          0.08681610226631165,
          0.08679825067520142,
          0.08682071417570114,
          0.08683454245328903,
          0.08685819804668427,
          0.0868537649512291,
          0.08680886775255203,
          0.08672832697629929
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_6",
         "type": "scatter",
         "y": [
          0.3557964861392975,
          0.3571271002292633,
          0.35660481452941895,
          0.35589689016342163,
          0.3558734059333801,
          0.35546889901161194,
          0.3556188642978668,
          0.35632121562957764,
          0.3560011088848114,
          0.3556795120239258,
          0.35565441846847534,
          0.3553994596004486,
          0.3553156554698944,
          0.355212539434433,
          0.35504618287086487,
          0.3550299406051636,
          0.355173259973526,
          0.3551698327064514,
          0.35510560870170593,
          0.3551087975502014,
          0.3550879955291748,
          0.35509881377220154,
          0.35509997606277466,
          0.3550929129123688,
          0.3551485538482666,
          0.3551848828792572,
          0.35527023673057556,
          0.3553905487060547,
          0.35551831126213074,
          0.3560572862625122,
          0.3567080795764923,
          0.3568234443664551,
          0.35663682222366333,
          0.3563743233680725,
          0.35640689730644226,
          0.3568582236766815,
          0.3570367991924286,
          0.3569222390651703,
          0.3571816086769104,
          0.3575511872768402,
          0.35764700174331665,
          0.35763296484947205,
          0.3574906885623932,
          0.35731324553489685,
          0.3573271334171295,
          0.3576427102088928,
          0.35838860273361206,
          0.359004944562912,
          0.35862985253334045,
          0.35826006531715393,
          0.358768492937088,
          0.35907450318336487,
          0.3585911989212036,
          0.357677161693573,
          0.35697075724601746,
          0.356900691986084,
          0.356893926858902,
          0.3566625416278839,
          0.35670748353004456,
          0.35681456327438354,
          0.35663121938705444,
          0.3564006984233856,
          0.3562362790107727,
          0.3561689257621765,
          0.35620227456092834,
          0.3562891483306885,
          0.3566538691520691,
          0.3573308289051056,
          0.3575592339038849,
          0.35702526569366455,
          0.3563867211341858,
          0.3560694754123688,
          0.3561566472053528,
          0.35647857189178467,
          0.35655155777931213,
          0.35652679204940796,
          0.3568568229675293,
          0.3570939898490906,
          0.3569852411746979,
          0.3571011424064636,
          0.35740962624549866,
          0.35748156905174255,
          0.35719606280326843,
          0.3566262125968933,
          0.35635894536972046,
          0.35721129179000854,
          0.35905566811561584,
          0.36076638102531433,
          0.3614785373210907,
          0.3616434931755066,
          0.36266541481018066,
          0.3647348880767822,
          0.36571866273880005,
          0.36449378728866577,
          0.362928181886673,
          0.36237430572509766,
          0.36223992705345154,
          0.3617855906486511,
          0.361083984375,
          0.3613913357257843,
          0.3630331754684448,
          0.3639712929725647,
          0.3630315065383911,
          0.3614111542701721,
          0.3604525625705719,
          0.36059635877609253,
          0.3619774580001831,
          0.3637672960758209,
          0.3638392388820648,
          0.3622266948223114,
          0.36144474148750305,
          0.3616129457950592,
          0.3611096441745758,
          0.3602089583873749,
          0.3596112132072449,
          0.3592239022254944,
          0.35909003019332886,
          0.3590271770954132,
          0.3588128983974457,
          0.35870787501335144,
          0.35866695642471313,
          0.35818931460380554,
          0.35735657811164856,
          0.35695603489875793,
          0.35717469453811646,
          0.3572061359882355,
          0.3568204343318939,
          0.3566499650478363,
          0.3566705584526062,
          0.3564840257167816,
          0.35630175471305847,
          0.3562158942222595,
          0.3560152053833008,
          0.35593363642692566,
          0.3562873899936676,
          0.3568516671657562,
          0.35716357827186584,
          0.3571651875972748,
          0.3572327494621277,
          0.3574642837047577,
          0.35752037167549133,
          0.35743290185928345,
          0.3576442301273346,
          0.35807427763938904,
          0.35825303196907043,
          0.35820868611335754,
          0.3583362400531769,
          0.3586917519569397,
          0.3590613305568695,
          0.35942116379737854,
          0.3599015474319458,
          0.3606823682785034,
          0.36197376251220703,
          0.36353597044944763,
          0.3645762801170349,
          0.3647938668727875,
          0.3647753596305847,
          0.36495405435562134,
          0.3650122284889221,
          0.3646140694618225,
          0.3639683425426483,
          0.36363235116004944,
          0.36396268010139465,
          0.3645721673965454,
          0.36473166942596436,
          0.364427775144577,
          0.36413002014160156,
          0.36381587386131287,
          0.3632350265979767,
          0.362690806388855,
          0.3625429570674896,
          0.36252257227897644,
          0.3622276186943054,
          0.36173760890960693,
          0.3612719476222992,
          0.3608337640762329,
          0.3604341447353363,
          0.36048781871795654,
          0.3618624806404114,
          0.3650444746017456,
          0.3691225051879883,
          0.37224674224853516,
          0.3733084797859192,
          0.37261924147605896,
          0.37096667289733887,
          0.3688802421092987,
          0.3667430877685547,
          0.3649256229400635,
          0.36364588141441345,
          0.3628728985786438,
          0.36237654089927673,
          0.36192357540130615,
          0.3615059554576874,
          0.3614264726638794,
          0.36190730333328247,
          0.36265966296195984,
          0.36310380697250366,
          0.3630659580230713,
          0.36285123229026794,
          0.36269113421440125,
          0.36257681250572205,
          0.36252787709236145,
          0.3625316023826599,
          0.3624629080295563,
          0.362298846244812,
          0.36225563287734985,
          0.3624766767024994,
          0.362779438495636,
          0.362833172082901,
          0.36253443360328674,
          0.36207500100135803,
          0.3616921901702881,
          0.3615127503871918,
          0.36149337887763977,
          0.3615671694278717,
          0.36172303557395935,
          0.3618682324886322,
          0.3618439733982086,
          0.3615921437740326,
          0.3612002730369568,
          0.3608003258705139,
          0.3604622185230255,
          0.3602145314216614,
          0.3600286543369293,
          0.35982465744018555,
          0.3595554232597351,
          0.35925984382629395,
          0.3589749038219452,
          0.35870054364204407,
          0.35845375061035156,
          0.3582465350627899,
          0.3581094443798065,
          0.3580765128135681,
          0.35809221863746643,
          0.3580141067504883,
          0.3577984869480133,
          0.3575790226459503,
          0.3574635684490204,
          0.35737133026123047,
          0.35719749331474304,
          0.35703063011169434,
          0.35702112317085266,
          0.35712212324142456,
          0.3571401536464691,
          0.3569931387901306,
          0.3567696809768677,
          0.35656771063804626,
          0.35640978813171387,
          0.356291800737381,
          0.3561990559101105,
          0.35611432790756226,
          0.35604920983314514,
          0.35602471232414246,
          0.3560101389884949,
          0.355962872505188,
          0.3559114933013916,
          0.3559190034866333,
          0.35597896575927734,
          0.3560059070587158,
          0.3559444844722748,
          0.35583820939064026,
          0.3557661473751068,
          0.3557469844818115,
          0.3557415306568146,
          0.3557177186012268,
          0.3556821942329407,
          0.3556520938873291,
          0.35562044382095337,
          0.35556986927986145,
          0.35551032423973083,
          0.35548263788223267,
          0.35550642013549805,
          0.355548620223999,
          0.3555602729320526,
          0.355534166097641,
          0.3555053174495697,
          0.3555053770542145,
          0.35553449392318726,
          0.355569988489151,
          0.3555830717086792,
          0.35555708408355713,
          0.35550177097320557,
          0.3554436266422272
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_6",
         "type": "scatter",
         "y": [
          0.3653956651687622,
          0.36753079295158386,
          0.36742088198661804,
          0.36737433075904846,
          0.36736252903938293,
          0.367012083530426,
          0.3672114908695221,
          0.36734262108802795,
          0.36722686886787415,
          0.36698439717292786,
          0.36698830127716064,
          0.3668596148490906,
          0.36678066849708557,
          0.366725891828537,
          0.3660849332809448,
          0.36580607295036316,
          0.36573243141174316,
          0.3655821681022644,
          0.3655131161212921,
          0.3655909597873688,
          0.3655240833759308,
          0.3655474781990051,
          0.3653288185596466,
          0.3653148114681244,
          0.3653734028339386,
          0.36536332964897156,
          0.3653808832168579,
          0.36550286412239075,
          0.3657136559486389,
          0.366241455078125,
          0.36688002943992615,
          0.36691513657569885,
          0.36680176854133606,
          0.3665871024131775,
          0.3665861189365387,
          0.3668787181377411,
          0.36690548062324524,
          0.3668905794620514,
          0.367084264755249,
          0.3672274947166443,
          0.3673687279224396,
          0.3673854470252991,
          0.36736729741096497,
          0.3672350347042084,
          0.36738812923431396,
          0.367474764585495,
          0.36768797039985657,
          0.3677078187465668,
          0.3676561117172241,
          0.36757397651672363,
          0.36765146255493164,
          0.36767297983169556,
          0.3676661252975464,
          0.36745867133140564,
          0.3672943413257599,
          0.3672265112400055,
          0.367144376039505,
          0.3671881854534149,
          0.367384135723114,
          0.36737263202667236,
          0.367325097322464,
          0.3672747015953064,
          0.3671639561653137,
          0.3670283555984497,
          0.3669992685317993,
          0.3672242760658264,
          0.36733755469322205,
          0.3675888776779175,
          0.3676559627056122,
          0.36748212575912476,
          0.3673084080219269,
          0.36714601516723633,
          0.367177277803421,
          0.3672366738319397,
          0.36731067299842834,
          0.367374062538147,
          0.36752793192863464,
          0.3675377666950226,
          0.36741113662719727,
          0.3673636317253113,
          0.3674721121788025,
          0.3674822151660919,
          0.3673965036869049,
          0.36711859703063965,
          0.367317795753479,
          0.3676261305809021,
          0.3679659068584442,
          0.36853551864624023,
          0.36857205629348755,
          0.36856940388679504,
          0.3686895966529846,
          0.36888667941093445,
          0.3690650761127472,
          0.3689829707145691,
          0.3686444163322449,
          0.36862513422966003,
          0.36876723170280457,
          0.36880895495414734,
          0.3686886727809906,
          0.3686503767967224,
          0.36894863843917847,
          0.36923280358314514,
          0.369268536567688,
          0.36896443367004395,
          0.36858558654785156,
          0.36847689747810364,
          0.36876899003982544,
          0.36917904019355774,
          0.36927834153175354,
          0.3688783347606659,
          0.36854514479637146,
          0.36854660511016846,
          0.36854127049446106,
          0.3684360384941101,
          0.3683883845806122,
          0.36837300658226013,
          0.3684023916721344,
          0.36845216155052185,
          0.3685179352760315,
          0.3686192035675049,
          0.3687475621700287,
          0.36867502331733704,
          0.36839091777801514,
          0.36818206310272217,
          0.36826255917549133,
          0.3685329556465149,
          0.3684244453907013,
          0.36819571256637573,
          0.36812883615493774,
          0.36799508333206177,
          0.36800631880760193,
          0.3680068254470825,
          0.36795175075531006,
          0.3679569363594055,
          0.36810123920440674,
          0.368333637714386,
          0.3684234917163849,
          0.3685882091522217,
          0.3687073588371277,
          0.36871838569641113,
          0.3687320053577423,
          0.36861079931259155,
          0.36874356865882874,
          0.36921894550323486,
          0.3689802289009094,
          0.36879271268844604,
          0.36882445216178894,
          0.36888864636421204,
          0.3687393069267273,
          0.3687443137168884,
          0.36874863505363464,
          0.3688046634197235,
          0.369111567735672,
          0.3697028160095215,
          0.3699018359184265,
          0.37001749873161316,
          0.3700599670410156,
          0.37022969126701355,
          0.3702465891838074,
          0.370191752910614,
          0.3698979616165161,
          0.3697720766067505,
          0.36984896659851074,
          0.36998966336250305,
          0.36996525526046753,
          0.3697099983692169,
          0.3695175349712372,
          0.3694870173931122,
          0.3694022595882416,
          0.3691714107990265,
          0.3688202500343323,
          0.36878111958503723,
          0.36872440576553345,
          0.3686750829219818,
          0.36866113543510437,
          0.3685619831085205,
          0.3685253858566284,
          0.3685588240623474,
          0.36908605694770813,
          0.3700258135795593,
          0.3708750903606415,
          0.3713926672935486,
          0.37157773971557617,
          0.3716556131839752,
          0.3712455630302429,
          0.370537132024765,
          0.369887113571167,
          0.36938974261283875,
          0.36902618408203125,
          0.3688620328903198,
          0.3687800467014313,
          0.3687136769294739,
          0.36867058277130127,
          0.3687078058719635,
          0.36917346715927124,
          0.36928024888038635,
          0.3694125711917877,
          0.3692095875740051,
          0.36890295147895813,
          0.3687448501586914,
          0.36872103810310364,
          0.3687373101711273,
          0.36880913376808167,
          0.3689332902431488,
          0.36893531680107117,
          0.3688230812549591,
          0.3688207268714905,
          0.3688961863517761,
          0.36884990334510803,
          0.368721067905426,
          0.368640661239624,
          0.3685876429080963,
          0.36855655908584595,
          0.36855626106262207,
          0.368560791015625,
          0.3685470223426819,
          0.36857086420059204,
          0.3685654103755951,
          0.3685570955276489,
          0.3685181736946106,
          0.3684437572956085,
          0.36839187145233154,
          0.3683769106864929,
          0.3683750331401825,
          0.36835429072380066,
          0.36830994486808777,
          0.36828523874282837,
          0.3682568073272705,
          0.3681846261024475,
          0.36812466382980347,
          0.3680805563926697,
          0.3680494725704193,
          0.3680277168750763,
          0.36804017424583435,
          0.3680214285850525,
          0.36797794699668884,
          0.36789894104003906,
          0.3678811490535736,
          0.3678257167339325,
          0.36770451068878174,
          0.3676610589027405,
          0.3676334619522095,
          0.36762988567352295,
          0.36763471364974976,
          0.36761608719825745,
          0.3675895929336548,
          0.36752617359161377,
          0.3674350678920746,
          0.36734244227409363,
          0.36722999811172485,
          0.36713793873786926,
          0.36709535121917725,
          0.36697232723236084,
          0.3667003810405731,
          0.36676955223083496,
          0.3665882647037506,
          0.36659061908721924,
          0.3667154610157013,
          0.36655861139297485,
          0.3666078746318817,
          0.3665529191493988,
          0.3664042055606842,
          0.36628252267837524,
          0.36620989441871643,
          0.3660890460014343,
          0.36601272225379944,
          0.36594903469085693,
          0.3658905625343323,
          0.36580702662467957,
          0.36569932103157043,
          0.36568066477775574,
          0.365645170211792,
          0.36567452549934387,
          0.36569714546203613,
          0.3656673729419708,
          0.36559823155403137,
          0.36558037996292114,
          0.36560285091400146,
          0.36561667919158936,
          0.3656403422355652,
          0.3656359016895294,
          0.36559101939201355,
          0.3655104637145996
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_7",
         "type": "scatter",
         "y": [
          0.4351442754268646,
          0.43569374084472656,
          0.43561363220214844,
          0.4355781376361847,
          0.43558555841445923,
          0.43548446893692017,
          0.4354780614376068,
          0.43560272455215454,
          0.435566246509552,
          0.43548211455345154,
          0.43545666337013245,
          0.435448557138443,
          0.435467392206192,
          0.43539395928382874,
          0.4352737069129944,
          0.43522340059280396,
          0.4351974427700043,
          0.43516120314598083,
          0.43516406416893005,
          0.43519648909568787,
          0.43519335985183716,
          0.43517953157424927,
          0.4351349472999573,
          0.4350808262825012,
          0.43503373861312866,
          0.434967577457428,
          0.434926837682724,
          0.4349120855331421,
          0.43489786982536316,
          0.43491438031196594,
          0.43495774269104004,
          0.43497219681739807,
          0.4349762499332428,
          0.43498215079307556,
          0.4349927604198456,
          0.4350048899650574,
          0.4349974989891052,
          0.4349978268146515,
          0.4350317418575287,
          0.43506020307540894,
          0.435057133436203,
          0.43505725264549255,
          0.4350917637348175,
          0.43513137102127075,
          0.4351402223110199,
          0.435149222612381,
          0.43517377972602844,
          0.4351789355278015,
          0.43516474962234497,
          0.4351581931114197,
          0.43517807126045227,
          0.43523550033569336,
          0.43526187539100647,
          0.43523532152175903,
          0.43522965908050537,
          0.43523624539375305,
          0.43522700667381287,
          0.43524596095085144,
          0.43529361486434937,
          0.4353103041648865,
          0.43532854318618774,
          0.43537795543670654,
          0.4354167878627777,
          0.4354303181171417,
          0.4354489743709564,
          0.43551105260849,
          0.43559345602989197,
          0.4356623589992523,
          0.43570056557655334,
          0.43569040298461914,
          0.43564215302467346,
          0.43560338020324707,
          0.43562471866607666,
          0.43567684292793274,
          0.43568772077560425,
          0.43567928671836853,
          0.4357050061225891,
          0.4357275366783142,
          0.43572142720222473,
          0.43572312593460083,
          0.4357280135154724,
          0.4357137680053711,
          0.4356766939163208,
          0.4356291592121124,
          0.43562957644462585,
          0.43572676181793213,
          0.4358721077442169,
          0.4359879493713379,
          0.436051607131958,
          0.4360869526863098,
          0.436156302690506,
          0.4362724721431732,
          0.43632832169532776,
          0.4362689256668091,
          0.43619585037231445,
          0.43617919087409973,
          0.43617725372314453,
          0.436139851808548,
          0.4360809624195099,
          0.43608856201171875,
          0.43617892265319824,
          0.43622836470603943,
          0.43617501854896545,
          0.43608754873275757,
          0.4360329806804657,
          0.4360348582267761,
          0.4361035227775574,
          0.436198353767395,
          0.4362085163593292,
          0.43612784147262573,
          0.43608784675598145,
          0.4361051023006439,
          0.436089426279068,
          0.4360435903072357,
          0.4360080361366272,
          0.43598419427871704,
          0.43597155809402466,
          0.43596431612968445,
          0.4359581470489502,
          0.43595924973487854,
          0.43595585227012634,
          0.43592211604118347,
          0.4358588755130768,
          0.43581297993659973,
          0.4358113408088684,
          0.43580231070518494,
          0.4357602894306183,
          0.4357360601425171,
          0.43573132157325745,
          0.43570613861083984,
          0.43567097187042236,
          0.4356321692466736,
          0.43558117747306824,
          0.43556198477745056,
          0.4356115162372589,
          0.4356846809387207,
          0.43570929765701294,
          0.4356733560562134,
          0.43563222885131836,
          0.435619056224823,
          0.43560317158699036,
          0.4355790317058563,
          0.4355734884738922,
          0.43555477261543274,
          0.43549829721450806,
          0.43546542525291443,
          0.435480535030365,
          0.4354782998561859,
          0.43544068932533264,
          0.43541979789733887,
          0.4354262351989746,
          0.43543753027915955,
          0.43545854091644287,
          0.43548721075057983,
          0.4355010688304901,
          0.435501366853714,
          0.4355090856552124,
          0.43552911281585693,
          0.4355480372905731,
          0.43555113673210144,
          0.4355327785015106,
          0.43551260232925415,
          0.4355190694332123,
          0.4355379343032837,
          0.43552663922309875,
          0.4354882538318634,
          0.43546342849731445,
          0.43546250462532043,
          0.4354599118232727,
          0.43543803691864014,
          0.43540453910827637,
          0.4353804588317871,
          0.43538016080856323,
          0.43539223074913025,
          0.4353927671909332,
          0.4353795349597931,
          0.4353748559951782,
          0.43539944291114807,
          0.435459166765213,
          0.43554481863975525,
          0.43563002347946167,
          0.43568000197410583,
          0.43567749857902527,
          0.43563541769981384,
          0.4355795383453369,
          0.43552571535110474,
          0.4354739189147949,
          0.43542376160621643,
          0.43539154529571533,
          0.4353885352611542,
          0.4353989362716675,
          0.4354010224342346,
          0.4353998303413391,
          0.4354185163974762,
          0.43545809388160706,
          0.4354880750179291,
          0.4354802072048187,
          0.43544140458106995,
          0.4354006350040436,
          0.4353731870651245,
          0.4353591799736023,
          0.4353651702404022,
          0.43539321422576904,
          0.435418963432312,
          0.43541836738586426,
          0.43540090322494507,
          0.43539178371429443,
          0.4353969693183899,
          0.43539828062057495,
          0.43537992238998413,
          0.4353477954864502,
          0.43532052636146545,
          0.4353081285953522,
          0.43530091643333435,
          0.4352882206439972,
          0.4352741241455078,
          0.4352669417858124,
          0.43526315689086914,
          0.4352528154850006,
          0.43523454666137695,
          0.4352157711982727,
          0.4352027177810669,
          0.4351925551891327,
          0.435177206993103,
          0.4351561665534973,
          0.435137540102005,
          0.4351271688938141,
          0.43511927127838135,
          0.4351062774658203,
          0.4350894093513489,
          0.43507489562034607,
          0.4350667893886566,
          0.43506407737731934,
          0.4350602328777313,
          0.435048371553421,
          0.4350315034389496,
          0.4350197911262512,
          0.43501365184783936,
          0.43500280380249023,
          0.43498486280441284,
          0.43497174978256226,
          0.43497005105018616,
          0.4349704682826996,
          0.434965044260025,
          0.434957355260849,
          0.43494993448257446,
          0.4349364638328552,
          0.4349171221256256,
          0.434905081987381,
          0.43490859866142273,
          0.43492090702056885,
          0.43493151664733887,
          0.4349355101585388,
          0.4349302649497986,
          0.4349164366722107,
          0.4349025785923004,
          0.43489575386047363,
          0.4348921775817871,
          0.4348865747451782,
          0.4348829984664917,
          0.43488508462905884,
          0.4348859488964081,
          0.4348815083503723,
          0.4348808825016022,
          0.4348914921283722,
          0.43490493297576904,
          0.4349079132080078,
          0.43489959836006165,
          0.434888631105423,
          0.43487849831581116,
          0.4348674714565277,
          0.43485602736473083,
          0.43484732508659363,
          0.43484261631965637,
          0.43484172224998474,
          0.4348425269126892,
          0.4348410964012146,
          0.43483924865722656,
          0.43484583497047424,
          0.4348633289337158,
          0.4348788857460022,
          0.43487784266471863,
          0.4348623752593994
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_7",
         "type": "scatter",
         "y": [
          0.43409308791160583,
          0.4362282156944275,
          0.43611830472946167,
          0.4360717535018921,
          0.43605995178222656,
          0.43570950627326965,
          0.4359089136123657,
          0.4360400438308716,
          0.4359242916107178,
          0.4356818199157715,
          0.4356857240200043,
          0.4355570375919342,
          0.4354780912399292,
          0.4354233145713806,
          0.43478235602378845,
          0.4345034956932068,
          0.4344298541545868,
          0.43427959084510803,
          0.43421053886413574,
          0.4342883825302124,
          0.4342215061187744,
          0.43424490094184875,
          0.43402624130249023,
          0.434012234210968,
          0.4340708255767822,
          0.4340607523918152,
          0.43407830595970154,
          0.4342002868652344,
          0.43441107869148254,
          0.43493887782096863,
          0.4355774521827698,
          0.4356125593185425,
          0.4354991912841797,
          0.4352845251560211,
          0.4352835416793823,
          0.4355761408805847,
          0.43560290336608887,
          0.435588002204895,
          0.43578168749809265,
          0.4359249174594879,
          0.4360661506652832,
          0.4360828697681427,
          0.4360647201538086,
          0.435932457447052,
          0.4360855519771576,
          0.4361721873283386,
          0.4363853931427002,
          0.4364052414894104,
          0.43635353446006775,
          0.43627139925956726,
          0.43634888529777527,
          0.4363704025745392,
          0.43636354804039,
          0.43615609407424927,
          0.4359917640686035,
          0.4359239339828491,
          0.43584179878234863,
          0.43588560819625854,
          0.43608155846595764,
          0.436070054769516,
          0.4360225200653076,
          0.43597212433815,
          0.43586137890815735,
          0.43572577834129333,
          0.43569669127464294,
          0.43592169880867004,
          0.4360349774360657,
          0.4362863004207611,
          0.4363533854484558,
          0.4361795485019684,
          0.4360058307647705,
          0.43584343791007996,
          0.43587470054626465,
          0.4359340965747833,
          0.436008095741272,
          0.4360714852809906,
          0.43622535467147827,
          0.4362351894378662,
          0.4361085593700409,
          0.4360610544681549,
          0.4361695349216461,
          0.43617963790893555,
          0.43609392642974854,
          0.4358160197734833,
          0.43601521849632263,
          0.4363235533237457,
          0.43666332960128784,
          0.43723294138908386,
          0.4372694790363312,
          0.43726682662963867,
          0.43738701939582825,
          0.4375841021537781,
          0.4377624988555908,
          0.4376803934574127,
          0.4373418390750885,
          0.43732255697250366,
          0.4374646544456482,
          0.43750637769699097,
          0.43738609552383423,
          0.43734779953956604,
          0.4376460611820221,
          0.43793022632598877,
          0.4379659593105316,
          0.4376618564128876,
          0.4372830092906952,
          0.43717432022094727,
          0.43746641278266907,
          0.43787646293640137,
          0.43797576427459717,
          0.4375757575035095,
          0.4372425675392151,
          0.4372440278530121,
          0.4372386932373047,
          0.43713346123695374,
          0.4370858073234558,
          0.43707042932510376,
          0.437099814414978,
          0.4371495842933655,
          0.4372153580188751,
          0.4373166263103485,
          0.4374449849128723,
          0.43737244606018066,
          0.43708834052085876,
          0.4368794858455658,
          0.43695998191833496,
          0.4372303783893585,
          0.4371218681335449,
          0.43689313530921936,
          0.43682625889778137,
          0.4366925060749054,
          0.43670374155044556,
          0.43670424818992615,
          0.4366491734981537,
          0.43665435910224915,
          0.43679866194725037,
          0.4370310604572296,
          0.4371209144592285,
          0.4372856318950653,
          0.4374047815799713,
          0.43741580843925476,
          0.43742942810058594,
          0.4373082220554352,
          0.43744099140167236,
          0.4379163682460785,
          0.43767765164375305,
          0.4374901354312897,
          0.43752187490463257,
          0.43758606910705566,
          0.4374367296695709,
          0.43744173645973206,
          0.43744605779647827,
          0.43750208616256714,
          0.4378089904785156,
          0.4384002387523651,
          0.43859925866127014,
          0.4387149214744568,
          0.43875738978385925,
          0.4389271140098572,
          0.438944011926651,
          0.43888917565345764,
          0.43859538435935974,
          0.4384694993495941,
          0.43854638934135437,
          0.4386870861053467,
          0.43866267800331116,
          0.43840742111206055,
          0.4382149577140808,
          0.4381844401359558,
          0.4380996823310852,
          0.4378688335418701,
          0.4375176727771759,
          0.43747854232788086,
          0.4374218285083771,
          0.43737250566482544,
          0.437358558177948,
          0.43725940585136414,
          0.43722280859947205,
          0.43725624680519104,
          0.43778347969055176,
          0.43872323632240295,
          0.4395725131034851,
          0.4400901198387146,
          0.4402751624584198,
          0.44035303592681885,
          0.43994298577308655,
          0.43923455476760864,
          0.4385845363140106,
          0.4380871653556824,
          0.4377236068248749,
          0.43755945563316345,
          0.4374774694442749,
          0.4374110996723175,
          0.4373680055141449,
          0.43740522861480713,
          0.43787088990211487,
          0.43797767162323,
          0.43810999393463135,
          0.43790701031684875,
          0.43760037422180176,
          0.43744227290153503,
          0.43741846084594727,
          0.43743473291397095,
          0.4375065565109253,
          0.43763071298599243,
          0.4376327395439148,
          0.43752050399780273,
          0.4375181496143341,
          0.43759360909461975,
          0.43754732608795166,
          0.43741849064826965,
          0.43733808398246765,
          0.43728506565093994,
          0.4372539818286896,
          0.4372536838054657,
          0.43725821375846863,
          0.4372444450855255,
          0.43726828694343567,
          0.4372628331184387,
          0.43725451827049255,
          0.4372155964374542,
          0.43714118003845215,
          0.43708929419517517,
          0.43707433342933655,
          0.4370724558830261,
          0.4370517134666443,
          0.4370073676109314,
          0.436982661485672,
          0.43695423007011414,
          0.43688204884529114,
          0.4368220865726471,
          0.4367779791355133,
          0.43674689531326294,
          0.4367251396179199,
          0.436737596988678,
          0.4367188513278961,
          0.43667536973953247,
          0.4365963637828827,
          0.43657857179641724,
          0.4365231394767761,
          0.43640193343162537,
          0.4363584816455841,
          0.4363308846950531,
          0.4363273084163666,
          0.4363321363925934,
          0.4363135099411011,
          0.4362870156764984,
          0.4362235963344574,
          0.4361324906349182,
          0.43603986501693726,
          0.4359274208545685,
          0.4358353614807129,
          0.4357927739620209,
          0.43566974997520447,
          0.43539780378341675,
          0.4354669749736786,
          0.43528568744659424,
          0.43528804183006287,
          0.4354128837585449,
          0.4352560341358185,
          0.43530529737472534,
          0.43525034189224243,
          0.43510162830352783,
          0.43497994542121887,
          0.43490731716156006,
          0.43478646874427795,
          0.43471014499664307,
          0.43464645743370056,
          0.4345879852771759,
          0.4345044493675232,
          0.43439674377441406,
          0.43437808752059937,
          0.4343425929546356,
          0.4343719482421875,
          0.43439456820487976,
          0.43436479568481445,
          0.434295654296875,
          0.43427780270576477,
          0.4343002736568451,
          0.434314101934433,
          0.4343377649784088,
          0.43433332443237305,
          0.4342884421348572,
          0.43420788645744324
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_8",
         "type": "scatter",
         "y": [
          0.7762425541877747,
          0.7776783108711243,
          0.7771409153938293,
          0.776922345161438,
          0.7766185998916626,
          0.7760292291641235,
          0.7764089107513428,
          0.7774969935417175,
          0.7773389220237732,
          0.7769221067428589,
          0.7768872380256653,
          0.7767302989959717,
          0.7767496705055237,
          0.7764452695846558,
          0.7758709192276001,
          0.7757059335708618,
          0.7758837938308716,
          0.7758551239967346,
          0.7758311629295349,
          0.775915801525116,
          0.7758205533027649,
          0.775714099407196,
          0.7755885720252991,
          0.7754501700401306,
          0.7754079699516296,
          0.7753767967224121,
          0.7754048705101013,
          0.7754650115966797,
          0.7755332589149475,
          0.7758313417434692,
          0.7761850953102112,
          0.776214063167572,
          0.7760985493659973,
          0.7760106921195984,
          0.7760590314865112,
          0.7762784957885742,
          0.7763628363609314,
          0.7763141989707947,
          0.7764702439308167,
          0.7766717672348022,
          0.7767181396484375,
          0.7767226696014404,
          0.7766671180725098,
          0.7765957713127136,
          0.7766316533088684,
          0.7767969965934753,
          0.7771399021148682,
          0.7774096131324768,
          0.7772054672241211,
          0.7770100831985474,
          0.7772524952888489,
          0.7774345278739929,
          0.7772265076637268,
          0.776776909828186,
          0.7764354944229126,
          0.7763805389404297,
          0.7763389945030212,
          0.7762446403503418,
          0.776313304901123,
          0.7763547897338867,
          0.7762473821640015,
          0.7761696577072144,
          0.7761256694793701,
          0.7761081457138062,
          0.7761780023574829,
          0.7763513922691345,
          0.7766738533973694,
          0.7770918011665344,
          0.7772666811943054,
          0.7770758867263794,
          0.7767521739006042,
          0.7765281200408936,
          0.7766026258468628,
          0.7768645882606506,
          0.7769330143928528,
          0.7769010066986084,
          0.7771267890930176,
          0.7773818373680115,
          0.7774010896682739,
          0.7774503231048584,
          0.7775675654411316,
          0.777489185333252,
          0.7771626114845276,
          0.776730477809906,
          0.7766814231872559,
          0.7776496410369873,
          0.7793800830841064,
          0.7809324264526367,
          0.7817733287811279,
          0.7821645736694336,
          0.7829785346984863,
          0.7843170762062073,
          0.7848962545394897,
          0.7842819094657898,
          0.7836289405822754,
          0.7834535241127014,
          0.7832520008087158,
          0.7827244400978088,
          0.782174825668335,
          0.7823904752731323,
          0.7832757234573364,
          0.7834827899932861,
          0.7826600670814514,
          0.7818284034729004,
          0.7815600037574768,
          0.781715989112854,
          0.7822647094726562,
          0.7829958200454712,
          0.7830681204795837,
          0.7824639678001404,
          0.7822922468185425,
          0.7825555801391602,
          0.7823793888092041,
          0.7818785905838013,
          0.7815033197402954,
          0.7812463045120239,
          0.7810633778572083,
          0.7808763384819031,
          0.780648410320282,
          0.7804846167564392,
          0.7802797555923462,
          0.7797623872756958,
          0.7789844274520874,
          0.7783998250961304,
          0.7781950831413269,
          0.7779505252838135,
          0.7775171995162964,
          0.7772760987281799,
          0.7771943807601929,
          0.7769713401794434,
          0.7767272591590881,
          0.7765626311302185,
          0.7763722538948059,
          0.7762970924377441,
          0.7765077352523804,
          0.7768344879150391,
          0.77698814868927,
          0.7769266963005066,
          0.7768820524215698,
          0.7769610285758972,
          0.7769876718521118,
          0.7769551277160645,
          0.77707839012146,
          0.7772765159606934,
          0.777321457862854,
          0.7773144841194153,
          0.7774394750595093,
          0.7776120901107788,
          0.7777318954467773,
          0.7778878808021545,
          0.7781473994255066,
          0.7785233855247498,
          0.7790901064872742,
          0.7797649502754211,
          0.7802233099937439,
          0.7803431749343872,
          0.7803653478622437,
          0.7804612517356873,
          0.7805083990097046,
          0.7803823351860046,
          0.7801511883735657,
          0.7800096273422241,
          0.7801064252853394,
          0.7803168296813965,
          0.780368983745575,
          0.7802568078041077,
          0.7801455855369568,
          0.7799994349479675,
          0.7797203660011292,
          0.7794525623321533,
          0.7793529033660889,
          0.7793142795562744,
          0.7791668772697449,
          0.7789266109466553,
          0.7786893844604492,
          0.7784847617149353,
          0.778322696685791,
          0.7783739566802979,
          0.7790344953536987,
          0.7805137634277344,
          0.7823504209518433,
          0.7836890816688538,
          0.7840824723243713,
          0.7837271094322205,
          0.7829793691635132,
          0.7820562720298767,
          0.7811159491539001,
          0.7803180813789368,
          0.7797617316246033,
          0.7794462442398071,
          0.7792737483978271,
          0.7791242599487305,
          0.7789753675460815,
          0.7789626121520996,
          0.7791919708251953,
          0.7795273065567017,
          0.779695987701416,
          0.7796293497085571,
          0.7794937491416931,
          0.7794160842895508,
          0.7793809175491333,
          0.7793828248977661,
          0.7794250845909119,
          0.779453456401825,
          0.7794193625450134,
          0.7793843746185303,
          0.7794260382652283,
          0.7795006632804871,
          0.7794839143753052,
          0.7793400883674622,
          0.7791542410850525,
          0.7790126800537109,
          0.7789439558982849,
          0.7789179086685181,
          0.7789105176925659,
          0.7789329886436462,
          0.7789674401283264,
          0.778953492641449,
          0.7788490653038025,
          0.7786707282066345,
          0.7784770131111145,
          0.778315007686615,
          0.7781999111175537,
          0.7781007885932922,
          0.7779713273048401,
          0.7778071761131287,
          0.7776519060134888,
          0.777522087097168,
          0.7773913145065308,
          0.7772494554519653,
          0.777113139629364,
          0.7770166397094727,
          0.7769800424575806,
          0.7769733667373657,
          0.7769269347190857,
          0.7768165469169617,
          0.7767007350921631,
          0.7766315340995789,
          0.7765750288963318,
          0.7764790654182434,
          0.7763802409172058,
          0.7763507962226868,
          0.7763752937316895,
          0.7763727307319641,
          0.776313304901123,
          0.7762306928634644,
          0.7761440873146057,
          0.7760496139526367,
          0.7759652733802795,
          0.7759092450141907,
          0.7758713960647583,
          0.7758392691612244,
          0.7758178114891052,
          0.7758004665374756,
          0.7757713794708252,
          0.7757431268692017,
          0.7757458686828613,
          0.775777280330658,
          0.7757959365844727,
          0.7757730484008789,
          0.7757241725921631,
          0.7756819725036621,
          0.7756587266921997,
          0.7756452560424805,
          0.7756292223930359,
          0.7756076455116272,
          0.7755829691886902,
          0.7755540609359741,
          0.7755188345909119,
          0.775485634803772,
          0.7754738330841064,
          0.775489866733551,
          0.7755142450332642,
          0.775520920753479,
          0.775505542755127,
          0.7754861116409302,
          0.7754818797111511,
          0.7754970788955688,
          0.775519609451294,
          0.7755292057991028,
          0.7755135893821716,
          0.775479793548584,
          0.7754449844360352
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_8",
         "type": "scatter",
         "y": [
          0.7923147678375244,
          0.7944498658180237,
          0.7943399548530579,
          0.7942934632301331,
          0.7942816615104675,
          0.7939311861991882,
          0.7941305637359619,
          0.7942616939544678,
          0.794145941734314,
          0.7939034700393677,
          0.7939074039459229,
          0.7937787175178528,
          0.7936997413635254,
          0.7936450242996216,
          0.793004035949707,
          0.792725145816803,
          0.7926515340805054,
          0.7925012707710266,
          0.7924322485923767,
          0.7925100326538086,
          0.7924431562423706,
          0.7924665808677673,
          0.7922478914260864,
          0.7922338843345642,
          0.7922924757003784,
          0.7922824025154114,
          0.7922999858856201,
          0.7924219369888306,
          0.7926327586174011,
          0.7931605577468872,
          0.793799102306366,
          0.7938342094421387,
          0.7937208414077759,
          0.7935062050819397,
          0.7935052514076233,
          0.7937977910041809,
          0.7938245534896851,
          0.7938096523284912,
          0.7940033674240112,
          0.7941465973854065,
          0.7942878603935242,
          0.7943045496940613,
          0.7942863702774048,
          0.7941541075706482,
          0.7943072319030762,
          0.7943938374519348,
          0.7946070432662964,
          0.7946268916130066,
          0.7945752143859863,
          0.7944930791854858,
          0.7945705652236938,
          0.7945920825004578,
          0.7945852279663086,
          0.7943777441978455,
          0.7942134141921997,
          0.7941456437110901,
          0.7940634489059448,
          0.7941073179244995,
          0.7943032383918762,
          0.7942917346954346,
          0.7942442297935486,
          0.7941938042640686,
          0.7940830588340759,
          0.7939474582672119,
          0.7939183712005615,
          0.7941433787345886,
          0.7942566871643066,
          0.7945079803466797,
          0.794575035572052,
          0.794401228427887,
          0.7942275404930115,
          0.7940651178359985,
          0.7940963506698608,
          0.7941557765007019,
          0.7942297458648682,
          0.7942931652069092,
          0.7944470643997192,
          0.7944568395614624,
          0.7943302392959595,
          0.7942827343940735,
          0.7943912148475647,
          0.7944013476371765,
          0.7943156361579895,
          0.7940376996994019,
          0.7942368984222412,
          0.7945452332496643,
          0.7948850393295288,
          0.7954546213150024,
          0.7954911589622498,
          0.7954885363578796,
          0.7956086993217468,
          0.795805811882019,
          0.795984148979187,
          0.7959020733833313,
          0.7955635190010071,
          0.7955442070960999,
          0.7956863045692444,
          0.7957280874252319,
          0.7956078052520752,
          0.7955694794654846,
          0.7958677411079407,
          0.796151876449585,
          0.7961876392364502,
          0.7958835363388062,
          0.7955046892166138,
          0.7953959703445435,
          0.7956880927085876,
          0.7960981726646423,
          0.7961974143981934,
          0.7957974672317505,
          0.795464277267456,
          0.7954657077789307,
          0.7954604029655457,
          0.7953551411628723,
          0.7953075170516968,
          0.7952920794487,
          0.795321524143219,
          0.7953712344169617,
          0.7954370379447937,
          0.7955383062362671,
          0.7956666350364685,
          0.7955941557884216,
          0.7953100204467773,
          0.7951011657714844,
          0.7951816320419312,
          0.7954520583152771,
          0.7953435778617859,
          0.7951148152351379,
          0.7950479388237,
          0.794914186000824,
          0.7949253916740417,
          0.7949259281158447,
          0.7948708534240723,
          0.7948760390281677,
          0.795020341873169,
          0.7952527403831482,
          0.7953425645828247,
          0.7955073118209839,
          0.7956264615058899,
          0.7956374883651733,
          0.7956510782241821,
          0.7955299019813538,
          0.7956626415252686,
          0.7961380481719971,
          0.7958993315696716,
          0.7957118153572083,
          0.7957435846328735,
          0.7958077788352966,
          0.7956584095954895,
          0.7956634163856506,
          0.7956677675247192,
          0.7957237958908081,
          0.7960306406021118,
          0.7966219186782837,
          0.7968209385871887,
          0.796936571598053,
          0.7969790697097778,
          0.7971487641334534,
          0.7971656918525696,
          0.7971108555793762,
          0.7968170642852783,
          0.7966911792755127,
          0.796768069267273,
          0.7969087958335876,
          0.7968843579292297,
          0.7966291308403015,
          0.7964366674423218,
          0.796406090259552,
          0.7963213324546814,
          0.7960904836654663,
          0.7957393527030945,
          0.795700192451477,
          0.7956435084342957,
          0.7955942153930664,
          0.795580267906189,
          0.7954810857772827,
          0.7954444885253906,
          0.7954779267311096,
          0.796005129814148,
          0.7969449162483215,
          0.7977942228317261,
          0.7983117699623108,
          0.7984968423843384,
          0.798574686050415,
          0.7981646656990051,
          0.7974562048912048,
          0.7968062162399292,
          0.7963088154792786,
          0.7959452867507935,
          0.795781135559082,
          0.7956991195678711,
          0.7956327795982361,
          0.7955896854400635,
          0.7956268787384033,
          0.7960925698280334,
          0.7961993217468262,
          0.7963316440582275,
          0.7961286902427673,
          0.795822024345398,
          0.7956639528274536,
          0.7956401705741882,
          0.7956564426422119,
          0.7957282662391663,
          0.7958524227142334,
          0.795854389667511,
          0.7957422137260437,
          0.7957398295402527,
          0.7958152890205383,
          0.7957690358161926,
          0.7956401705741882,
          0.7955597639083862,
          0.7955067157745361,
          0.7954756617546082,
          0.7954753637313843,
          0.7954798936843872,
          0.7954661250114441,
          0.7954899668693542,
          0.7954845428466797,
          0.7954761981964111,
          0.7954372763633728,
          0.7953628301620483,
          0.7953109741210938,
          0.7952960133552551,
          0.7952941656112671,
          0.7952734231948853,
          0.7952290773391724,
          0.7952043414115906,
          0.7951759099960327,
          0.7951037287712097,
          0.7950437664985657,
          0.7949996590614319,
          0.7949685454368591,
          0.7949467897415161,
          0.7949592471122742,
          0.7949405312538147,
          0.7948970794677734,
          0.7948180437088013,
          0.7948002815246582,
          0.7947448492050171,
          0.794623613357544,
          0.7945801615715027,
          0.7945525646209717,
          0.7945489883422852,
          0.794553816318512,
          0.7945351600646973,
          0.794508695602417,
          0.794445276260376,
          0.7943541407585144,
          0.7942615151405334,
          0.7941491007804871,
          0.7940570712089539,
          0.7940144538879395,
          0.793891429901123,
          0.7936194539070129,
          0.7936886548995972,
          0.7935073375701904,
          0.7935097217559814,
          0.7936345934867859,
          0.7934777140617371,
          0.7935270071029663,
          0.7934719920158386,
          0.7933233380317688,
          0.7932016253471375,
          0.793129026889801,
          0.7930081486701965,
          0.792931854724884,
          0.7928681373596191,
          0.7928096652030945,
          0.7927260994911194,
          0.792618453502655,
          0.7925997376441956,
          0.7925642728805542,
          0.7925935983657837,
          0.7926162481307983,
          0.7925864458084106,
          0.792517364025116,
          0.7924994826316833,
          0.7925219535827637,
          0.7925357818603516,
          0.7925594449043274,
          0.7925549745559692,
          0.7925100922584534,
          0.7924295663833618
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "gt_9",
         "type": "scatter",
         "y": [
          0.14924989640712738,
          0.1519104540348053,
          0.15164649486541748,
          0.15135881304740906,
          0.15204472839832306,
          0.1516747921705246,
          0.15132735669612885,
          0.1515282839536667,
          0.15122634172439575,
          0.1510796993970871,
          0.15105679631233215,
          0.1511939913034439,
          0.1513611525297165,
          0.15140217542648315,
          0.1511026918888092,
          0.15062831342220306,
          0.15069086849689484,
          0.15058180689811707,
          0.15018300712108612,
          0.15000350773334503,
          0.14988303184509277,
          0.14999563992023468,
          0.14997079968452454,
          0.14976520836353302,
          0.14969003200531006,
          0.14955636858940125,
          0.14935502409934998,
          0.14923763275146484,
          0.149222269654274,
          0.14933395385742188,
          0.14951328933238983,
          0.14947959780693054,
          0.14942820370197296,
          0.1495417058467865,
          0.1496313512325287,
          0.1496526300907135,
          0.14960819482803345,
          0.1494334191083908,
          0.1494230479001999,
          0.14959672093391418,
          0.14965008199214935,
          0.14977526664733887,
          0.150018110871315,
          0.15010952949523926,
          0.15006166696548462,
          0.15013091266155243,
          0.1502762883901596,
          0.1502145677804947,
          0.15006528794765472,
          0.15005703270435333,
          0.1502189040184021,
          0.1505085676908493,
          0.15063776075839996,
          0.1506260633468628,
          0.15066397190093994,
          0.15063637495040894,
          0.15063408017158508,
          0.15091197192668915,
          0.15121755003929138,
          0.1511194407939911,
          0.15101781487464905,
          0.15124017000198364,
          0.15147309005260468,
          0.15149249136447906,
          0.15154466032981873,
          0.1519981026649475,
          0.15272323787212372,
          0.1533723771572113,
          0.1536238044500351,
          0.15335123240947723,
          0.15281842648983002,
          0.15242505073547363,
          0.15241554379463196,
          0.1526651680469513,
          0.15289001166820526,
          0.15314564108848572,
          0.1534184068441391,
          0.1532800942659378,
          0.15284955501556396,
          0.15264347195625305,
          0.1526496559381485,
          0.1527240127325058,
          0.15267066657543182,
          0.1524486094713211,
          0.15246078372001648,
          0.1527983546257019,
          0.15309831500053406,
          0.15321098268032074,
          0.15328256785869598,
          0.1533704400062561,
          0.1534094512462616,
          0.1535302996635437,
          0.15375414490699768,
          0.153771311044693,
          0.15360954403877258,
          0.15363605320453644,
          0.1538943648338318,
          0.15402863919734955,
          0.15388593077659607,
          0.15389017760753632,
          0.15428230166435242,
          0.15466098487377167,
          0.15463696420192719,
          0.15420708060264587,
          0.15371771156787872,
          0.1536055952310562,
          0.15398448705673218,
          0.15450772643089294,
          0.15452860295772552,
          0.15396256744861603,
          0.15358100831508636,
          0.15368351340293884,
          0.15371665358543396,
          0.15350507199764252,
          0.15338195860385895,
          0.15343424677848816,
          0.15353742241859436,
          0.15366919338703156,
          0.1538311094045639,
          0.1540267914533615,
          0.1542675495147705,
          0.1543487161397934,
          0.15410295128822327,
          0.15389108657836914,
          0.15407249331474304,
          0.154293954372406,
          0.15418021380901337,
          0.15393899381160736,
          0.15378400683403015,
          0.1537826955318451,
          0.15398664772510529,
          0.15405628085136414,
          0.15372057259082794,
          0.15343844890594482,
          0.1537066102027893,
          0.15428999066352844,
          0.15456177294254303,
          0.15430304408073425,
          0.15395712852478027,
          0.15385983884334564,
          0.1537645161151886,
          0.1536729484796524,
          0.15388354659080505,
          0.15406061708927155,
          0.15380172431468964,
          0.1535174697637558,
          0.15354707837104797,
          0.15354996919631958,
          0.1533389687538147,
          0.15316639840602875,
          0.15313869714736938,
          0.1531628519296646,
          0.153239905834198,
          0.153372123837471,
          0.15349236130714417,
          0.15357892215251923,
          0.15363141894340515,
          0.15364021062850952,
          0.15358668565750122,
          0.15346094965934753,
          0.15332065522670746,
          0.15328684449195862,
          0.1533549726009369,
          0.1533408761024475,
          0.15315568447113037,
          0.15296097099781036,
          0.15291252732276917,
          0.1529560536146164,
          0.15295937657356262,
          0.15286870300769806,
          0.1527072787284851,
          0.15255673229694366,
          0.15251199901103973,
          0.1525799185037613,
          0.15262755751609802,
          0.15253102779388428,
          0.15238110721111298,
          0.1523994505405426,
          0.15264251828193665,
          0.15295352041721344,
          0.15318718552589417,
          0.15333867073059082,
          0.1534639149904251,
          0.15357482433319092,
          0.15361960232257843,
          0.1535382866859436,
          0.15330389142036438,
          0.15296457707881927,
          0.15264736115932465,
          0.1524471938610077,
          0.15235157310962677,
          0.15228679776191711,
          0.15222638845443726,
          0.15222403407096863,
          0.15231488645076752,
          0.15242263674736023,
          0.15241853892803192,
          0.15228496491909027,
          0.15213638544082642,
          0.15206141769886017,
          0.15206508338451385,
          0.15214112401008606,
          0.15224894881248474,
          0.15227016806602478,
          0.15215890109539032,
          0.15202844142913818,
          0.1519934982061386,
          0.15203532576560974,
          0.15206024050712585,
          0.15202265977859497,
          0.15195897221565247,
          0.1519327163696289,
          0.15195776522159576,
          0.15197689831256866,
          0.15195178985595703,
          0.15191026031970978,
          0.15188200771808624,
          0.15185096859931946,
          0.15180249512195587,
          0.1517597734928131,
          0.1517389565706253,
          0.15171432495117188,
          0.15165738761425018,
          0.15157976746559143,
          0.15152542293071747,
          0.15151561796665192,
          0.15152177214622498,
          0.15150105953216553,
          0.15144884586334229,
          0.15138645470142365,
          0.15133564174175262,
          0.15132160484790802,
          0.1513528823852539,
          0.15137766301631927,
          0.15133415162563324,
          0.15124225616455078,
          0.15116901695728302,
          0.15112267434597015,
          0.15106463432312012,
          0.1509963870048523,
          0.15095290541648865,
          0.15091846883296967,
          0.15085162222385406,
          0.1507692188024521,
          0.1507115513086319,
          0.15066221356391907,
          0.15057140588760376,
          0.15044575929641724,
          0.1503439098596573,
          0.15030071139335632,
          0.15031060576438904,
          0.15034836530685425,
          0.15036451816558838,
          0.15029586851596832,
          0.150146484375,
          0.15002192556858063,
          0.15000703930854797,
          0.15006183087825775,
          0.15008273720741272,
          0.1500319242477417,
          0.14996092021465302,
          0.1499253213405609,
          0.14992696046829224,
          0.1499270349740982,
          0.14990003407001495,
          0.14987270534038544,
          0.14989446103572845,
          0.1499650776386261,
          0.1500120311975479,
          0.1499616503715515,
          0.14982619881629944,
          0.14968660473823547,
          0.1496027708053589,
          0.14957675337791443,
          0.14958447217941284,
          0.1495988816022873,
          0.1495835781097412,
          0.14952237904071808,
          0.14945541322231293,
          0.14943827688694,
          0.14946334064006805,
          0.14945673942565918,
          0.14938391745090485
         ]
        },
        {
         "marker": {
          "size": 3
         },
         "mode": "markers",
         "name": "pred_9",
         "type": "scatter",
         "y": [
          0.1558350771665573,
          0.15797017514705658,
          0.15786027908325195,
          0.15781372785568237,
          0.15780192613601685,
          0.15745149552822113,
          0.1576508730649948,
          0.15778201818466187,
          0.15766626596450806,
          0.15742379426956177,
          0.15742769837379456,
          0.1572990119457245,
          0.15722006559371948,
          0.1571652889251709,
          0.15652433037757874,
          0.15624547004699707,
          0.15617181360721588,
          0.15602156519889832,
          0.15595251321792603,
          0.15603035688400269,
          0.1559634804725647,
          0.15598687529563904,
          0.15576821565628052,
          0.1557541936635971,
          0.1558127999305725,
          0.15580272674560547,
          0.15582029521465302,
          0.15594226121902466,
          0.15615305304527283,
          0.1566808521747589,
          0.15731942653656006,
          0.15735453367233276,
          0.15724115073680878,
          0.1570264995098114,
          0.1570255309343338,
          0.1573181003332138,
          0.15734487771987915,
          0.1573299765586853,
          0.15752366185188293,
          0.1576668918132782,
          0.1578081250190735,
          0.15782484412193298,
          0.15780669450759888,
          0.15767443180084229,
          0.15782751142978668,
          0.1579141616821289,
          0.15812736749649048,
          0.15814721584320068,
          0.15809550881385803,
          0.15801337361335754,
          0.15809085965156555,
          0.15811239182949066,
          0.1581055074930191,
          0.15789805352687836,
          0.1577337235212326,
          0.1576659232378006,
          0.15758375823497772,
          0.15762759745121002,
          0.15782354772090912,
          0.15781202912330627,
          0.1577645093202591,
          0.1577140986919403,
          0.15760335326194763,
          0.15746775269508362,
          0.15743866562843323,
          0.15766367316246033,
          0.15777695178985596,
          0.15802828967571259,
          0.1580953598022461,
          0.15792152285575867,
          0.1577478051185608,
          0.15758541226387024,
          0.15761665999889374,
          0.1576760709285736,
          0.15775007009506226,
          0.15781347453594208,
          0.15796732902526855,
          0.1579771637916565,
          0.15785054862499237,
          0.157803013920784,
          0.1579115241765976,
          0.15792161226272583,
          0.1578359156847,
          0.15755799412727356,
          0.15775719285011292,
          0.15806551277637482,
          0.15840531885623932,
          0.15897490084171295,
          0.15901145339012146,
          0.15900880098342896,
          0.15912900865077972,
          0.15932609140872955,
          0.1595044583082199,
          0.1594223827123642,
          0.15908382833003998,
          0.15906453132629395,
          0.15920662879943848,
          0.15924836695194244,
          0.1591280698776245,
          0.15908977389335632,
          0.15938802063465118,
          0.15967218577861786,
          0.1597079336643219,
          0.15940383076667786,
          0.15902498364448547,
          0.15891629457473755,
          0.15920840203762054,
          0.15961843729019165,
          0.15971772372722626,
          0.1593177318572998,
          0.15898455679416656,
          0.15898600220680237,
          0.15898066759109497,
          0.15887542068958282,
          0.1588277816772461,
          0.15881240367889404,
          0.1588417887687683,
          0.15889155864715576,
          0.1589573174715042,
          0.1590586006641388,
          0.1591869443655014,
          0.15911443531513214,
          0.15883032977581024,
          0.15862144529819489,
          0.15870194137096405,
          0.1589723527431488,
          0.1588638424873352,
          0.15863512456417084,
          0.15856821835041046,
          0.15843448042869568,
          0.15844571590423584,
          0.15844622254371643,
          0.15839116275310516,
          0.15839633345603943,
          0.15854063630104065,
          0.1587730199098587,
          0.1588628888130188,
          0.1590276062488556,
          0.1591467708349228,
          0.15915778279304504,
          0.15917138755321503,
          0.15905019640922546,
          0.15918295085430145,
          0.15965832769870758,
          0.15941964089870453,
          0.15923212468624115,
          0.15926384925842285,
          0.15932805836200714,
          0.1591787040233612,
          0.15918371081352234,
          0.15918804705142975,
          0.15924406051635742,
          0.1595509648323059,
          0.1601422131061554,
          0.16034124791622162,
          0.16045689582824707,
          0.16049936413764954,
          0.16066908836364746,
          0.16068598628044128,
          0.16063115000724792,
          0.16033735871315002,
          0.1602114737033844,
          0.16028836369514465,
          0.16042907536029816,
          0.16040465235710144,
          0.16014941036701202,
          0.1599569469690323,
          0.1599264144897461,
          0.1598416417837143,
          0.1596107929944992,
          0.1592596471309662,
          0.15922050178050995,
          0.15916380286216736,
          0.15911449491977692,
          0.15910053253173828,
          0.15900136530399323,
          0.15896479785442352,
          0.15899823606014252,
          0.15952543914318085,
          0.16046521067619324,
          0.1613144874572754,
          0.1618320792913437,
          0.1620171219110489,
          0.16209499537944794,
          0.16168496012687683,
          0.16097651422023773,
          0.1603264957666397,
          0.15982912480831146,
          0.15946559607982635,
          0.15930142998695374,
          0.159219428896904,
          0.15915308892726898,
          0.15910997986793518,
          0.15914718806743622,
          0.15961287915706635,
          0.15971964597702026,
          0.15985195338726044,
          0.15964899957180023,
          0.15934234857559204,
          0.15918424725532532,
          0.15916043519973755,
          0.15917672216892242,
          0.15924854576587677,
          0.1593727022409439,
          0.15937469899654388,
          0.1592624932527542,
          0.1592601239681244,
          0.15933559834957123,
          0.15928930044174194,
          0.15916046500205994,
          0.15908004343509674,
          0.15902702510356903,
          0.15899595618247986,
          0.15899565815925598,
          0.1590002030134201,
          0.158986434340477,
          0.15901026129722595,
          0.159004807472229,
          0.15899650752544403,
          0.1589575856924057,
          0.15888315439224243,
          0.15883126854896545,
          0.15881630778312683,
          0.1588144451379776,
          0.15879368782043457,
          0.15874934196472168,
          0.1587246209383011,
          0.15869620442390442,
          0.15862402319908142,
          0.15856406092643738,
          0.1585199385881424,
          0.15848886966705322,
          0.1584671139717102,
          0.15847955644130707,
          0.1584608256816864,
          0.15841735899448395,
          0.15833832323551178,
          0.15832054615020752,
          0.1582651287317276,
          0.15814389288425446,
          0.1581004410982132,
          0.15807285904884338,
          0.15806929767131805,
          0.15807411074638367,
          0.15805548429489136,
          0.1580290049314499,
          0.1579655557870865,
          0.1578744649887085,
          0.15778182446956635,
          0.15766939520835876,
          0.15757735073566437,
          0.15753474831581116,
          0.15741170942783356,
          0.15713976323604584,
          0.15720893442630768,
          0.15702766180038452,
          0.15703000128269196,
          0.1571548581123352,
          0.15699802339076996,
          0.15704728662967682,
          0.15699231624603271,
          0.1568436175584793,
          0.15672191977500916,
          0.15664929151535034,
          0.15652844309806824,
          0.15645211935043335,
          0.15638843178749084,
          0.1563299596309662,
          0.15624642372131348,
          0.15613871812820435,
          0.15612004697322845,
          0.1560845822095871,
          0.15611392259597778,
          0.15613652765750885,
          0.15610677003860474,
          0.15603762865066528,
          0.15601977705955505,
          0.15604224801063538,
          0.15605607628822327,
          0.1560797393321991,
          0.15607529878616333,
          0.15603040158748627,
          0.15594986081123352
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calcStats(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(test_batch[0])\n",
    "print('overall',(log_likelihood_maxScaling(test_batch[1],outputs)))\n",
    "#for i in range(batch_size):\n",
    "#    print(f'batch {i}',(log_likelihood_maxScaling(batch[1][i,:],outputs[i:i+1,:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in test_dataset:\n",
    "    outputs = model.predict(x)\n",
    "    outputs[:,0:283,0]=y\n",
    "    outputs[:,283,0] = 0\n",
    "    m = mse(y,outputs)\n",
    "    s = tf.exp(outputs[:,0:283,1])*maxTrainLabels\n",
    "    print(np.mean(m), np.mean(s), np.min(s),np.max(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(4):#[2,6,10,20,100]:\n",
    "    fig.add_trace(go.Scatter(y=batch[0][i,:,0,0],mode='markers',name=f'f_{i}',marker=dict(size=3)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.predict(batch[0])\n",
    "pred = outputs[:,0:283,:]\n",
    "pred[:,:,0] = pred[:,:,0]+outputs[:,283,0:1]\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(10): #range(12):# \n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): #range(12):#\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

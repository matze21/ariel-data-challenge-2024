{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf0 = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf0 = labelDf0.set_index('planet_id')\n",
    "labelDf0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.6):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "meanLabels = np.mean(labelDf.mean())\n",
    "stdLabels = np.std(labelDf.std())\n",
    "maxLabels = np.max(labelDf.max())\n",
    "minLabels = np.min(labelDf.min())\n",
    "\n",
    "trainLabels = labelDf.loc[[int(star) for star in train_stars]]\n",
    "meanTrainLabels = np.mean(trainLabels.mean())\n",
    "stdTrainLabels = np.std(trainLabels.std())\n",
    "maxTrainLabels = np.max(trainLabels.max())\n",
    "minTrainLabels = np.min(trainLabels.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTrainLabels)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrain(train_stars):\n",
    "    i = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,:]\n",
    "            if i ==0:\n",
    "                mean = np.mean(x,axis=(0))\n",
    "                sumS = np.sum(x**2,axis=0)\n",
    "            else:\n",
    "                mean = mean + np.mean(x, axis=(0))\n",
    "                sumS += np.sum(x**2,axis=0)\n",
    "            i=i+1\n",
    "    meanTrain = mean / i\n",
    "    stdTrain = np.sqrt(sumS / (i*x.shape[0]) - meanTrain**2)    \n",
    "    return meanTrain, stdTrain\n",
    "meanTrain, stdTrain = calcMeanAndStdOfTrain(train_stars)\n",
    "\n",
    "def normalize_over_train(features, labels):\n",
    "    features = (features - meanTrain) / (stdTrain + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "# normalize over time per samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrainPerStar(x):\n",
    "    mean = np.mean(x,axis=(0))\n",
    "    sumS = np.sum(x**2,axis=0)\n",
    "    stdTrain = np.sqrt(sumS / (x.shape[0]) - mean**2)    \n",
    "    return mean, stdTrain\n",
    "def normalize_per_sample(features, labels):\n",
    "    m,s = calcMeanAndStdOfTrainPerStar(features)\n",
    "    features = (features) / (s + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            features = np.reshape(features,(-1,25,283,4))\n",
    "            features = np.mean(features,axis=1)\n",
    "            #features, labels = normalize_per_sample(features,labels)\n",
    "            features, labels = normalize_over_train(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([225, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283])))) #5625\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('helpers_origiData.npz',meanTrain=meanTrain, stdTrain=stdTrain,meanLabels=meanLabels,stdLabels=stdLabels,maxTrainLabels=maxTrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStars=[821591986,3213637048,2816803663,3875726419,2020870068,553530207,3584299946,3082702832,497611894,396266265,3421574015,1594641765,2250572533,17002355,2066433619,24135240,598943473,3943521076,1907037428,3464855505,1528112893,2737173563,3957394378,2346223738,1949835946,3342950340,1946982092,265537422,235073610,2890766898,2791509259,2675006243,1362719118,1161253140,2061358467,2731744229,2026728872,3247773632,3603727260,2999629755,782689910,3838183950,1240300219,2325932273,3257778880,3163168710,1851017591,1093580069,1318438045,2561946317,3283934223,1806781124,636190362,312842524,3501979807,1438611471,453007076,1539890953,4055463712,2911173444,3074485005,2016150489,1653686448,2720835155,2887450026,2762956949,4094260407,3625059830,1583438553,4046238097,3143365022,2452762851,2074815590,1508325713,3247461698,640765770,3267648621,229701106,1388423740,1071978212,1859732941,4093882471,3861091963,2027199735,112782545,772915060,2895446126,2790829338,966155807,4013505860,3290009033,3866291478,2074140845,1656580533,1839665471,3724419524,2780013210,1340895569,1055599128,1579203925,1981364273,1495061258,3760333362,3032660829,773356318,3959779372,3103511751,93311500,1901457679,2132729888,4156643187,2253206868,2206519853,3685062779,3391288154,3137638902,1965638805,377187848,3952673580,1101707909,2465743719,887347899,1114710037,1803632865,2758726928,3482461315,1604027523,3937238283,2364740784,1971619027,2503200730,2894958665,1540575349,1990548959,3689192911,1867440938,3034529013,3796624928,1723375224,2982016260,4176562407,3475796311,3101987149,3642128364,3214557574,3109601437,1963502034,3892132996,2811219064,2633183716,1609759068,2570413614,1909239671,1068095157,1693505406,3885613633,3671097679,4011881058,998289603,3960808611,1993590741,239407592,1720388855,463235157,3274607066,3983854782,2233970670,1012571429,2894861895,193269369,3187674344,2941815911,1080605842,1439196998,210689998,2545226729,749245819,1542505990,3117487606,1429910325,1944230894,4154084890,1060351656,3779392391,168297458,3932146472,3035295868,2276622978,1842626173,1592156923,1885744867,2713826507,2009927905,777145011,2968910705,2081786992,1011759019,2237942433,496472369,141323216,1390687693,901361366,3705719250,1035787464,1236398737,1254987835,3483698151,1954167669,1226886844,735692747,3912318842,540395971,1040089248,1725624016,2558442317,3270929768,2137970284,2870915117,2561838459,2550833762,3936719360,2575579664,2702825697,835158662,2914588022,1924898052,2899982705,3842343027,3699474491,1788675439,846551798,62282335,3953028309,1754973667,1636090864,1770216760,4246618328,2047136560,2609107638,1218579540,4199801857,4249337798,4257395405,3734125987,3763353910,233675174,1285749933,2913196659,33548644,3962521565,302857654,1649111978,2194084083,2455073775,61961538,2906452125,155746318,2601096321,286376329,1798931901,603237272,1431415873,1954665634,3271952720,137536026,3503831607,1802809018,3597960801,57518461]\n",
    "testStars=[4167675411,1437547087,2692159256,3235958758,3227501674,208669662,1482309188,521954473,342497505,1438554421,1891663121,4165611714,273602018,1711683835,1082014264,1394990039,1101079594,421386193,619448599,3888934497,1122354479,557327341,2660295140,2384398114,3182552250,1771027672,1732981508,3961748982,159506197,2869539781,565785039,2684704287,2758941622,1313620170,724899303,1406894216,1297419156,3308013333,3691267724,2817108197,1819058753,3747847047,386885929,2536093666,2958627654,178016934,2853876513,1980994560,343035647,1662041072,1934262433,2930443698,2385019512,3340984301,25070640,3124355137,1284352058,2571663188,2758428682,3756521544,1012409820,1205725641,1817510468,521001195,3545994138,3531723440,534940522,785834,1656095778,3082264358,2034314944,3339424565,4046148192,1705473070,92029723,827488781,2465063333,2858591687,2636709977,2573514144,2751412614,1087365206,2927554342,301818265,2590423296,3713067054,3329685364,3718288752,3636815200,2885306803,165516591,2660562218,1184823185,902166631,1227146905,2386941703,3963820198,469234852,3311807250,2287863054,4141729806,26372015,511262612,3023519715,1854569585,847875671,3000955010,2530918707,637988352,1806321292,1129361124,1604366860,3277793728,14485303,2314241972,77523557,2995259050,2863140027,2910555879,3269061542,615304059,4286133832,2224327373,1718377842,2025689074,1502650989,1005054328,2040852024,1994184718,544877920,3737988195,3913138351,3539478092,3454679764,1504537906,4017761584,1021609935,3338393823,1264467043,3039307861,989333577,830053386,2845050954,985957621,3459443535,2193939147,1215971796,3284427611,4197469272,2237015181,746498582,3405376171,696618104,2197931868,50637799,3452964090,1569258985,737266850,1315791861,3207402559,1137318614,3704372959,2042816882,2993461427,1277115141,2821963592,2849517986,79009616,864681126,2248456905,3796601559,2062797353,385995586,1947998963,793844575,2862728730,1273256942,4196709764,1569213600,2059081577,3215997593,3478404079,3135808055,2254353101,1661748415,262048960,3472047977,2597490105,4228013544,3804631547,506349321,1599653349,2303635924,2877156894,855962840,1655967875,2445749116,3678150269,3399621278,1076727237,4012276852,3549820907,1173280312,3690278318,90833891,514590299,1196124605,29348276,4150857627,4185365244,3693798020,4133293270,2560123809,2644487066,1044371083,1907771348,514678999,1629044074,1669788725,2217132592,1370642288,2450147796,1989641334,421791107,3538208212,708534821,2877883556,2039381353,795613079,2954316800,1583591551,2899498670,611290582,3041422882,53735941,3983871342,3673677284,3750875292,506148838,2708602782,3441268113,75219649,4273166473,1887580996,3710453865,1697530312,1413437034,3771476325,1676288765,1510814158,2295018082,730244369,1347888499,2291997915,2734053664,2986259922,557779238,1085856726,3795252140,2838808228,100468857,1872623256,303438165,4180636816,848292925,401017528,2670442737,1598287691,391267577,1949187031,3875101730,4247918843,1012051641,612015401,1859679770,3432603686,723247337,998050736,807484101,743163637,57669658,1645275092,1053056651,2372647732,3689245843,4017291223,3059098383,3094871953,1392362496,811352473,1841403352,818295678,1608928178,1273237025,1544052178,3913207660,3838792377,1762835180,528183114,3193829447,720318091,3385600500,2466007563,4138071307,2333658941,957610605,2486025247,1785765384,3625877959,3175562842,3874271955,2086295868,2764364262,473550121,208206471,1396564311,2506720535,3210028778,277905930,2227472043,912497889,283919092,1338777429,2178609948,4247255382,2515050523,3056887059,3648438690,1885277587,4080013326,968882953,1910935418,1429936612,2506277319,2078072980,3023703329,2939180626,1877571378,3575998926,2994980578,803524404,890677371,780066751,1583874489,1468955155,2630811963,1261712158,222583025,1420180584,2015045591,525433641,2982060809,2641456641,4205863992,145136829,3046407287,1671135782,4096233594,3633474854,1242413576,2470022534,704002160,2898410358,2962442845,1574318756,3058413680,944701519,1548764506,2128229789,2883014146,1852150662,824687737,2891311453,919342592,1864259668,1017422390,2560873375,585737420,3533872876,2189033555,3308226296,941186385,2035492415,4266129805,1127135463,688695194,3904845904,2247624106,2161819831,1554937367,113675418,1151088909,333122533,595119503,1537427309,4175492103,1767944001,1585014334,2196974738,2320457425,2556934812,871770446,1615603560,1121250116]\n",
    "\n",
    "common=list(set(trainStars) & set(testStars))\n",
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in test_dataset:\n",
    "    #print(x.shape, y.shape)\n",
    "    a=0\n",
    "    #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = 225\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "\n",
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        return x\n",
    "    \n",
    "class reduce(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        mean = tf.expand_dims(mean, axis=-1)\n",
    "        return mean\n",
    "class reduce1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        return mean\n",
    "\n",
    "\n",
    "def cnnM(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1]\n",
    "\n",
    "\n",
    "    #x = Reshape11()(x)\n",
    "    dim = timepoints\n",
    "    for i in range(3):\n",
    "        x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(5), padding='valid')(x)\n",
    "        x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    #mean = tf.keras.layers.Dense(1,activation='relu')(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    #x_pred = x_pred+mean\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = cnnM() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "test_batch = next(iter(test_dataset))\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_zScoreTarget(y_trueZScore, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueZScore * stdTrainLabels + meanTrainLabels   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predZScore = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predZScore * stdTrainLabels + meanTrainLabels\n",
    "    stdDev = tf.exp(log_sigma)*stdTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = log_sigma + tf.math.log(stdTrainLabels)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / stdDev)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels*stdLabels) + tf.square(y_true-meanLabels) / stdLabels / stdLabels)   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal)*283*5625 - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScaling(y_trueMax, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = tf.math.log(sigma*sigma)# + tf.math.log(max)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square((y_true - y_pred0) / sigma))\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels**4) + tf.square((y_true - meanLabels)/(stdLabels*stdLabels)))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log((1e-5)**4)) * tf.ones_like(y_predMax)\n",
    "    #print(L_pred)\n",
    "    #print(L_ref)\n",
    "    #print(L_ideal)\n",
    "    #print(tf.reduce_sum(L_pred),tf.reduce_sum(L_ideal),tf.reduce_sum(L_ref))\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal) - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScaling_scipy(y_trueMax, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "\n",
    "    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred0, scale=sigma))\n",
    "    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=(1e-10) * np.ones_like(y_true)))\n",
    "    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=meanLabels * np.ones_like(y_true), scale=(stdLabels*stdLabels) * np.ones_like(y_true)))\n",
    "\n",
    "    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n",
    "    #print(GLL_pred, GLL_true, GLL_mean)\n",
    "    \n",
    "    return submit_score\n",
    "\n",
    "log_likelihood_zScoreTarget(batch[1], out)\n",
    "log_likelihood_maxScaling(batch[1], out),log_likelihood_maxScaling_scipy(batch[1],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate metric: take away only good in a certain range depending on mse\n",
    "scores=[]\n",
    "expValues=[]\n",
    "copyOut=out.numpy()\n",
    "for i,f in enumerate(np.linspace(0,3,100)):\n",
    "    factor =f#2**(f-10)\n",
    "    copyOut[:,:,1] = np.log(tf.ones_like(out[:,:,1]) * factor)/maxTrainLabels\n",
    "    s = log_likelihood_maxScaling_scipy(batch[1],copyOut)\n",
    "    print(i, s)\n",
    "    scores.append(s)\n",
    "print(scores)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(len(scores)), scores, 'b', label='Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_mae(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(20.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(20.0), logConfidence)\n",
    "    \n",
    "    loss = tf.math.abs(y_true_zScore - y_predZScore)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "    loss_2 = tf.math.abs(loss-(logConfidence))\n",
    "    return tf.reduce_sum(loss, axis=-1) + tf.reduce_sum(loss_2,axis=-1)\n",
    "\n",
    "def mae(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    loss = tf.math.abs(y_true_zScore - y_predZScore)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "def mse(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    loss = tf.square(y_true_zScore - y_predZScore)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "    return tf.reduce_sum(loss, axis=-1)\n",
    "\n",
    "def loss_mse(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(20.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(20.0), logConfidence)\n",
    "    \n",
    "    loss = tf.square(y_true_zScore - y_predZScore)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "    loss_2 = tf.square(loss-(logConfidence))\n",
    "    return tf.reduce_sum(loss, axis=-1)# + 0.001*tf.reduce_sum(loss_2,axis=-1)\n",
    "\n",
    "def log_loss_maxScaling(y_trueMax, y_pred):\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = tf.math.log(sigma*sigma)# + tf.math.log(max)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square((y_true - y_pred0) / sigma))\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels**4) + tf.square((y_true - meanLabels)/(stdLabels*stdLabels)))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log((1e-5)**4)) * tf.ones_like(y_predMax)\n",
    "    L = -((tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal) - tf.reduce_sum(L_ref)) -1)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "model.compile(loss=loss_mse,metrics=[log_likelihood_maxScaling], optimizer=optimizer)\n",
    "#model.compile(loss='mae', ,metrics=[log_likelihood_zScoreTarget]optimizer=optimizer)\n",
    "#model.compile(loss=loss_mae, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(#train_dataset, \n",
    "                    batch[0],batch[1], #verbose=2,\n",
    "                    #validation_data=test_dataset,\n",
    "                    #validation_data=(test_batch[0],test_batch[1]),\n",
    "                    epochs=2000, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiments with original data (faster processing)\n",
    "# equal weighted loss of mean & stddev -> mse ~ 7/7 -> but can't fit targes at all!!!!!! this is the issue\n",
    "\n",
    "# training one batch for 1200 epochs -> loss (mse) ~0.02 seems to fit okish to data\n",
    "# 1600 epochs ~ 0.003 gets better and better\n",
    "# 2000 epochs ~0.001\n",
    "\n",
    "# 2000 epochs with smaller net -> 1.4, can't fit\n",
    "\n",
    "# lr0.000001 get's stuck at like 30, lr much smaller + retrain, doesn't beat 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('originalData_222_epochs_accLoss_reluActivation_8_10.keras')\n",
    "# Save weights\n",
    "model.save_weights('originalData_222_epochs_accLoss_reluActivation_8_10.weights.h5')\n",
    "\n",
    "# Load weights\n",
    "#loaded_weights = model.load_weights('170_epochs_accLoss_reluActivation_23_23.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('120_epochs_accLoss31_30.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(#train_dataset, \n",
    "                    batch[0],batch[1], #verbose=2,\n",
    "                    #validation_data=test_dataset,\n",
    "                    #validation_data=next(iter(test_dataset)),\n",
    "                    epochs=400, batch_size=batch_size)\n",
    "\n",
    "#one epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try couldn't fit the values, just predicted mean if I kept the shape (output layer of shape 1 - tensor 283x100 -> 283x1)\n",
    "# having a flatten layer between converges\n",
    "\n",
    "# flatten layer and 12 samples -> predict the same for all 12 samples, maybe not enough filters\n",
    "\n",
    "# PROBLEM why we can't fit multiple targets: layer normalization!! use batch norm instead\n",
    "\n",
    "#---- with batch norm\n",
    "# cnn model + mean estimation, loss ~80, but predicting differnt mean\n",
    "# fcn model, loss ~81\n",
    "# fcn model / min scaling -> loss 0.8 / 27 (lots of negative predictions)\n",
    "# fcn model / max scaling / relu activation -> 3.1/8 (lots of 0 predictions) / with scale of 100, loss =14.9/43691\n",
    "# cnn model / max scaling / mean pred -> 6.0/inf\n",
    "# cnn ... no layer norm in beginning -> 15\n",
    "\n",
    "# loss function for every output (batch,283) / 100 epochs\n",
    "# cnn 1.5 loss\n",
    "# cnn with smaller LR 0.22(also after 200 epochs)\n",
    "# cnn with separate mean prediction loss 20.5 (lr0.0001) vs 3.5(lr0.0005) / can't even fit 2 samples (0.5 for lr 0.0005)\n",
    "\n",
    "# cnn without mean prediction (2 samples, lr0.0005) 22.4   / lr0.001 0.4 loss, but targets still fit badly / only fitting target noVar 0.08 still bad\n",
    "\n",
    "# difference between train / test = batch norm has significant effect here\n",
    "#fcn + mean, 2 samples LR0.0005 -> \n",
    "#fcn + mean, 2 samples only loss on target -> \n",
    "#fcn + mean, 1 sample, only loss on target -> 0.03 targets are far off\n",
    "#fcn, 1 sample, only loss on target -> 0.4 targets are far off\n",
    "# -> train data was not normalized!!\n",
    "\n",
    "# with regularization / without regularization doesn't matter that much as long as sample is normalized\n",
    "# normalization per sample -> predict the same for all targets ~0.0978\n",
    "# norm per sample + bis estimation -> predict same for all targets (besides 1) ~0.0978\n",
    "\n",
    "# with learning rate schedule -> 0.06 lots more possible to not get stuck in local minima\n",
    "\n",
    "#cnn / norm over train / bias estimation / lr0.01 / only target -> ~45 sum loss\n",
    "#cnn / norm over train / bias estimation / lf0.01 / target + loss2 -> ~47 after 95 epochs (15 after~150epochs)\n",
    "\n",
    "#cnn / norm over train / bais est / lr0.01 / target + loss / activation function relu instead of linear (conf + bias / still nan bc stddev =0, log(0) = nan)\n",
    "# 39/40 but training seems to be a lot more stable\n",
    "# after 170 epochs 23.7/23.5\n",
    "# after 220 epochs 11/19 (but already went down to 14/16)\n",
    "# after 250 epochs 12/16 (but already 16/15)\n",
    "# after 300 epochs 12/17\n",
    "\n",
    "\n",
    "# Assuming 'history' is your model's training history\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "#plt.plot(epochs, test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = model.predict(normData)\n",
    "pred = model.predict(batch[0])\n",
    "pred[:,0:2,0]*maxLabels, batch[1][:,0:2]*maxLabels ,np.exp(pred[:,0:2,1])*maxLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall',(mae(batch[1],pred)))\n",
    "for i in range(batch_size):\n",
    "    print(f'batch {i}',(mse(batch[1][i,:],pred[i:i+1,:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(4):#[2,6,10,20,100]:\n",
    "    fig.add_trace(go.Scatter(y=batch[0][i,:,0,0],mode='markers',name=f'f_{i}',marker=dict(size=3)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(10): #range(12):# \n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): #range(12):#\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

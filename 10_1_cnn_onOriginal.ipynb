{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf0 = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf0 = labelDf0.set_index('planet_id')\n",
    "labelDf0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.2):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "meanLabels = np.mean(labelDf.mean())\n",
    "stdLabels = np.std(labelDf.std())\n",
    "maxLabels = np.max(labelDf.max())\n",
    "minLabels = np.min(labelDf.min())\n",
    "\n",
    "trainLabels = labelDf.loc[[int(star) for star in train_stars]]\n",
    "meanTrainLabels = np.mean(trainLabels.mean())\n",
    "stdTrainLabels = np.std(trainLabels.std())\n",
    "maxTrainLabels = np.max(trainLabels.max())\n",
    "minTrainLabels = np.min(trainLabels.min())\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col]) / (maxTrainLabels)\n",
    "\n",
    "# normalize over time and all samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrain(train_stars):\n",
    "    i = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,:]\n",
    "            if i ==0:\n",
    "                mean = np.mean(x,axis=(0))\n",
    "                sumS = np.sum(x**2,axis=0)\n",
    "            else:\n",
    "                mean = mean + np.mean(x, axis=(0))\n",
    "                sumS += np.sum(x**2,axis=0)\n",
    "            i=i+1\n",
    "    meanTrain = mean / i\n",
    "    stdTrain = np.sqrt(sumS / (i*x.shape[0]) - meanTrain**2)    \n",
    "    return meanTrain, stdTrain\n",
    "meanTrain, stdTrain = calcMeanAndStdOfTrain(train_stars)\n",
    "\n",
    "def normalize_over_train(features, labels):\n",
    "    features = (features - meanTrain) / (stdTrain + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "# normalize over time per samples, so we have a mean and a std dev per wavelength for all samples\n",
    "def calcMeanAndStdOfTrainPerStar(x):\n",
    "    mean = np.mean(x,axis=(0))\n",
    "    sumS = np.sum(x**2,axis=0)\n",
    "    stdTrain = np.sqrt(sumS / (x.shape[0]) - mean**2)    \n",
    "    return mean, stdTrain\n",
    "def normalize_per_sample(features, labels):\n",
    "    m,s = calcMeanAndStdOfTrainPerStar(features)\n",
    "    features = (features) / (s + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "            features = np.reshape(features,(-1,25,283,4))\n",
    "            features = np.mean(features,axis=1)\n",
    "            #features, labels = normalize_per_sample(features,labels)\n",
    "            features, labels = normalize_over_train(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([225, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283])))) #5625\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('helpers_origiData.npz',meanTrain=meanTrain, stdTrain=stdTrain,meanLabels=meanLabels,stdLabels=stdLabels,maxTrainLabels=maxTrainLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 12\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dataset:\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timepoints = 225\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "\n",
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        return x\n",
    "    \n",
    "class reduce(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        mean = tf.expand_dims(mean, axis=-1)\n",
    "        return mean\n",
    "class reduce1(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        mean = tf.reduce_sum(x,axis=-1)\n",
    "        return mean\n",
    "\n",
    "\n",
    "def cnnM(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,1]\n",
    "\n",
    "\n",
    "    #x = Reshape11()(x)\n",
    "    dim = timepoints\n",
    "    for i in range(3):\n",
    "        x = tf.keras.layers.Conv1D(filters=wavelengths, kernel_size=(5), padding='valid')(x)\n",
    "        x = tf.keras.layers.AveragePooling1D(2)(x)\n",
    "\n",
    "    x = Reshape11()(x)\n",
    "    x = tf.keras.layers.Dense(100)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(1000)(x)\n",
    "    mean = tf.keras.layers.Dense(1,activation='relu')(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_pred = x_pred+mean\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = cnnM() \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "test_batch = next(iter(test_dataset))\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_zScoreTarget(y_trueZScore, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueZScore * stdTrainLabels + meanTrainLabels   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predZScore = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predZScore * stdTrainLabels + meanTrainLabels\n",
    "    stdDev = tf.exp(log_sigma)*stdTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = log_sigma + tf.math.log(stdTrainLabels)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / stdDev)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels*stdLabels) + tf.square(y_true-meanLabels) / stdLabels / stdLabels)   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal)*283*5625 - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScaling(y_trueMax, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = tf.math.log(sigma)# + tf.math.log(max)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / sigma)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(stdLabels*stdLabels) + tf.square((y_trueMax*maxTrainLabels - meanLabels)/stdLabels))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10)) * tf.ones_like(y_predMax)\n",
    "\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal) - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_likelihood_maxScaling_scipy(y_trueMax, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueMax * maxTrainLabels #std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predMax = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predMax *maxTrainLabels #* std + mean\n",
    "    sigma = tf.exp(log_sigma)*maxTrainLabels  # Exponentiate to get variance + scale back from zscore \n",
    "\n",
    "    GLL_pred = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_pred0, scale=sigma))\n",
    "    GLL_true = np.sum(scipy.stats.norm.logpdf(y_true, loc=y_true, scale=(1e-10) * np.ones_like(y_true)))\n",
    "    GLL_mean = np.sum(scipy.stats.norm.logpdf(y_true, loc=meanLabels * np.ones_like(y_true), scale=(stdLabels*stdLabels) * np.ones_like(y_true)))\n",
    "\n",
    "    submit_score = (GLL_pred - GLL_mean)/(GLL_true - GLL_mean)\n",
    "    \n",
    "    return submit_score\n",
    "\n",
    "log_likelihood_zScoreTarget(batch[1], out)\n",
    "log_likelihood_maxScaling(batch[1], out),log_likelihood_maxScaling_scipy(batch[1],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=300\n",
    "LR_SCHEDULE = [0.01*((np.cos(step/epochs *np.pi) if np.cos(step/epochs*np.pi)>0.001 else np.sin(step/epochs*np.pi))) for step in range(epochs)]\n",
    "plt.figure()\n",
    "plt.plot(LR_SCHEDULE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.exp(10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "def loss_fn0(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    logConfidence = tf.math.exp(y_pred[:, :,1]) # logSigma = log(sigma / std)  we predict sigma NOT stdDev!!\n",
    "    #tf.print(logConfidence)\n",
    "    largerThanT = tf.greater(logConfidence, tf.exp(20.0))\n",
    "    logConfidence = tf.where(largerThanT, y_pred[:,:,1] + tf.exp(20.0), logConfidence)\n",
    "    #tf.print(logConfidence)\n",
    "    \n",
    "    loss = tf.math.abs(y_true_zScore - y_predZScore)#tf.math.abs(y_true_zScore-y_predZScore)\n",
    "    loss_2 = tf.math.abs(loss-(logConfidence))\n",
    "    \n",
    "    l = tf.reduce_sum(loss, axis=-1) + tf.reduce_sum(loss_2,axis=-1)\n",
    "    return l #loss#+0.0001*loss_2#tf.reduce_mean(loss+0.001*loss_2, axis=-1)\n",
    "\n",
    "\n",
    "def loss_mae(y_true_zScore,y_pred):\n",
    "    y_true_meanVal = tf.math.reduce_mean(y_true_zScore,axis=1)\n",
    "    y_pred = y_pred[:,0]\n",
    "    absVal = tf.math.abs(y_true_meanVal - y_pred)\n",
    "    m = tf.reduce_mean(absVal,axis=0)\n",
    "    return m\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda step: LR_SCHEDULE[step], verbose=0)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(loss=loss_fn0,metrics=[log_likelihood_maxScaling], optimizer=optimizer)\n",
    "#model.compile(loss='mae', ,metrics=[log_likelihood_zScoreTarget]optimizer=optimizer)\n",
    "#model.compile(loss=loss_mae, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    #validation_data=(test_batch[0],test_batch[1]),\n",
    "                    epochs=120, batch_size=batch_size,\n",
    "                    #callbacks=[lr_callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('originalData_222_epochs_accLoss_reluActivation_8_10.keras')\n",
    "# Save weights\n",
    "model.save_weights('originalData_222_epochs_accLoss_reluActivation_8_10.weights.h5')\n",
    "\n",
    "# Load weights\n",
    "#loaded_weights = model.load_weights('170_epochs_accLoss_reluActivation_23_23.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model('120_epochs_accLoss31_30.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    #validation_data=next(iter(test_dataset)),\n",
    "                    epochs=2, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first try couldn't fit the values, just predicted mean if I kept the shape (output layer of shape 1 - tensor 283x100 -> 283x1)\n",
    "# having a flatten layer between converges\n",
    "\n",
    "# flatten layer and 12 samples -> predict the same for all 12 samples, maybe not enough filters\n",
    "\n",
    "# PROBLEM why we can't fit multiple targets: layer normalization!! use batch norm instead\n",
    "\n",
    "#---- with batch norm\n",
    "# cnn model + mean estimation, loss ~80, but predicting differnt mean\n",
    "# fcn model, loss ~81\n",
    "# fcn model / min scaling -> loss 0.8 / 27 (lots of negative predictions)\n",
    "# fcn model / max scaling / relu activation -> 3.1/8 (lots of 0 predictions) / with scale of 100, loss =14.9/43691\n",
    "# cnn model / max scaling / mean pred -> 6.0/inf\n",
    "# cnn ... no layer norm in beginning -> 15\n",
    "\n",
    "# loss function for every output (batch,283) / 100 epochs\n",
    "# cnn 1.5 loss\n",
    "# cnn with smaller LR 0.22(also after 200 epochs)\n",
    "# cnn with separate mean prediction loss 20.5 (lr0.0001) vs 3.5(lr0.0005) / can't even fit 2 samples (0.5 for lr 0.0005)\n",
    "\n",
    "# cnn without mean prediction (2 samples, lr0.0005) 22.4   / lr0.001 0.4 loss, but targets still fit badly / only fitting target noVar 0.08 still bad\n",
    "\n",
    "# difference between train / test = batch norm has significant effect here\n",
    "#fcn + mean, 2 samples LR0.0005 -> \n",
    "#fcn + mean, 2 samples only loss on target -> \n",
    "#fcn + mean, 1 sample, only loss on target -> 0.03 targets are far off\n",
    "#fcn, 1 sample, only loss on target -> 0.4 targets are far off\n",
    "# -> train data was not normalized!!\n",
    "\n",
    "# with regularization / without regularization doesn't matter that much as long as sample is normalized\n",
    "# normalization per sample -> predict the same for all targets ~0.0978\n",
    "# norm per sample + bis estimation -> predict same for all targets (besides 1) ~0.0978\n",
    "\n",
    "# with learning rate schedule -> 0.06 lots more possible to not get stuck in local minima\n",
    "\n",
    "#cnn / norm over train / bias estimation / lr0.01 / only target -> ~45 sum loss\n",
    "#cnn / norm over train / bias estimation / lf0.01 / target + loss2 -> ~47 after 95 epochs (15 after~150epochs)\n",
    "\n",
    "#cnn / norm over train / bais est / lr0.01 / target + loss / activation function relu instead of linear (conf + bias / still nan bc stddev =0, log(0) = nan)\n",
    "# 39/40 but training seems to be a lot more stable\n",
    "# after 170 epochs 23.7/23.5\n",
    "# after 220 epochs 11/19 (but already went down to 14/16)\n",
    "# after 250 epochs 12/16 (but already 16/15)\n",
    "# after 300 epochs 12/17\n",
    "\n",
    "\n",
    "# Assuming 'history' is your model's training history\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
    "#plt.plot(epochs, test_loss, 'r', label='Test loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = model.predict(normData)\n",
    "pred = model.predict(test_batch[0])\n",
    "pred[:,0:2,0]*maxLabels, test_batch[1][:,0:2]*maxLabels ,np.exp(pred[:,0:2,1])*maxLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall',(loss_fn0(batch[1],pred)))\n",
    "for i in range(batch_size):\n",
    "    print(f'batch {i}',np.mean(loss_fn0(batch[1][i,:],pred[i:i+1,:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(4):#[2,6,10,20,100]:\n",
    "    fig.add_trace(go.Scatter(y=batch[0][i,:,0,0],mode='markers',name=f'f_{i}',marker=dict(size=3)))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(12):#[2,6,10,20,100]:\n",
    "    fig.add_trace(go.Scatter(y=batch[1][i,:],mode='markers',name=f'gt_{i}',marker=dict(size=3)))\n",
    "    fig.add_trace(go.Scatter(y=pred[i,:,0],mode='markers',name=f'pred_{i}',marker=dict(size=3)))\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

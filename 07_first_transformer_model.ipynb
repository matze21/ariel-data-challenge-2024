{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check available physical devices (GPUs)\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) == 0:\n",
    "    print(\"No GPU devices found.\")\n",
    "else:\n",
    "    for device in physical_devices:\n",
    "        print(\"GPU:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.2):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    # Perform any necessary preprocessing here\n",
    "    return features, labels\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "\n",
    "            features, labels = preprocess_data(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([5625, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283]))))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in train_dataset:\n",
    "    print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "class ReduceDim(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        return x\n",
    "    \n",
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, timepoints, representations, wavelengths, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.timepoints = timepoints\n",
    "        self.wavelengths = wavelengths\n",
    "        self.representations = representations\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "        self.ffn1 = tf.keras.layers.Dense(embed_dim)\n",
    "        self.ffn2 = tf.keras.layers.Dense(feed_forward_dim)\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.att(x, x)\n",
    "        x = x + residual\n",
    "        #x = self.ffn1(x)\n",
    "        x = self.ffn2(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "timepoints = 5625\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "def buildModel(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp\n",
    "\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # 2. transpose, timepoints,wavelengths -> wavelengths,timepoints\n",
    "\n",
    "       # batch, wavelengths, time, representation\n",
    "\n",
    "    x = Reshape1(timepoints, representations, wavelengths)(x) #make to [batch_size, wavelengths, time,repr]\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='valid')(x)\n",
    "    x = ReduceDim()(x)\n",
    "    dim = int(timepoints/4)# x.shape[2]\n",
    "    x = tf.keras.layers.Dense(dim)(x)\n",
    "    #x = tf.keras.layers.Conv2D(filters=dim, kernel_size=(3, 3), padding='valid')(x)\n",
    "\n",
    "    #x = tf.keras.layers.Conv1D(filters=dim, kernel_size=8, padding='same', activation='relu')(x)\n",
    "\n",
    "    for i in range(5):\n",
    "        x = TransformerEncoder(embed_dim=dim, num_heads=4, feed_forward_dim=int(dim/(2)))(x)\n",
    "        dim = int(dim/2)\n",
    "\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x = Reshape2()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = buildModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(y_true, y_pred):\n",
    "    print(y_true.shape, y_pred.shape)\n",
    "    # y_pred is expected to contain both mean and log variance\n",
    "    mu = y_pred[:, :,0]  # Mean\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev)\n",
    "\n",
    "    # Calculate the variance\n",
    "    sigma2 = tf.exp(log_sigma)  # Exponentiate to get variance\n",
    "    nll = 0.5 * (tf.math.log(2 * np.pi) + log_sigma + tf.square(y_true - mu) / sigma2)\n",
    "    \n",
    "    return tf.reduce_mean(nll)\n",
    "\n",
    "\n",
    "\n",
    "def loss_fn(y_true, y_pred):\n",
    "    mu = y_pred[:, :,0]  # Mean\n",
    "    confidence = y_pred[:, :,1] \n",
    "\n",
    "    loss = tf.math.abs(y_true-mu)\n",
    "    loss_2 = tf.math.abs(loss-confidence)\n",
    "    return tf.reduce_sum(loss+loss_2)\n",
    "\n",
    "loss_fn(batch[1], out)\n",
    "gaussian_log_likelihood(batch[1], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "loss = loss_fn\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.0001)\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "history = model.fit(#train_dataset, \n",
    "                    batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    epochs=100, batch_size=batch_size,\n",
    "                    #callbacks=[validation_callback(val_metric_list), lr_callback, WeightDecayCallback()]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred\n",
    "# TODO: use normalization / scale targets such that gradients are big!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

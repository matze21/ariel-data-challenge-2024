{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "\n",
    "mean = np.mean(labelDf.mean())\n",
    "std = np.std(labelDf.std())\n",
    "max = np.max(labelDf.max())\n",
    "min = np.min(labelDf.min())\n",
    "mean, std, max, min\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col] - mean) / (std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.2):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    # Perform any necessary preprocessing here\n",
    "    return features, labels\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "\n",
    "            features, labels = preprocess_data(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([5625, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283]))))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "class ReduceDim(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        return x\n",
    "    \n",
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, timepoints, representations, wavelengths, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.timepoints = timepoints\n",
    "        self.wavelengths = wavelengths\n",
    "        self.representations = representations\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "        self.ffn1 = tf.keras.layers.Dense(embed_dim)\n",
    "        self.ffn2 = tf.keras.layers.Dense(feed_forward_dim)\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.att(x, x)\n",
    "        x = x + residual\n",
    "        residual = x\n",
    "        #x = self.ffn1(x)\n",
    "        x = self.ffn2(x)\n",
    "        x = self.layer_norm(x + residual)\n",
    "        return x\n",
    "    \n",
    "\n",
    "timepoints = 5625\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "def buildModel_bad(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp\n",
    "\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # 2. transpose, timepoints,wavelengths -> wavelengths,timepoints\n",
    "\n",
    "    # edit: doesn't make sense since transformer should not learn similarities between wavelengths but rather similarities between timepoints\n",
    "\n",
    "       # batch, wavelengths, time, representation\n",
    "\n",
    "    x = Reshape1(timepoints, representations, wavelengths)(x) #make to [batch_size, wavelengths, time,repr]\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='valid')(x)\n",
    "    x = ReduceDim()(x)\n",
    "    dim = timepoints #int(timepoints/4)# x.shape[2]\n",
    "    x = tf.keras.layers.Dense(dim)(x)\n",
    "\n",
    "    for i in range(5):\n",
    "        x = TransformerEncoder(embed_dim=dim, num_heads=4, feed_forward_dim=int(dim/(2)))(x)\n",
    "        dim = int(dim/2)\n",
    "\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "    x = Reshape2()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildModel(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp\n",
    "\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='valid')(x)\n",
    "    x = ReduceDim()(x)\n",
    "    dim = wavelengths #int(timepoints/4)# x.shape[2]\n",
    "    #x = tf.keras.layers.Dense(dim)(x)\n",
    "    #x = tf.keras.layers.Conv2D(filters=dim, kernel_size=(3, 3), padding='valid')(x)\n",
    "\n",
    "    #x = tf.keras.layers.Conv1D(filters=dim, kernel_size=8, padding='same', activation='relu')(x)\n",
    "\n",
    "    for i in range(5):\n",
    "        x = TransformerEncoder(embed_dim=dim, num_heads=4, feed_forward_dim=dim)(x)\n",
    "        #dim = int(dim/2)\n",
    "\n",
    "    x = Reshape11()(x)  # reshape to wavelengths, timestamps\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(2000)(x)\n",
    "    x = tf.keras.layers.Dense(200)(x)\n",
    "    x = tf.keras.layers.Dense(5)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='relu')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='relu')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "def buildModel(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "    #x = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='valid')(x)\n",
    "    #x = ReduceDim()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(dim)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(283*4, activation='relu')(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(283*4, activation='relu')(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(283*4, activation='relu')(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = buildModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "test_batch = next(iter(test_dataset))\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_log_likelihood(y_true, y_pred):\n",
    "    # y_pred is expected to contain both mean and log variance\n",
    "    mu = y_pred[:, :,0]  # Mean\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev)\n",
    "\n",
    "    # Calculate the variance\n",
    "    sigma2 = tf.exp(log_sigma)  # Exponentiate to get variance\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + log_sigma + tf.square(y_true - mu) / sigma2)\n",
    "\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(std*std) + tf.square(y_true))   # ( (y_trueN - mean)/std )² -> y_true = y_trueN * std + mean\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    #print(L_pred, L_ref, L_ideal)\n",
    "\n",
    "    L = (L_pred -L_ref) / (L_ideal - L_ref)\n",
    "    #print(L)\n",
    "    \n",
    "    return tf.reduce_mean(L)\n",
    "gaussian_log_likelihood(batch[1], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "def loss_fn0(y_true, y_pred):\n",
    "    mu = y_pred[:, :,0]  # Mean\n",
    "    confidence = tf.exp(y_pred[:, :,1] )  #let's predict the log\n",
    "\n",
    "    loss = tf.math.square(y_true-mu)\n",
    "    loss_2 = tf.math.square(loss-confidence)\n",
    "    return tf.reduce_mean(loss+loss_2)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(loss=loss_fn0,metrics=[gaussian_log_likelihood], optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    validation_data=test_dataset,\n",
    "                    epochs=200, batch_size=batch_size,\n",
    "                    #callbacks=[validation_callback(val_metric_list), lr_callback, WeightDecayCallback()]\n",
    "                    )\n",
    "\n",
    "# fcn loss on one batch ~1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(batch[0])\n",
    "pred[:,:,0],np.exp(pred[:,:,1]), batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.abs(pred[:,:,0]-batch[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_log_likelihood(batch[1],pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn0(batch[1],np.zeros((batch[1].shape[0], batch[1].shape[1],2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

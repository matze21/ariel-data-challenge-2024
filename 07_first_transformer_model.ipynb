{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelDf = pd.read_csv(\"train_labels.csv\")\n",
    "labelDf = labelDf.set_index('planet_id')\n",
    "\n",
    "mean = np.mean(labelDf.mean())\n",
    "std = np.std(labelDf.std())\n",
    "max = np.max(labelDf.max())\n",
    "min = np.min(labelDf.min())\n",
    "mean, std, max, min\n",
    "\n",
    "for col in labelDf.columns:\n",
    "    labelDf.loc[:,col] = (labelDf[col] - mean) / (std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "files = glob.glob(os.path.join('train/', '*/*'))\n",
    "stars = []\n",
    "for file in files:\n",
    "    file_name = file.split('\\\\')[1]\n",
    "    stars.append(file_name)\n",
    "stars = np.unique(stars)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def split_star_list(file_list, test_ratio=0.2):\n",
    "    random.shuffle(file_list)\n",
    "    split_index = int(len(file_list) * (1 - test_ratio))\n",
    "    train_files = file_list[:split_index]\n",
    "    test_files = file_list[split_index:]\n",
    "    return train_files, test_files\n",
    "\n",
    "train_stars, test_stars = split_star_list(stars)\n",
    "\n",
    "def calcMeanAndStdOfTrain(train_stars):\n",
    "    i = 0\n",
    "    for star in train_stars:\n",
    "        file_path = 'train/'+str(star)+'/combined.npz'\n",
    "        with np.load(file_path) as data:\n",
    "            x = data['a'][0,:,0:283,:]\n",
    "            if i ==0:\n",
    "                mean = np.mean(x,axis=(0))\n",
    "                sumS = np.sum(x**2,axis=0)\n",
    "            else:\n",
    "                mean = mean + np.mean(x, axis=(0))\n",
    "                sumS += np.sum(x**2,axis=0)\n",
    "            i=i+1\n",
    "    meanTrain = mean / i\n",
    "    stdTrain = np.sqrt(sumS / (i*x.shape[0]) - meanTrain**2)    \n",
    "    return meanTrain, stdTrain\n",
    "\n",
    "meanTrain, stdTrain = calcMeanAndStdOfTrain(train_stars)\n",
    "\n",
    "def preprocess_data(features, labels):\n",
    "    features = (features - meanTrain) / (stdTrain + 1e-6)\n",
    "    return features, labels\n",
    "\n",
    "def load_npz(star):\n",
    "    integer_value = tf.strings.to_number(star, out_type=tf.int64)\n",
    "    python_int = integer_value.numpy()\n",
    "\n",
    "    file_path = 'train/'+str(python_int)+'/combined.npz'\n",
    "    try:\n",
    "        with np.load(file_path) as data:\n",
    "            features = data['a'][0,:,0:283,:]\n",
    "            labels = labelDf.loc[python_int].to_numpy()\n",
    "\n",
    "            features, labels = preprocess_data(features,labels)\n",
    "            return features, labels\n",
    "    except Exception as e:\n",
    "        print(\"Error loading file:\", e, python_int)\n",
    "    \n",
    "\n",
    "def create_dataset(star_list, batch_size, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(star_list)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(star_list))\n",
    "    def load_and_process(x):\n",
    "        features, labels = tf.py_function(\n",
    "            func=load_npz,\n",
    "            inp=[x],\n",
    "            Tout=[tf.float64, tf.float32]\n",
    "        )\n",
    "        return features, labels\n",
    "\n",
    "    dataset = dataset.map(load_and_process, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.map(lambda x, y: (tf.ensure_shape(x,tf.TensorShape([5625, 283, 4])), tf.ensure_shape(y, tf.TensorShape([283]))))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "batch_size = 12\n",
    "\n",
    "train_dataset = create_dataset(train_stars, batch_size, shuffle=True)\n",
    "test_dataset = create_dataset(test_stars, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "class ReduceDim(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.squeeze(x, axis=-1)\n",
    "        return x\n",
    "    \n",
    "class Reshape1(tf.keras.layers.Layer):\n",
    "    def __init__(self, timepoints, representations, wavelengths, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.timepoints = timepoints\n",
    "        self.wavelengths = wavelengths\n",
    "        self.representations = representations\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1,3])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "    \n",
    "class Reshape11(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x):\n",
    "        x = tf.transpose(x, perm=[0,2,1])\n",
    "        #x = tf.reshape(x, [-1, self.timepoints, tf.cast(self.wavelengths * self.representations, tf.int32)])\n",
    "        return x\n",
    "\n",
    "class Reshape2(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Reshape22(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    def call(self, x_pred, x_confidence):\n",
    "        x_pred = tf.expand_dims(x_pred, axis=-1)\n",
    "        x_confidence = tf.expand_dims(x_confidence, axis=-1)\n",
    "        x = tf.concat([x_pred, x_confidence], axis = -1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim):\n",
    "        super().__init__()\n",
    "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim//num_heads)\n",
    "        self.ffn2 = tf.keras.layers.Dense(embed_dim)\n",
    "        self.ffn1 = tf.keras.layers.Dense(feed_forward_dim)\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.att(x, x)\n",
    "        x = x + residual\n",
    "        residual = x\n",
    "        x = self.ffn1(x)\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.ffn2(x)\n",
    "        x = self.layer_norm2(x + residual)\n",
    "        return x\n",
    "    \n",
    "\n",
    "timepoints = 5625\n",
    "representations = 4\n",
    "wavelengths = 283\n",
    "targetWavelengths = 283\n",
    "def buildModel_bad(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp\n",
    "\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # 2. transpose, timepoints,wavelengths -> wavelengths,timepoints\n",
    "\n",
    "    # edit: doesn't make sense since transformer should not learn similarities between wavelengths but rather similarities between timepoints\n",
    "\n",
    "       # batch, wavelengths, time, representation\n",
    "\n",
    "    x = Reshape1(timepoints, representations, wavelengths)(x) #make to [batch_size, wavelengths, time,repr]\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "    x = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='valid')(x)\n",
    "    x = ReduceDim()(x)\n",
    "    dim = timepoints #int(timepoints/4)# x.shape[2]\n",
    "    x = tf.keras.layers.Dense(dim)(x)\n",
    "\n",
    "    for i in range(5):\n",
    "        x = TransformerEncoder(embed_dim=dim, num_heads=4, feed_forward_dim=int(dim/(2)))(x)\n",
    "        dim = int(dim/2)\n",
    "\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(1, activation='relu')(x)\n",
    "    x = Reshape2()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def buildTransfModel(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "    #x = tf.keras.layers.Conv2D(filters=1, kernel_size=(1, 1), padding='valid')(x)\n",
    "    #x = ReduceDim()(x)\n",
    "    dim = wavelengths*4 #int(timepoints/4)# x.shape[2]\n",
    "    #x = tf.keras.layers.Dense(dim)(x)\n",
    "    #x = tf.keras.layers.Conv2D(filters=dim, kernel_size=(3, 3), padding='valid')(x)\n",
    "\n",
    "    #x = tf.keras.layers.Conv1D(filters=dim, kernel_size=8, padding='same', activation='relu')(x)\n",
    "\n",
    "    for i in range(5):\n",
    "        x = TransformerEncoder(embed_dim=wavelengths, num_heads=4, feed_forward_dim=wavelengths*4)(x)\n",
    "        #dim = int(dim/2)\n",
    "\n",
    "    x = Reshape11()(x)  # reshape to wavelengths, timestamps\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(3000)(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    # = tf.keras.layers.Dense(1000)(x)\n",
    "    # = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    # = tf.keras.layers.Dense(200)(x)\n",
    "    # = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    # = tf.keras.layers.Dense(20)(x)\n",
    "    # = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    #x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    x_pred = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(1, activation='linear')(x)\n",
    "    x = Reshape2()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "def buildFCNModel(outputDim = 283):\n",
    "    inp = tf.keras.Input(shape=(timepoints, wavelengths, representations))\n",
    "    x = inp[:,:,:,0]\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6, axis=[2])(x)\n",
    "    # plan:\n",
    "    # 1. use cnn filter so go from timepoints, wavelengths, representations -> timepoints, wavelengths (283 1d filter)\n",
    "    # Use a 1D Convolutional layer with kernel size of 1 to reduce the last dimension\n",
    "\n",
    "    #x = tf.keras.layers.Conv2D(filters=20, kernel_size=(1, 1), padding='valid')(x) # seems to be worse\n",
    "    #x = tf.keras.layers.Dense(1)(x)\n",
    "    #x = ReduceDim()(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    #x = tf.keras.layers.Dense(dim)(x)\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    factor = 1\n",
    "    x = tf.keras.layers.Dense(283*factor, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(283*factor, activation='relu')(x)\n",
    "    #x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = tf.keras.layers.Dense(283*factor, activation='relu')(x)\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x_pred = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x_confidence = tf.keras.layers.Dense(283, activation='linear')(x)\n",
    "    x = Reshape22()(x_pred, x_confidence)\n",
    "\n",
    "    model = tf.keras.Model(inp, x)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = buildFCNModel() \n",
    "#model = buildTransfModel()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataset))\n",
    "out = model(batch[0])\n",
    "test_batch = next(iter(test_dataset))\n",
    "batch[0].dtype ,batch[1].dtype, out.dtype,batch[0].shape ,batch[1].shape, out.shape\n",
    "\n",
    "#normData = np.zeros_like(batch[0])\n",
    "#normTest = np.zeros_like(test_batch[0])\n",
    "#for i in range(batch[0].shape[2]):\n",
    "#    for dim in range(batch[0].shape[3]):\n",
    "#        meanNorm = np.mean(batch[0][:,:,i,dim])\n",
    "#        stdNorm = np.std(batch[0][:,:,i,dim])\n",
    "#        normData[:,:,i,dim] = ((batch[0][:,:,i,dim] - meanNorm) / (stdNorm + 1e-7))\n",
    "#        normTest[:,:,i,dim] = ((test_batch[0][:,:,i,dim] - meanNorm) / (stdNorm + 1e-7))\n",
    "#\n",
    "##normData.dtype\n",
    "#out = model(normData)\n",
    "##normData = normData.astype(np.float64)\n",
    "#normData, batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_zScoreTarget(y_trueZScore, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueZScore * std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predZScore = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predZScore * std + mean\n",
    "    stdDev = tf.exp(log_sigma)*std  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = log_sigma + tf.math.log(std)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / stdDev)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(std*std) + tf.square(y_trueZScore))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    L = (tf.reduce_sum(L_pred) -tf.reduce_sum(L_ref)) / (tf.reduce_sum(L_ideal)*283*5625 - tf.reduce_sum(L_ref))\n",
    "    \n",
    "    return L\n",
    "\n",
    "def log_loss_zScoreTarget(y_trueZScore, y_pred):\n",
    "    # stdDev_zScorePred = 1/n * sqrt((y_zScore - y_zScoreMean)^2) = 1/n *sqrt(sum( (y-mean)/std - (y_mean-mean)/std )^2) = 1/n * sqrt(sum( (y-y_mean)/std )^2 )) = 1/std * 1/n * sqrt(sum(y-y_mean)^2) = stdDev / std\n",
    "    # stdDev_zScorePred = stdDev_pred / std\n",
    "    # y_pred contains 1. y_zScore 2. log(stdDev_zScore)\n",
    "\n",
    "    y_true = y_trueZScore * std + mean   # y_zScore = (y - mean) / std -> y = y_zScore *std + mean\n",
    "\n",
    "    y_predZScore = y_pred[:, :,0]\n",
    "    log_sigma = y_pred[:, :,1]  # Log of the standard deviation / we predict log(stdDev_zScore) = log(stdDev / std) = log(stdDev) - log(std) -> log(stdDev) = log(stdDev_zScore) + log(std)\n",
    "\n",
    "    y_pred0 = y_predZScore * std + mean\n",
    "    stdDev = tf.exp(log_sigma)*std  # Exponentiate to get variance + scale back from zscore \n",
    "    logStdDev = log_sigma + tf.math.log(std)\n",
    "\n",
    "    L_pred = -0.5*(tf.math.log(2*np.pi) + logStdDev + tf.square(y_true - y_pred0) / stdDev)\n",
    "    L_ref = -0.5*(tf.math.log(2*np.pi) +  tf.math.log(std*std) + tf.square(y_trueZScore))   # ( (y_true - mean)/std )^2 = y_trueZScore^2  (y_true = y_trueZScore * std + mean)\n",
    "    L_ideal = -0.5*(tf.math.log(2*np.pi) + tf.math.log(1e-10))\n",
    "\n",
    "    L = (L_pred -L_ref) / (L_ideal - L_ref)\n",
    "    loss = -(L - 1)\n",
    "    #print(L)\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "#log_likelihood_zScoreTarget(batch[1], out),log_loss_zScoreTarget(batch[1], out)\n",
    "log_likelihood_zScoreTarget(batch[1], out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler\n",
    "def decay_schedule(epoch, lr):\n",
    "    # decay by 0.1 every 5 epochs; use `% 1` to decay after each epoch\n",
    "    if (epoch % 5 == 0) and (epoch != 0):\n",
    "        lr = lr * 0.1\n",
    "    return lr\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(decay_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "def loss_fn0(y_true_zScore, y_pred):\n",
    "    y_predZScore = y_pred[:, :,0]  # y_zScore = (y - mean)/std\n",
    "    logConfidence = y_pred[:, :,1] # logStdDev = log(stdDev / std)\n",
    "\n",
    "    loss = tf.math.abs(y_true_zScore-y_predZScore)\n",
    "    loss_2 = tf.math.abs(loss-tf.exp(logConfidence))\n",
    "    return tf.reduce_mean(loss+0.0001*loss_2)\n",
    "\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(loss=loss_fn0,metrics=[log_likelihood_zScoreTarget], optimizer=optimizer)\n",
    "\n",
    "history = model.fit(train_dataset, \n",
    "                    #batch[0],batch[1], #verbose=2,\n",
    "                    #normData, batch[1].numpy(),\n",
    "                    validation_data=test_dataset,\n",
    "                    #validation_data=(normTest, test_batch[1]),\n",
    "                    epochs=400, batch_size=batch_size,\n",
    "                    #callbacks=[lr_scheduler]\n",
    "                    #callbacks=[validation_callback(val_metric_list), lr_callback, WeightDecayCallback()]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fcn loss on one batch ~1e-8\n",
    "#gaussian_log_likelihood: -0.1196 - loss: 309017632.0000 - val_gaussian_log_likelihood: 0.8712 - val_loss: 1794403072.0000\n",
    "#gaussian_log_likelihood: -24139.4531 - loss: 117853856.0000 - val_gaussian_log_likelihood: -18151.4355 - val_loss: 987686784.0000\n",
    "\n",
    "# FCN: with conv (~70 loss -> doesnt converge as well)\n",
    "# FCN: without conv, best after 400 epochs ~40 -> still values are very similar from one position\n",
    "\n",
    "# experiments, only 1 target\n",
    "# ~0.17 for fcn we can fit the data well\n",
    "# ~0.16-0.17 for fcn + conv NN in the beginning\n",
    "# ~19 for normalization only before last layer / same for normalization all throughout\n",
    "# normalization after first dense -> helped, got better towards the end (after input doesn't converge)\n",
    "# normalization of input features over time -> nans\n",
    "\n",
    "# ---- 2 targets working --- (normalization of input features!)\n",
    "# normalization of input features over time + normalization of last layer -> waaay better ~0.12\n",
    "# normalization of input features over time + normalization of last alyer + conv encoder ~ 0.11\n",
    "# normalization layer instead of input f norm doesn't seem to work\n",
    "# making network smaller (factor 1) ->~0.04\n",
    "# instead of 3 fc layer only 1 fc layer + 2 output layer  -> 0.033\n",
    "\n",
    "# ----- 12 samples ---\n",
    "# 1 fc layer +2 output layers -> ~39loss / 59 loss test (overfitting after ~200 epochs)\n",
    "# 3fc layer * 4 size -> ~44logg / 66 test loss (overfitting after ~85 epochs)\n",
    "# transformer predicts same values again, even with norm values, loss ~70\n",
    "\n",
    "# ----- 1 target + transformers\n",
    "# base ~4.5 loss on one target \n",
    "# with only 1 fcn layer between output and trans a lot worse ~25 loss\n",
    "\n",
    "# ---- 2 targets + transf\n",
    "# larger encoder of transformer same issue, only fits mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pred = model.predict(normData)\n",
    "pred = model.predict(batch[0])\n",
    "pred[:,0:10,0], batch[1][:,0:10] ,np.exp(pred[:,0:10,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[:,0:2,0], batch[1][:,0:2] ,np.exp(pred[:,0:2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('overall',loss_fn0(batch[1],pred))\n",
    "for i in range(batch_size):\n",
    "    print(f'batch {i}',loss_fn0(batch[1][i,:],pred[i:i+1,:,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
